{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da93c77",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b130ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import printoptions\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5dcd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True #Enable processing images(prevent OSError: image file is truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296158fb",
   "metadata": {},
   "source": [
    "# 2. Set paths\n",
    " - To load dataset\n",
    " - To save checkpoints & best checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d82f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date today: 20211207\n"
     ]
    }
   ],
   "source": [
    "#Set directories as you want.\n",
    "path = \"/home/ubuntu/Desktop/Project\"\n",
    "dataset_path = os.path.join(path, \"datasets/circlin_feeds_dataset/image_dataset\")\n",
    "\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "print(f\"Date today: {date}\")\n",
    "checkpoint_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "model_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "metric_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "\n",
    "# Save path for logs\n",
    "# logdir = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590f6ed",
   "metadata": {},
   "source": [
    "# 3. Training settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9dde8",
   "metadata": {},
   "source": [
    "## 3-1. Set seed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52637839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all seeds to make experiments reproducible\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed(2020)\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c10801",
   "metadata": {},
   "source": [
    "## 3-2. Hyperparameters\n",
    " - __Adjust: <u>mean</u>, <u>std</u>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training parameters.s\n",
    "NUM_WORKERS = 8 # Number of CPU processes for data preprocessing\n",
    "LEARNING_RATE = 1e-4 # Learning rate\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 128\n",
    "save_freq = 1 # Save checkpoint frequency (epochs)\n",
    "test_freq = 200 # Test model frequency (iterations)\n",
    "EPOCHS = 48 # Number of epochs for training \n",
    "# Note: on the small subset of data overfitting happens after 30-35 epochs\n",
    "\n",
    "\n",
    "#For normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Run tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943551f",
   "metadata": {},
   "source": [
    "## 3-3. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe33abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets) #BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030b9d7",
   "metadata": {},
   "source": [
    "## 3-4. Check GPU status & Enable distributed processing\n",
    " - __Should be improved!__ \n",
    "   - As is : Using DatParallel\n",
    "   - To be: Use DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf1ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Device check(for GPU computing)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4527158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For multiple GPU utilization: This should be improved...\n",
    "\n",
    "# dist.init_process_group(\n",
    "#     backend='nccl',\n",
    "#     init_method='tcp://localhost:9999', #FREEPORT\n",
    "#     world_size=2,\n",
    "#     rank=0,\n",
    "# )\n",
    "\n",
    "# dist.init_process_group(\n",
    "#     backend=\"nccl\",\n",
    "#     init_method='tcp://127.0.0.1:9999',\n",
    "#     rank=0,\n",
    "#     world_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761908f2",
   "metadata": {},
   "source": [
    "## 3-5. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534520d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "def make_optimizer(model, lr):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params =  model.parameters(), \n",
    "        lr=lr)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68000b",
   "metadata": {},
   "source": [
    "# 4. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e0e93",
   "metadata": {},
   "source": [
    "## 4-1. Define target labels(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fb22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define taret labels\n",
    "labels = ['간편식', '건강간식', '건강식', '건강음료', '걷기/산책', '격투기', '골프', \n",
    "          '기타식단', '기타운동', '농구', '달리기/조깅', '당구', '등산/등반', '루틴기록', '맨몸', '무술', \n",
    "          '배구', '배드민턴', '보조제', '보충제', '볼링', '수상스포츠', '스키/스노보드', '승마', '신체기록', \n",
    "          '야구', '온라인클래스', '요가', '운동기구', '운동용품', '웨이트', '유산소기록', '의류', '일반간식', \n",
    "          '일반식', '일반음료', '일상생활', '자전거', '종합운동', '줄넘기', '축구/풋살', '탁구', '테니스', \n",
    "          '폴댄스', '필라테스', '홈트'] #46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08841c18",
   "metadata": {},
   "source": [
    "## 4-2. Create custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161492d",
   "metadata": {},
   "source": [
    "- At Image.open in __ __getitem__ __  needs .convert('RGB') because Image.open returns grayscale.\n",
    "    - https://stackoverflow.com/questions/59218671/runtimeerror-output-with-shape-1-224-224-doesnt-match-the-broadcast-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        self.feed_image = df['url'] #Series of file name\n",
    "        self.labels = self.df[labels].values #df.values: np.array #one-hot encoded: [0, 1, 0, ...., 1, 1]\n",
    "        \n",
    "        #self.image_list = self.feed_image.tolist()\n",
    "        #self.label_list = self.labels.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feed_image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        image_url = self.feed_image[index]\n",
    "        \n",
    "        #Needs .convert('RGB') because Image.open returns grayscale.\n",
    "        image = Image.open(image_url).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "    \n",
    "# train_annotations = os.path.join(img_folder, 'small_train.json')\n",
    "# train_dataset = CustomDataset(img_folder, train_annotations, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bff5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'seq', 'url', 'deidentification_x', '간편식', '건강간식', '건강식',\n",
      "       '건강음료', '걷기/산책', '격투기', '골프', '기타식단', '기타운동', '농구', '달리기/조깅', '당구',\n",
      "       '등산/등반', '루틴기록', '맨몸', '무술', '배구', '배드민턴', '보조제', '보충제', '볼링', '수상스포츠',\n",
      "       '스키/스노보드', '승마', '신체기록', '야구', '온라인클래스', '요가', '운동기구', '운동용품', '웨이트',\n",
      "       '유산소기록', '의류', '일반간식', '일반식', '일반음료', '일상생활', '자전거', '종합운동', '줄넘기',\n",
      "       '축구/풋살', '탁구', '테니스', '폴댄스', '필라테스', '홈트'],\n",
      "      dtype='object')\n",
      "['n']\n",
      "215145\n"
     ]
    }
   ],
   "source": [
    "#Get image dataset\n",
    "dataset = os.path.join(dataset_path, \"20211201_image_dataset(change_url).csv\")\n",
    "whole_df = pd.read_csv(dataset)\n",
    "print(whole_df.columns)\n",
    "print(whole_df['deidentification_x'].unique())\n",
    "print(len(whole_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0505f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', '간편식', '건강간식', '건강식', '건강음료', '걷기/산책', '격투기', '골프', '기타식단',\n",
      "       '기타운동', '농구', '달리기/조깅', '당구', '등산/등반', '루틴기록', '맨몸', '무술', '배구', '배드민턴',\n",
      "       '보조제', '보충제', '볼링', '수상스포츠', '스키/스노보드', '승마', '신체기록', '야구', '온라인클래스',\n",
      "       '요가', '운동기구', '운동용품', '웨이트', '유산소기록', '의류', '일반간식', '일반식', '일반음료',\n",
      "       '일상생활', '자전거', '종합운동', '줄넘기', '축구/풋살', '탁구', '테니스', '폴댄스', '필라테스',\n",
      "       '홈트'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>간편식</th>\n",
       "      <th>건강간식</th>\n",
       "      <th>건강식</th>\n",
       "      <th>건강음료</th>\n",
       "      <th>걷기/산책</th>\n",
       "      <th>격투기</th>\n",
       "      <th>골프</th>\n",
       "      <th>기타식단</th>\n",
       "      <th>기타운동</th>\n",
       "      <th>...</th>\n",
       "      <th>일상생활</th>\n",
       "      <th>자전거</th>\n",
       "      <th>종합운동</th>\n",
       "      <th>줄넘기</th>\n",
       "      <th>축구/풋살</th>\n",
       "      <th>탁구</th>\n",
       "      <th>테니스</th>\n",
       "      <th>폴댄스</th>\n",
       "      <th>필라테스</th>\n",
       "      <th>홈트</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  간편식  건강간식  건강식  건강음료  \\\n",
       "0  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "1  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "2  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "3  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "4  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "5  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "6  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "7  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "8  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "9  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    1     0   \n",
       "\n",
       "   걷기/산책  격투기  골프  기타식단  기타운동  ...  일상생활  자전거  종합운동  줄넘기  축구/풋살  탁구  테니스  폴댄스  \\\n",
       "0      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "1      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "2      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "3      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "4      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "5      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "6      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "7      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "8      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "9      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "\n",
       "   필라테스  홈트  \n",
       "0     0   0  \n",
       "1     0   0  \n",
       "2     0   0  \n",
       "3     0   0  \n",
       "4     0   0  \n",
       "5     0   1  \n",
       "6     0   0  \n",
       "7     0   0  \n",
       "8     0   0  \n",
       "9     0   0  \n",
       "\n",
       "[10 rows x 47 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop useless features/columns\n",
    "copy_df = whole_df.copy()\n",
    "copy_df.drop(labels=['index', 'seq', 'deidentification_x'], axis=1, inplace=True)\n",
    "print(copy_df.columns)\n",
    "copy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b464ec3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123.675, 116.28, 103.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torchvision/transforms/transforms.py:1362: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torchvision/transforms/transforms.py:1376: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
      "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n"
     ]
    }
   ],
   "source": [
    "#Train - validation split\n",
    "train_size = 0.8\n",
    "train_df = copy_df.copy().sample(frac=train_size, random_state=200).reset_index(drop=True)\n",
    "val_df = copy_df.drop(train_df.index).reset_index(drop=True)\n",
    "\n",
    "# Train preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomAffine(degrees=20, \n",
    "                            translate=(0.2, 0.2),\n",
    "                            scale=(0.5, 1.5),\n",
    "                            shear=None,\n",
    "                            resample=False, \n",
    "                            fillcolor=tuple(np.array(np.array(mean)*255).astype(int).tolist())),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Test preprocessing\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(tuple(np.array(np.array(mean)*255).tolist()))\n",
    "\n",
    "train_dataset = CustomDataset(train_df, train_transform)\n",
    "valid_dataset = CustomDataset(val_df, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edd955c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader =  torch.utils.data.DataLoader(train_dataset, \n",
    "                              batch_size=TRAIN_BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS,  #0?\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "val_data_loader =  torch.utils.data.DataLoader(valid_dataset, \n",
    "                             batch_size=VALID_BATCH_SIZE, \n",
    "                             num_workers=NUM_WORKERS) #0?\n",
    "\n",
    "#num_train_batches = int(np.ceil(len(train_dataset) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41255b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To explore file shape.\n",
    "# batchlist = []\n",
    "# datalist = []\n",
    "# for batch_idx, data in enumerate(train_data_loader):\n",
    "#     #print(batch_idx, data)\n",
    "#     batchlist.append(batch_idx)\n",
    "#     datalist.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d76fb",
   "metadata": {},
   "source": [
    "# 5. Make feed image classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5095c0",
   "metadata": {},
   "source": [
    "## 5-1. Define functions that save checkpoint of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d59b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min #valid_loss_min.item()\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c09f1",
   "metadata": {},
   "source": [
    "## 5-2. Define Resnext50 model as a class\n",
    " - Use pytorch implemented pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73a7eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class ResNeXt50Class(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "#         super().__init__()\n",
    "#         resnet = models.resnext50_32x4d(pretrained=True)\n",
    "#         resnet.fc = nn.Sequential(\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
    "#         )\n",
    "#         self.base_model = resnet\n",
    "        super(ResNeXt50Class, self).__init__()\n",
    "        self.resnext_model = models.resnext50_32x4d(pretrained=True)\n",
    "        self.resnext_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=self.resnext_model.fc.in_features, \n",
    "                      out_features=n_classes)\n",
    "        )\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.sigm(self.resnext_model(x))\n",
    "        \n",
    "        return output\n",
    "        #return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f8b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNeXt50Class(\n",
       "    (resnext_model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Sequential(\n",
       "        (0): Dropout(p=0.2, inplace=False)\n",
       "        (1): Linear(in_features=2048, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (sigm): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNeXt50Class(len(labels)) #or len(labels) #train_dataset.classes\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model) #Distributed\n",
    "#model = nn.parallel.DistributedDataParallel(model, device_ids=[0, 1]) #Distributed DataParallel  ===> Should use this!!!!!!!!!!!!!!!!!\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c183807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = make_optimizer(model, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec6432a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets = []\n",
    "val_outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d56e37",
   "metadata": {},
   "source": [
    "## 5-3. Training\n",
    " - __<u>Add Train Loss!!!!!!!!!!</u>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832aef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [\n",
    "                 0.00001, 0.00002, 0.00003, 0.00004, 0.00005,                 \n",
    "                 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n",
    "                 0.001, 0.002, 0.003, 0.004, 0.005,\n",
    "                 0.01, 0.02, 0.03, 0.04, 0.05,\n",
    "                 0.1, 0.2, 0.3, 0.4, 0.5,\n",
    "                 0.000001, 0.000002, 0.000003, 0.000004, 0.000005]\n",
    "train_losses_lr = {}\n",
    "avg_train_losses_lr = {}\n",
    "val_losses_lr = {}\n",
    "avg_val_losses_lr = {}\n",
    "epoch_list = [int(x) for x in np.linspace(1, EPOCHS, EPOCHS).tolist()]\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d1c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs,\n",
    "                       training_loader,\n",
    "                       validation_loader,\n",
    "                       model,\n",
    "                       optimizer,\n",
    "                       checkpoint_path,\n",
    "                       best_model_path,\n",
    "                       metric_path,\n",
    "                       date):\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    train_loss_epoch = []\n",
    "    avg_train_loss_epoch = []    \n",
    "    val_loss_epoch = [] #append to val_loss_list\n",
    "    avg_val_loss_epoch = [] #append to avg_val_list\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        print(f'############# Epoch {epoch}: Training Start   #############')\n",
    "        for batch_idx, data in enumerate(training_loader):\n",
    "            images, targets = data[0], data[1]\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if batch_idx%5000==0:\n",
    "                print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "        print('############# Epoch {}: Training End     #############'.format(epoch))\n",
    "        train_loss_epoch.append(train_loss)\n",
    "        print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "\n",
    "        model.eval()\n",
    "   \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(validation_loader, 0):\n",
    "                images, targets = data[0], data[1]\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
    "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "            print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
    "           # calculate average losses\n",
    "#             print('before calculate avg train loss', train_loss)\n",
    "            val_loss_epoch.append(valid_loss) \n",
    "            avg_train_loss = train_loss/len(training_loader)\n",
    "            avg_valid_loss = valid_loss/len(validation_loader)\n",
    "            #Print training/validation statistics\n",
    "            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                avg_train_loss,\n",
    "                avg_valid_loss\n",
    "            ))\n",
    "            avg_train_loss_epoch.append(avg_train_loss)\n",
    "            avg_val_loss_epoch.append(avg_valid_loss) \n",
    "            \n",
    "\n",
    "            # create checkpoint variable and add important data\n",
    "            checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'valid_loss_min': avg_valid_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "              }\n",
    "\n",
    "            save_ckp(checkpoint, False,  f\"{checkpoint_path}_{epoch}\", best_model_path)\n",
    "            \n",
    "            ## TODO: save the model if validation loss has decreased\n",
    "            if avg_valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,avg_valid_loss))\n",
    "                # save checkpoint as best model\n",
    "                save_ckp(checkpoint, True,  f\"{checkpoint_path}_{epoch}\", best_model_path)\n",
    "                valid_loss_min = avg_valid_loss\n",
    "\n",
    "        now = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_text = f\"[{now}]: [Learning Rate {lr}, Epoch {epoch}] - train_loss = {train_loss}, avg_train_loss = {avg_train_loss}, validation_loss = {valid_loss}, avg_validation_loss = {avg_valid_loss}\\n\"\n",
    "        if os.path.isfile(os.path.join(metric_path, f\"metric_logs_resnext_{date}.txt\")):\n",
    "            with open(os.path.join(metric_path, f\"metric_logs_resnext_{date}.txt\"), 'a', encoding='utf-8') as f:\n",
    "                f.write(log_text)\n",
    "        else:\n",
    "            with open(os.path.join(metric_path, f\"metric_logs_resnext_{date}.txt\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(log_text)       \n",
    "        print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
    "\n",
    "    train_losses_lr[lr] = train_loss_epoch\n",
    "    avg_train_losses_lr[lr] = avg_train_loss_epoch\n",
    "    val_losses_lr[lr] = val_loss_epoch\n",
    "    avg_val_losses_lr[lr] = avg_val_loss_epoch\n",
    "    print(f\"train_losses_lr for LR {lr}: \\n {train_losses_lr}\")\n",
    "    print(f\"avg_train_losses_lr for LR {lr}: \\n {avg_train_losses_lr}\")\n",
    "    print(f\"val_losses_lr for LR {lr}: \\n {val_losses_lr}\")\n",
    "    print(f\"avg_val_losses_lr {lr}: \\n {avg_val_losses_lr}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "248e1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use threshold to define predicted labels and invoke sklearn's metrics with different averaging strategies.\n",
    "# def calculate_metrics(pred, target, threshold=0.5):\n",
    "#     pred = np.array(pred > threshold, dtype=float)\n",
    "#     return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187d033",
   "metadata": {},
   "source": [
    "### Set checkpoint path, best model's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02029cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(checkpoint_path, \"curr_ckpt\")\n",
    "best_model_path = os.path.join(checkpoint_path, \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd769d99",
   "metadata": {},
   "source": [
    "### Training start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33a3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 1e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n",
      "Epoch: 1, Training Loss:  0.6824885606765747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torch/cuda/nccl.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000104 \tAverage Validation Loss: 0.000174\n",
      "Validation loss decreased (inf --> 0.000174).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.06876169145107269\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000045 \tAverage Validation Loss: 0.000148\n",
      "Validation loss decreased (0.000174 --> 0.000148).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.0607546828687191\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000040 \tAverage Validation Loss: 0.000135\n",
      "Validation loss decreased (0.000148 --> 0.000135).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.05122576653957367\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000038 \tAverage Validation Loss: 0.000129\n",
      "Validation loss decreased (0.000135 --> 0.000129).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.05332066863775253\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000036 \tAverage Validation Loss: 0.000125\n",
      "Validation loss decreased (0.000129 --> 0.000125).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.052196286618709564\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000035 \tAverage Validation Loss: 0.000121\n",
      "Validation loss decreased (0.000125 --> 0.000121).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.04224579781293869\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000034 \tAverage Validation Loss: 0.000116\n",
      "Validation loss decreased (0.000121 --> 0.000116).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.045439597219228745\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000033 \tAverage Validation Loss: 0.000113\n",
      "Validation loss decreased (0.000116 --> 0.000113).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.04310402646660805\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000032 \tAverage Validation Loss: 0.000111\n",
      "Validation loss decreased (0.000113 --> 0.000111).  Saving model ...\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.03744145482778549\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000031 \tAverage Validation Loss: 0.000108\n",
      "Validation loss decreased (0.000111 --> 0.000108).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.044631633907556534\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000031 \tAverage Validation Loss: 0.000108\n",
      "Validation loss decreased (0.000108 --> 0.000108).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.04086832329630852\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000030 \tAverage Validation Loss: 0.000105\n",
      "Validation loss decreased (0.000108 --> 0.000105).  Saving model ...\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.036244574934244156\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000029 \tAverage Validation Loss: 0.000102\n",
      "Validation loss decreased (0.000105 --> 0.000102).  Saving model ...\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.042563971132040024\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000029 \tAverage Validation Loss: 0.000100\n",
      "Validation loss decreased (0.000102 --> 0.000100).  Saving model ...\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.03703678026795387\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000028 \tAverage Validation Loss: 0.000098\n",
      "Validation loss decreased (0.000100 --> 0.000098).  Saving model ...\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.03625928983092308\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000028 \tAverage Validation Loss: 0.000098\n",
      "Validation loss decreased (0.000098 --> 0.000098).  Saving model ...\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.045902054756879807\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000027 \tAverage Validation Loss: 0.000095\n",
      "Validation loss decreased (0.000098 --> 0.000095).  Saving model ...\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.03144552931189537\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000027 \tAverage Validation Loss: 0.000094\n",
      "Validation loss decreased (0.000095 --> 0.000094).  Saving model ...\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss:  0.03407878056168556\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000026 \tAverage Validation Loss: 0.000092\n",
      "Validation loss decreased (0.000094 --> 0.000092).  Saving model ...\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.029506342485547066\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000025 \tAverage Validation Loss: 0.000090\n",
      "Validation loss decreased (0.000092 --> 0.000090).  Saving model ...\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.04187271371483803\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000025 \tAverage Validation Loss: 0.000089\n",
      "Validation loss decreased (0.000090 --> 0.000089).  Saving model ...\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.03239217773079872\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000024 \tAverage Validation Loss: 0.000088\n",
      "Validation loss decreased (0.000089 --> 0.000088).  Saving model ...\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.030674919486045837\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000024 \tAverage Validation Loss: 0.000088\n",
      "Validation loss decreased (0.000088 --> 0.000088).  Saving model ...\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.02665850892663002\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000023 \tAverage Validation Loss: 0.000085\n",
      "Validation loss decreased (0.000088 --> 0.000085).  Saving model ...\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.026753714308142662\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000023 \tAverage Validation Loss: 0.000084\n",
      "Validation loss decreased (0.000085 --> 0.000084).  Saving model ...\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.0308552049100399\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000023 \tAverage Validation Loss: 0.000081\n",
      "Validation loss decreased (0.000084 --> 0.000081).  Saving model ...\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.028119655326008797\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000022 \tAverage Validation Loss: 0.000081\n",
      "Validation loss decreased (0.000081 --> 0.000081).  Saving model ...\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.028047144412994385\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000022 \tAverage Validation Loss: 0.000079\n",
      "Validation loss decreased (0.000081 --> 0.000079).  Saving model ...\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.03588445857167244\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000021 \tAverage Validation Loss: 0.000079\n",
      "Validation loss decreased (0.000079 --> 0.000079).  Saving model ...\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.027533024549484253\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000021 \tAverage Validation Loss: 0.000077\n",
      "Validation loss decreased (0.000079 --> 0.000077).  Saving model ...\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.02324294112622738\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.022917989641427994\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000076\n",
      "Validation loss decreased (0.000077 --> 0.000076).  Saving model ...\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.02922304905951023\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000074\n",
      "Validation loss decreased (0.000076 --> 0.000074).  Saving model ...\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.02627529762685299\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.021358920261263847\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000073\n",
      "Validation loss decreased (0.000074 --> 0.000073).  Saving model ...\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.02515525370836258\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000072\n",
      "Validation loss decreased (0.000073 --> 0.000072).  Saving model ...\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Training Loss:  0.021261030808091164\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000072 --> 0.000071).  Saving model ...\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.0210094153881073\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000071 --> 0.000070).  Saving model ...\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.02338717319071293\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.02665204182267189\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000070 --> 0.000070).  Saving model ...\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.026454579085111618\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000070 --> 0.000069).  Saving model ...\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.021067192777991295\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000069 --> 0.000068).  Saving model ...\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.021449102088809013\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000067\n",
      "Validation loss decreased (0.000068 --> 0.000067).  Saving model ...\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.020477192476391792\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.018648482859134674\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000067 --> 0.000066).  Saving model ...\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.020638378337025642\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.01983586698770523\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000066 --> 0.000066).  Saving model ...\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.022610396146774292\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000065\n",
      "Validation loss decreased (0.000066 --> 0.000065).  Saving model ...\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 1e-05: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182]}\n",
      "avg_train_losses_lr for LR 1e-05: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05]}\n",
      "val_losses_lr for LR 1e-05: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932]}\n",
      "avg_val_losses_lr 1e-05: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 2e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.016760535538196564\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000067\n",
      "Validation loss decreased (inf --> 0.000067).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.020138459280133247\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.016811516135931015\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.019649503752589226\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000067\n",
      "Validation loss decreased (0.000067 --> 0.000067).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.01960809715092182\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.018613716587424278\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000067 --> 0.000066).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.017954012379050255\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.018744133412837982\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000066 --> 0.000064).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.015444489195942879\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.016100682318210602\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000064 --> 0.000064).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.01486255880445242\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000063\n",
      "Validation loss decreased (0.000064 --> 0.000063).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.01970139890909195\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.013195842504501343\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.015987614169716835\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.0178129393607378\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.012514682486653328\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000063 --> 0.000062).  Saving model ...\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.01691567339003086\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.0166409220546484\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.014947867020964622\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.0141150439158082\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.008654399774968624\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.014937784522771835\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.016284232959151268\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.018622899428009987\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.01365657802671194\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.011811885982751846\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.011868765577673912\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.00950858648866415\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000061\n",
      "Validation loss decreased (0.000062 --> 0.000061).  Saving model ...\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.008676997385919094\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000062\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.01149714831262827\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.009915265254676342\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.009953899309039116\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.00834877323359251\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.010806355625391006\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000062\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.011671191081404686\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000062\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.01252779085189104\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.00998594705015421\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000062\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.00754344230517745\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.01395192090421915\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000063\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.012485443614423275\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.012991693802177906\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.011038584634661674\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.010295897722244263\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.010325441136956215\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000065\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.01265723817050457\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.012346264906227589\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000065\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.011824285611510277\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.008301377296447754\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 2e-05: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104]}\n",
      "avg_train_losses_lr for LR 2e-05: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06]}\n",
      "val_losses_lr for LR 2e-05: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564]}\n",
      "avg_val_losses_lr 2e-05: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 3e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.013446479104459286\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000067\n",
      "Validation loss decreased (inf --> 0.000067).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.010101326741278172\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.01352034229785204\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000067 --> 0.000066).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.013878775760531425\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.01357512641698122\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.009653225541114807\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.013227932155132294\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000066 --> 0.000066).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.009986916556954384\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000065\n",
      "Validation loss decreased (0.000066 --> 0.000065).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.009403059259057045\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.009359435178339481\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.01228607539087534\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000065 --> 0.000064).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.007002133410423994\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.007279041223227978\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.007937231101095676\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.01197102852165699\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000065\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.011160684749484062\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.009128940291702747\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.009436344727873802\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.009504469111561775\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.007910189218819141\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training Loss:  0.008812353014945984\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.009995600208640099\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.00969082023948431\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.008790502324700356\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.009314185008406639\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.010344420559704304\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.006651565432548523\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.011283310130238533\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.0066456845961511135\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.008234233595430851\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.010321293026208878\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.011249203234910965\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.007045574951916933\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.007283438928425312\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.006049151066690683\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.005471576936542988\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.005663384683430195\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.008014990016818047\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.01106977742165327\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.009228487499058247\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.007909851148724556\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.00697623286396265\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.009282968938350677\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.00644477317109704\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.00903372373431921\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.0079219751060009\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.005187455099076033\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.006581892725080252\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 3e-05: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036]}\n",
      "avg_train_losses_lr for LR 3e-05: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06]}\n",
      "val_losses_lr for LR 3e-05: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238]}\n",
      "avg_val_losses_lr 3e-05: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 4e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.006953234318643808\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "Validation loss decreased (inf --> 0.000072).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.013013034127652645\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.00936105102300644\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "Validation loss decreased (0.000072 --> 0.000072).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.01071465015411377\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000072 --> 0.000071).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.012690857984125614\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.008420318365097046\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.006339829880744219\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000071 --> 0.000070).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.011019621044397354\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.004247892647981644\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.007569239940494299\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000070 --> 0.000069).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.009182978421449661\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.012093756347894669\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.008836551569402218\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.008519317023456097\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.006596129387617111\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.007685953751206398\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.005895090755075216\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.007294066715985537\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.008502953685820103\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.005694253835827112\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training Loss:  0.009431686252355576\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.006521516479551792\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.008831328712403774\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.008250276558101177\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.006857600528746843\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.008208015002310276\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.006042574066668749\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.006790893152356148\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.005936052650213242\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.00469103641808033\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.005926282610744238\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.005336238071322441\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.006800857838243246\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.008538936264812946\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.007015758194029331\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.008098790422081947\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.00567768607288599\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.007379848975688219\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.00848840270191431\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.006633877754211426\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.005490560550242662\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.006436180789023638\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.00733626214787364\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.007403505966067314\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.0040361699648201466\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.00745368329808116\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.00520071666687727\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.00918513722717762\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 4e-05: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194]}\n",
      "avg_train_losses_lr for LR 4e-05: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06]}\n",
      "val_losses_lr for LR 4e-05: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274]}\n",
      "avg_val_losses_lr 4e-05: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 5e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.007816086523234844\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "Validation loss decreased (inf --> 0.000081).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.009104158729314804\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "Validation loss decreased (0.000081 --> 0.000080).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.009603308513760567\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.007697950582951307\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "Validation loss decreased (0.000080 --> 0.000079).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.00935259461402893\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "Validation loss decreased (0.000079 --> 0.000076).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.005018969532102346\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.0077779837884008884\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "Validation loss decreased (0.000076 --> 0.000076).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.0034027881920337677\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "Validation loss decreased (0.000076 --> 0.000073).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.008960844948887825\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.0077893915586173534\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.00676428247243166\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.005436534062027931\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.007243380881845951\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.01192488893866539\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.006005967501550913\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.005425252951681614\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.00776288565248251\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.004733866546303034\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.00638193404302001\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.009089012630283833\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training Loss:  0.007950532250106335\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.008558157831430435\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.004327032249420881\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.005458369385451078\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.007432501297444105\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.007549503818154335\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.004184075631201267\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.0050729853101074696\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.00724705308675766\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.004991020075976849\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.003073558211326599\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.00917133316397667\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000087\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.010201970115303993\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000089\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.005648279562592506\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.0065131401643157005\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.005346397869288921\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.005824169609695673\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.0065922848880290985\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.005811239127069712\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.00970927719026804\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.005418939981609583\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.005811391863971949\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.006167133804410696\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.005923519842326641\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000085\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.006171020679175854\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.0066703930497169495\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.00932620745152235\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.00866563618183136\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 5e-05: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493]}\n",
      "avg_train_losses_lr for LR 5e-05: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06]}\n",
      "val_losses_lr for LR 5e-05: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626]}\n",
      "avg_val_losses_lr 5e-05: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0001 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.009349873289465904\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "Validation loss decreased (inf --> 0.000077).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.01403214130550623\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000077\n",
      "Validation loss decreased (0.000077 --> 0.000077).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.009844083338975906\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000077 --> 0.000070).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.011451305821537971\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.012298563495278358\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.010808438062667847\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.010810033418238163\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.012160811573266983\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.0060113100335001945\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.010627533309161663\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.010886741802096367\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.011990844272077084\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.012899580411612988\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000070 --> 0.000068).  Saving model ...\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.011534160003066063\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.008779226802289486\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.009685379453003407\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.008432181552052498\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.015825875103473663\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.010815998539328575\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.008426972664892673\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.009822634048759937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.013184295035898685\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.008866320364177227\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.00671013817191124\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.011921759694814682\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.007671681698411703\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.009976182132959366\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.008243849501013756\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.01049081701785326\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.007222693879157305\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.009943013079464436\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.008482338860630989\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.01011030562222004\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.005626835394650698\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.011291439644992352\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.007293225731700659\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.006721766199916601\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.010389472357928753\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.007191862445324659\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.007250633090734482\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.006135842297226191\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Training Loss:  0.007348954677581787\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.008040431886911392\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.005641268100589514\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.006522809620946646\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.006452217232435942\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.006320096552371979\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.007401275914162397\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.0001: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985]}\n",
      "avg_train_losses_lr for LR 0.0001: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06]}\n",
      "val_losses_lr for LR 0.0001: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466]}\n",
      "avg_val_losses_lr 0.0001: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0002 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.007610165514051914\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000075\n",
      "Validation loss decreased (inf --> 0.000075).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.011543639935553074\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000072\n",
      "Validation loss decreased (0.000075 --> 0.000072).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.016019674018025398\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000072 --> 0.000071).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.014270526356995106\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000071 --> 0.000068).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.012052686884999275\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.01675964519381523\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.01694757118821144\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000068 --> 0.000068).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.012904641218483448\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.014870221726596355\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.01294383779168129\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000068 --> 0.000066).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.012224296107888222\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.00870408583432436\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.011819155886769295\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000067\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.009961176663637161\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.013682490214705467\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.011431523598730564\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.01017771102488041\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000066 --> 0.000066).  Saving model ...\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.013283142820000648\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.012101557105779648\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.008830670267343521\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.011378764174878597\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.010794145986437798\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.011237561702728271\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.01158045418560505\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.012548664584755898\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.010069879703223705\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.008147211745381355\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.007315381895750761\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.008225762285292149\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.009729304350912571\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000069\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.007613546214997768\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.011883320286870003\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.007400790229439735\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.009806856513023376\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.012044857256114483\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.009979292750358582\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.011988661251962185\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.0057278405874967575\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.006519073620438576\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.011002633720636368\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.008607340045273304\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.010119074024260044\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.007542559877038002\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.007466090843081474\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.008634918369352818\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.01065124198794365\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.00953351054340601\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.007688294630497694\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.0002: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706]}\n",
      "avg_train_losses_lr for LR 0.0002: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06]}\n",
      "val_losses_lr for LR 0.0002: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397]}\n",
      "avg_val_losses_lr 0.0002: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0003 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.005775372497737408\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (inf --> 0.000071).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.009738319553434849\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.012948784977197647\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.009857019409537315\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.009222135879099369\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000071 --> 0.000069).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.011425575241446495\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.014227136969566345\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.009154076687991619\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.010309555567800999\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.011751923710107803\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000069 --> 0.000069).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.01133404765278101\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.00956475269049406\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.00849542859941721\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000069 --> 0.000066).  Saving model ...\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.013943681493401527\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.012433554977178574\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.0145258828997612\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.012582661584019661\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.008373538963496685\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.008350341580808163\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.007300813682377338\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.009472016245126724\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.008753150701522827\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.0120368218049407\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.011874671094119549\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.007926982827484608\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.009022107347846031\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.009010082110762596\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.007256286218762398\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.00771392323076725\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.009149222634732723\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.010235970839858055\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000068\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.010335734114050865\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.0067014447413384914\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.009322170168161392\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.009050815366208553\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.009131146594882011\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.00846297200769186\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.007790287025272846\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.008354492485523224\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.011033106595277786\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.007371831219643354\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.006434933748096228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.0073771169409155846\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.006474173627793789\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.005919359624385834\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.009510019794106483\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.009784174151718616\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.00641586072742939\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.0003: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706], 0.0003: [0.012649923188811448, 0.012886979752303365, 0.013064661253121746, 0.012776669194170703, 0.012717449952885684, 0.012622895056708876, 0.012493407813530033, 0.012345176194295551, 0.01228418961892159, 0.011904442857485261, 0.011872770472282787, 0.01176221937861382, 0.011755546346345597, 0.01147983250398066, 0.011406897356209809, 0.011273715252866086, 0.011117836107760454, 0.011100410664983509, 0.010980995508448586, 0.010884919475453591, 0.010663047040171014, 0.010613769883916362, 0.010488587144036075, 0.010372721744629191, 0.010250856866020638, 0.010216511461337724, 0.010188323876458802, 0.010019190751855981, 0.009914184962759651, 0.00978488870394012, 0.009763344133500036, 0.009616679967231392, 0.009524154194791464, 0.009595767747669012, 0.009324085868554112, 0.009346448879417322, 0.009228157250444862, 0.009081872637103684, 0.009101319909816443, 0.00890715618897092, 0.009007162021589459, 0.008798028554294773, 0.008824452554801524, 0.008752015123588669, 0.008672068164395634, 0.008579593918035412, 0.00853371461804878, 0.008454515820111341]}\n",
      "avg_train_losses_lr for LR 0.0003: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06], 0.0003: [9.412145229770422e-06, 9.588526601416194e-06, 9.720730099048918e-06, 9.506450293281773e-06, 9.462388357801848e-06, 9.392035012432199e-06, 9.29569033744794e-06, 9.185398954088952e-06, 9.140022037888088e-06, 8.857472364200343e-06, 8.833906601400884e-06, 8.751651323373378e-06, 8.746686269602378e-06, 8.541542041652277e-06, 8.487274818608488e-06, 8.388180991715838e-06, 8.272199484940813e-06, 8.259234125731777e-06, 8.170383562833768e-06, 8.098898419236304e-06, 7.933814762032005e-06, 7.897150211247294e-06, 7.804008291693509e-06, 7.717798917134816e-06, 7.627125644360593e-06, 7.601571027781045e-06, 7.58059812236518e-06, 7.454755023702367e-06, 7.376625716339026e-06, 7.280423142812589e-06, 7.264392956473241e-06, 7.155267832761452e-06, 7.0864242520769825e-06, 7.139708145587062e-06, 6.937563890293238e-06, 6.954203035280746e-06, 6.866188430390522e-06, 6.757345712130717e-06, 6.771815409089615e-06, 6.627348354889077e-06, 6.701757456539776e-06, 6.546152198135992e-06, 6.565812912798753e-06, 6.511916014574903e-06, 6.452431669937227e-06, 6.383626427109681e-06, 6.349490043191056e-06, 6.290562366154271e-06]}\n",
      "val_losses_lr for LR 0.0003: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397], 0.0003: [0.02408573273099539, 0.024182828313198478, 0.024790374642287782, 0.02458177833853826, 0.02339131645277938, 0.024531481175431972, 0.026272020316530764, 0.02454436187928087, 0.02434731038143577, 0.023198279480308902, 0.024936766564039058, 0.023669052442110167, 0.022143755735448103, 0.025581597503915834, 0.02374485474168281, 0.024072552351308688, 0.023924070040112252, 0.025042960539084343, 0.024283687136404036, 0.023892706331937876, 0.024570069904508256, 0.02588527220458856, 0.028893469642151375, 0.024864395319011613, 0.025167268635953845, 0.024857166295080748, 0.024627007994502348, 0.027185201584098963, 0.025770637282040484, 0.026210746300912465, 0.022948869892315895, 0.0254249283532648, 0.02513241907858042, 0.02547277655529869, 0.024997253107730558, 0.02678582271373102, 0.025994602792539838, 0.025194243609474122, 0.025160928533166572, 0.02643916141849774, 0.024568370694034797, 0.025540607363726254, 0.02523463865664708, 0.02817382014878868, 0.0262981737889239, 0.024399230591068438, 0.02562607989096545, 0.026157262881537947]}\n",
      "avg_val_losses_lr 0.0003: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05], 0.0003: [7.147101700592104e-05, 7.175913446052961e-05, 7.356194255871746e-05, 7.294296242889691e-05, 6.941043457798035e-05, 7.279371268674175e-05, 7.795851725973521e-05, 7.283193435988389e-05, 7.224721181434947e-05, 6.883762457064956e-05, 7.399633995263816e-05, 7.02345769795554e-05, 6.57084739924276e-05, 7.590978487808853e-05, 7.045950961923684e-05, 7.14319060869694e-05, 7.099130575700965e-05, 7.431145560559152e-05, 7.205841880238586e-05, 7.08982383737029e-05, 7.290821930121144e-05, 7.681089674952094e-05, 8.573729864139874e-05, 7.378158848371398e-05, 7.468032236188085e-05, 7.376013737412684e-05, 7.307717505787047e-05, 8.066825395875063e-05, 7.647073377460085e-05, 7.777669525493313e-05, 6.809753677245073e-05, 7.544489125597864e-05, 7.45769112124048e-05, 7.558687405133142e-05, 7.417582524549127e-05, 7.948315345320777e-05, 7.713531985916866e-05, 7.476036679369176e-05, 7.466150900049428e-05, 7.845448492135828e-05, 7.290317713363441e-05, 7.578815241461797e-05, 7.48802334025136e-05, 8.360184020412072e-05, 7.803612400274154e-05, 7.240127771830397e-05, 7.60417800918856e-05, 7.76179907464034e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0004 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.00883607380092144\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "Validation loss decreased (inf --> 0.000078).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.004694713279604912\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000078 --> 0.000070).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.009562636725604534\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.00975979957729578\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.00765999173745513\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.011709342710673809\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.007394487503916025\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.013388521037995815\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.006518174894154072\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.011165567673742771\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.006503942422568798\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.008753692731261253\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.00761076295748353\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.00508548179641366\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.007513710297644138\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.00879014190286398\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.007868765853345394\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.008900006301701069\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.006679518148303032\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000097\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.005725284107029438\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.00839932356029749\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.005030773114413023\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.006920620333403349\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.0074259438551962376\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.01188663300126791\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.011439336463809013\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.010019301436841488\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.009047465398907661\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.00658571207895875\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.006318523082882166\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.008360395208001137\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.005725090391933918\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.007415322121232748\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.007650100626051426\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.00621136836707592\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.005272131413221359\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.010386344976723194\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.004261150490492582\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.007258768659085035\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.006501705385744572\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.00736731244251132\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.00774791743606329\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.005108158104121685\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000087\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.008796700276434422\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.0066089509055018425\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.004741309210658073\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.004903939552605152\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.010685540735721588\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.0004: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706], 0.0003: [0.012649923188811448, 0.012886979752303365, 0.013064661253121746, 0.012776669194170703, 0.012717449952885684, 0.012622895056708876, 0.012493407813530033, 0.012345176194295551, 0.01228418961892159, 0.011904442857485261, 0.011872770472282787, 0.01176221937861382, 0.011755546346345597, 0.01147983250398066, 0.011406897356209809, 0.011273715252866086, 0.011117836107760454, 0.011100410664983509, 0.010980995508448586, 0.010884919475453591, 0.010663047040171014, 0.010613769883916362, 0.010488587144036075, 0.010372721744629191, 0.010250856866020638, 0.010216511461337724, 0.010188323876458802, 0.010019190751855981, 0.009914184962759651, 0.00978488870394012, 0.009763344133500036, 0.009616679967231392, 0.009524154194791464, 0.009595767747669012, 0.009324085868554112, 0.009346448879417322, 0.009228157250444862, 0.009081872637103684, 0.009101319909816443, 0.00890715618897092, 0.009007162021589459, 0.008798028554294773, 0.008824452554801524, 0.008752015123588669, 0.008672068164395634, 0.008579593918035412, 0.00853371461804878, 0.008454515820111341], 0.0004: [0.010461589537520737, 0.010591399666043883, 0.010611402501776234, 0.010521375114053853, 0.010463566356477154, 0.010185533027868687, 0.010282703834014856, 0.010240158361349514, 0.010141924851701513, 0.010020689212112863, 0.009825635712851577, 0.009849205724181282, 0.009697626133856817, 0.009704186213812567, 0.009605649036876409, 0.009570111616215262, 0.009373299329170205, 0.009318887470899855, 0.009483160142172861, 0.009012243430507142, 0.009241844184413947, 0.00910941833526956, 0.009012784126493248, 0.008988859170217653, 0.008882108767076187, 0.00878490531457437, 0.008673132161458773, 0.008606571863354128, 0.008647260404922297, 0.008644623380990073, 0.008487314101720194, 0.008351002751234257, 0.008405402253196217, 0.008290212967473209, 0.008252372652523708, 0.008173055193938574, 0.008112938052363471, 0.008104167956411225, 0.008026105214369085, 0.00802645246984044, 0.007950051948206398, 0.007910032287500585, 0.00782730131357398, 0.007763153648002547, 0.0077853449440139924, 0.007676174854644897, 0.007547849814727667, 0.007528017229993745]}\n",
      "avg_train_losses_lr for LR 0.0004: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06], 0.0003: [9.412145229770422e-06, 9.588526601416194e-06, 9.720730099048918e-06, 9.506450293281773e-06, 9.462388357801848e-06, 9.392035012432199e-06, 9.29569033744794e-06, 9.185398954088952e-06, 9.140022037888088e-06, 8.857472364200343e-06, 8.833906601400884e-06, 8.751651323373378e-06, 8.746686269602378e-06, 8.541542041652277e-06, 8.487274818608488e-06, 8.388180991715838e-06, 8.272199484940813e-06, 8.259234125731777e-06, 8.170383562833768e-06, 8.098898419236304e-06, 7.933814762032005e-06, 7.897150211247294e-06, 7.804008291693509e-06, 7.717798917134816e-06, 7.627125644360593e-06, 7.601571027781045e-06, 7.58059812236518e-06, 7.454755023702367e-06, 7.376625716339026e-06, 7.280423142812589e-06, 7.264392956473241e-06, 7.155267832761452e-06, 7.0864242520769825e-06, 7.139708145587062e-06, 6.937563890293238e-06, 6.954203035280746e-06, 6.866188430390522e-06, 6.757345712130717e-06, 6.771815409089615e-06, 6.627348354889077e-06, 6.701757456539776e-06, 6.546152198135992e-06, 6.565812912798753e-06, 6.511916014574903e-06, 6.452431669937227e-06, 6.383626427109681e-06, 6.349490043191056e-06, 6.290562366154271e-06], 0.0004: [7.783920786845786e-06, 7.880505703901698e-06, 7.895388766202554e-06, 7.82840410271864e-06, 7.785391634283596e-06, 7.578521598116582e-06, 7.650821305070577e-06, 7.619165447432674e-06, 7.546075038468388e-06, 7.455869949488737e-06, 7.310740857776471e-06, 7.3282780685872635e-06, 7.215495635310132e-06, 7.220376647181969e-06, 7.147060295294947e-06, 7.120618762064927e-06, 6.974181048489736e-06, 6.9336960348957255e-06, 7.055922724830998e-06, 6.705538266746385e-06, 6.876372161022282e-06, 6.777841023266041e-06, 6.7059405703074755e-06, 6.688139263554801e-06, 6.60871188026502e-06, 6.536387882867836e-06, 6.45322333441873e-06, 6.403699303090869e-06, 6.4339735155671854e-06, 6.4320114441890425e-06, 6.314965849494192e-06, 6.213543713715965e-06, 6.254019533628138e-06, 6.168313219846138e-06, 6.14015822360395e-06, 6.081142257394772e-06, 6.036412241341869e-06, 6.029886872329781e-06, 5.971804474976998e-06, 5.972062849583661e-06, 5.915217223367856e-06, 5.885440690104602e-06, 5.82388490593302e-06, 5.7761559880971325e-06, 5.79266736905803e-06, 5.7114396239917395e-06, 5.615959683577133e-06, 5.6012032961262985e-06]}\n",
      "val_losses_lr for LR 0.0004: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397], 0.0003: [0.02408573273099539, 0.024182828313198478, 0.024790374642287782, 0.02458177833853826, 0.02339131645277938, 0.024531481175431972, 0.026272020316530764, 0.02454436187928087, 0.02434731038143577, 0.023198279480308902, 0.024936766564039058, 0.023669052442110167, 0.022143755735448103, 0.025581597503915834, 0.02374485474168281, 0.024072552351308688, 0.023924070040112252, 0.025042960539084343, 0.024283687136404036, 0.023892706331937876, 0.024570069904508256, 0.02588527220458856, 0.028893469642151375, 0.024864395319011613, 0.025167268635953845, 0.024857166295080748, 0.024627007994502348, 0.027185201584098963, 0.025770637282040484, 0.026210746300912465, 0.022948869892315895, 0.0254249283532648, 0.02513241907858042, 0.02547277655529869, 0.024997253107730558, 0.02678582271373102, 0.025994602792539838, 0.025194243609474122, 0.025160928533166572, 0.02643916141849774, 0.024568370694034797, 0.025540607363726254, 0.02523463865664708, 0.02817382014878868, 0.0262981737889239, 0.024399230591068438, 0.02562607989096545, 0.026157262881537947], 0.0004: [0.026278809181384158, 0.023705060013434486, 0.025593750923595643, 0.027324727791684846, 0.02460185573394865, 0.025028450420065518, 0.02551170178752627, 0.025140118417647895, 0.026120831898361084, 0.025370592288728188, 0.02511751862762368, 0.025592705450443558, 0.0258336896284066, 0.02403530665156527, 0.026249751076931352, 0.02577598462605336, 0.02570996779771605, 0.027508587698045945, 0.03280113583281415, 0.027151143507208403, 0.02668410157563735, 0.025600523190456772, 0.026449868263461202, 0.025828051571499346, 0.025364522461555572, 0.028217100775980285, 0.02708445101954288, 0.026910354294855637, 0.027114719188376945, 0.02553808566276814, 0.02528065429497454, 0.0245330685511983, 0.026881638546792118, 0.02627113357860628, 0.026417797403714076, 0.02682299967109052, 0.02556626528407549, 0.026438970473879622, 0.02763970633029099, 0.025504754230280566, 0.025093239730704868, 0.026834253668066584, 0.02918990797194646, 0.026541325412220824, 0.026575941802486094, 0.027197788508183364, 0.026163435080959176, 0.026935687168426724]}\n",
      "avg_val_losses_lr 0.0004: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05], 0.0003: [7.147101700592104e-05, 7.175913446052961e-05, 7.356194255871746e-05, 7.294296242889691e-05, 6.941043457798035e-05, 7.279371268674175e-05, 7.795851725973521e-05, 7.283193435988389e-05, 7.224721181434947e-05, 6.883762457064956e-05, 7.399633995263816e-05, 7.02345769795554e-05, 6.57084739924276e-05, 7.590978487808853e-05, 7.045950961923684e-05, 7.14319060869694e-05, 7.099130575700965e-05, 7.431145560559152e-05, 7.205841880238586e-05, 7.08982383737029e-05, 7.290821930121144e-05, 7.681089674952094e-05, 8.573729864139874e-05, 7.378158848371398e-05, 7.468032236188085e-05, 7.376013737412684e-05, 7.307717505787047e-05, 8.066825395875063e-05, 7.647073377460085e-05, 7.777669525493313e-05, 6.809753677245073e-05, 7.544489125597864e-05, 7.45769112124048e-05, 7.558687405133142e-05, 7.417582524549127e-05, 7.948315345320777e-05, 7.713531985916866e-05, 7.476036679369176e-05, 7.466150900049428e-05, 7.845448492135828e-05, 7.290317713363441e-05, 7.578815241461797e-05, 7.48802334025136e-05, 8.360184020412072e-05, 7.803612400274154e-05, 7.240127771830397e-05, 7.60417800918856e-05, 7.76179907464034e-05], 0.0004: [7.797866225930017e-05, 7.034142437220916e-05, 7.594584843796927e-05, 8.108227831360489e-05, 7.30025392698773e-05, 7.426839887259797e-05, 7.570237919147262e-05, 7.459975791586913e-05, 7.750988693875693e-05, 7.528365664311035e-05, 7.453269622440262e-05, 7.594274614374943e-05, 7.665783272524214e-05, 7.132138472274561e-05, 7.789243643006336e-05, 7.64866012642533e-05, 7.629070563120489e-05, 8.162785667075949e-05, 9.733274727838027e-05, 8.056719141604868e-05, 7.918131031346394e-05, 7.596594418533167e-05, 7.848625597466232e-05, 7.664110258605147e-05, 7.526564528651505e-05, 8.373026936492666e-05, 8.036929085917769e-05, 7.985268336752414e-05, 8.045910738390785e-05, 7.578066962245739e-05, 7.501677832336659e-05, 7.279842300058843e-05, 7.976747343261756e-05, 7.795588598992961e-05, 7.839109021873613e-05, 7.95934708340965e-05, 7.586428867678186e-05, 7.845391832011757e-05, 8.20169327308338e-05, 7.568176329460109e-05, 7.446065201989575e-05, 7.96268654838771e-05, 8.661693760221501e-05, 7.875764217276209e-05, 7.886036143170947e-05, 8.070560388184974e-05, 7.763630587821714e-05, 7.99278550991891e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0005 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.0074815452098846436\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "Validation loss decreased (inf --> 0.000075).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.006879424210637808\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "Validation loss decreased (0.000075 --> 0.000075).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.008213376626372337\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.007112367078661919\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.007262929808348417\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.011202006600797176\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.005384325981140137\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.009582402184605598\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.012266318313777447\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.007216357626020908\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.006397253833711147\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.004335983190685511\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.007143120747059584\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.005918652750551701\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.008106083609163761\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.007674534805119038\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.008635109290480614\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.006725674960762262\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.008512977510690689\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000085\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.0087036844342947\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.006979087367653847\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.005664338357746601\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.010141556151211262\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000088\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.006261680740863085\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.005697628948837519\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.00791213195770979\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.005032079294323921\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.00778971565887332\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.008541594259440899\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.007267800625413656\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.008787116967141628\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.006283381953835487\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000085\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.007089323364198208\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.006684092804789543\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000089\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.004455968271940947\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.006608912721276283\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.004375845659524202\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.008585839532315731\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.004428722895681858\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.00846245139837265\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.004883878398686647\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.005337852519005537\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.006850047502666712\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000085\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.006861372385174036\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.006643902510404587\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.0055657983757555485\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000087\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.007252922747284174\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.00504509499296546\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000005 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.0005: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706], 0.0003: [0.012649923188811448, 0.012886979752303365, 0.013064661253121746, 0.012776669194170703, 0.012717449952885684, 0.012622895056708876, 0.012493407813530033, 0.012345176194295551, 0.01228418961892159, 0.011904442857485261, 0.011872770472282787, 0.01176221937861382, 0.011755546346345597, 0.01147983250398066, 0.011406897356209809, 0.011273715252866086, 0.011117836107760454, 0.011100410664983509, 0.010980995508448586, 0.010884919475453591, 0.010663047040171014, 0.010613769883916362, 0.010488587144036075, 0.010372721744629191, 0.010250856866020638, 0.010216511461337724, 0.010188323876458802, 0.010019190751855981, 0.009914184962759651, 0.00978488870394012, 0.009763344133500036, 0.009616679967231392, 0.009524154194791464, 0.009595767747669012, 0.009324085868554112, 0.009346448879417322, 0.009228157250444862, 0.009081872637103684, 0.009101319909816443, 0.00890715618897092, 0.009007162021589459, 0.008798028554294773, 0.008824452554801524, 0.008752015123588669, 0.008672068164395634, 0.008579593918035412, 0.00853371461804878, 0.008454515820111341], 0.0004: [0.010461589537520737, 0.010591399666043883, 0.010611402501776234, 0.010521375114053853, 0.010463566356477154, 0.010185533027868687, 0.010282703834014856, 0.010240158361349514, 0.010141924851701513, 0.010020689212112863, 0.009825635712851577, 0.009849205724181282, 0.009697626133856817, 0.009704186213812567, 0.009605649036876409, 0.009570111616215262, 0.009373299329170205, 0.009318887470899855, 0.009483160142172861, 0.009012243430507142, 0.009241844184413947, 0.00910941833526956, 0.009012784126493248, 0.008988859170217653, 0.008882108767076187, 0.00878490531457437, 0.008673132161458773, 0.008606571863354128, 0.008647260404922297, 0.008644623380990073, 0.008487314101720194, 0.008351002751234257, 0.008405402253196217, 0.008290212967473209, 0.008252372652523708, 0.008173055193938574, 0.008112938052363471, 0.008104167956411225, 0.008026105214369085, 0.00802645246984044, 0.007950051948206398, 0.007910032287500585, 0.00782730131357398, 0.007763153648002547, 0.0077853449440139924, 0.007676174854644897, 0.007547849814727667, 0.007528017229993745], 0.0005: [0.008789883170504168, 0.009048614191366747, 0.00888007563688525, 0.008791782076774059, 0.008777922894729446, 0.008899526938919376, 0.008736818974977623, 0.008692446014936751, 0.008698188055339926, 0.008553287297781063, 0.008638999422795952, 0.008446284491496734, 0.00851975236929277, 0.008296450813282886, 0.008188020724663249, 0.008312198863374465, 0.00826485762098898, 0.008049722087889668, 0.008124918190463048, 0.008022059768076625, 0.007893119166380665, 0.007898263378592679, 0.00785035024212751, 0.007894374405753168, 0.007769841016159352, 0.007724415458055559, 0.007632197650396154, 0.007695974487606223, 0.0075009479171837656, 0.007611211390515877, 0.007501576964430218, 0.007417386832544461, 0.007427872373747431, 0.0073592548024386025, 0.00727199624331358, 0.0073228126162749015, 0.007132597597297883, 0.007277253147289485, 0.007088739172883966, 0.00717156446141113, 0.007080962069137184, 0.007078796061423851, 0.006964098615363976, 0.0069505628889601195, 0.0069488317671389825, 0.006881832833078949, 0.006857989698757943, 0.006811914339085058]}\n",
      "avg_train_losses_lr for LR 0.0005: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06], 0.0003: [9.412145229770422e-06, 9.588526601416194e-06, 9.720730099048918e-06, 9.506450293281773e-06, 9.462388357801848e-06, 9.392035012432199e-06, 9.29569033744794e-06, 9.185398954088952e-06, 9.140022037888088e-06, 8.857472364200343e-06, 8.833906601400884e-06, 8.751651323373378e-06, 8.746686269602378e-06, 8.541542041652277e-06, 8.487274818608488e-06, 8.388180991715838e-06, 8.272199484940813e-06, 8.259234125731777e-06, 8.170383562833768e-06, 8.098898419236304e-06, 7.933814762032005e-06, 7.897150211247294e-06, 7.804008291693509e-06, 7.717798917134816e-06, 7.627125644360593e-06, 7.601571027781045e-06, 7.58059812236518e-06, 7.454755023702367e-06, 7.376625716339026e-06, 7.280423142812589e-06, 7.264392956473241e-06, 7.155267832761452e-06, 7.0864242520769825e-06, 7.139708145587062e-06, 6.937563890293238e-06, 6.954203035280746e-06, 6.866188430390522e-06, 6.757345712130717e-06, 6.771815409089615e-06, 6.627348354889077e-06, 6.701757456539776e-06, 6.546152198135992e-06, 6.565812912798753e-06, 6.511916014574903e-06, 6.452431669937227e-06, 6.383626427109681e-06, 6.349490043191056e-06, 6.290562366154271e-06], 0.0004: [7.783920786845786e-06, 7.880505703901698e-06, 7.895388766202554e-06, 7.82840410271864e-06, 7.785391634283596e-06, 7.578521598116582e-06, 7.650821305070577e-06, 7.619165447432674e-06, 7.546075038468388e-06, 7.455869949488737e-06, 7.310740857776471e-06, 7.3282780685872635e-06, 7.215495635310132e-06, 7.220376647181969e-06, 7.147060295294947e-06, 7.120618762064927e-06, 6.974181048489736e-06, 6.9336960348957255e-06, 7.055922724830998e-06, 6.705538266746385e-06, 6.876372161022282e-06, 6.777841023266041e-06, 6.7059405703074755e-06, 6.688139263554801e-06, 6.60871188026502e-06, 6.536387882867836e-06, 6.45322333441873e-06, 6.403699303090869e-06, 6.4339735155671854e-06, 6.4320114441890425e-06, 6.314965849494192e-06, 6.213543713715965e-06, 6.254019533628138e-06, 6.168313219846138e-06, 6.14015822360395e-06, 6.081142257394772e-06, 6.036412241341869e-06, 6.029886872329781e-06, 5.971804474976998e-06, 5.972062849583661e-06, 5.915217223367856e-06, 5.885440690104602e-06, 5.82388490593302e-06, 5.7761559880971325e-06, 5.79266736905803e-06, 5.7114396239917395e-06, 5.615959683577133e-06, 5.6012032961262985e-06], 0.0005: [6.5400916447203634e-06, 6.732599844766925e-06, 6.60719913458724e-06, 6.54150452140927e-06, 6.531192630007028e-06, 6.621671829553107e-06, 6.50060935638216e-06, 6.467593761113654e-06, 6.471866112604112e-06, 6.364053048944243e-06, 6.427826951485084e-06, 6.284437865696975e-06, 6.339101465247597e-06, 6.1729544741688135e-06, 6.09227732489825e-06, 6.184671773344096e-06, 6.149447634664419e-06, 5.989376553489337e-06, 6.04532603457072e-06, 5.968794470295108e-06, 5.872856522604662e-06, 5.876684061452886e-06, 5.841034406344873e-06, 5.873790480471107e-06, 5.781131708451899e-06, 5.747332930100862e-06, 5.678718489878091e-06, 5.72617149375463e-06, 5.581062438380778e-06, 5.663103713181456e-06, 5.5815304794867695e-06, 5.518889012309867e-06, 5.526690754276362e-06, 5.4756360137191985e-06, 5.4107114905607e-06, 5.448521291871206e-06, 5.306992259894259e-06, 5.414622877447534e-06, 5.274359503633903e-06, 5.335985462359472e-06, 5.268572968108024e-06, 5.26696135522608e-06, 5.181620993574387e-06, 5.171549768571517e-06, 5.170261731502219e-06, 5.120411334136122e-06, 5.102670906813946e-06, 5.068388645152573e-06]}\n",
      "val_losses_lr for LR 0.0005: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397], 0.0003: [0.02408573273099539, 0.024182828313198478, 0.024790374642287782, 0.02458177833853826, 0.02339131645277938, 0.024531481175431972, 0.026272020316530764, 0.02454436187928087, 0.02434731038143577, 0.023198279480308902, 0.024936766564039058, 0.023669052442110167, 0.022143755735448103, 0.025581597503915834, 0.02374485474168281, 0.024072552351308688, 0.023924070040112252, 0.025042960539084343, 0.024283687136404036, 0.023892706331937876, 0.024570069904508256, 0.02588527220458856, 0.028893469642151375, 0.024864395319011613, 0.025167268635953845, 0.024857166295080748, 0.024627007994502348, 0.027185201584098963, 0.025770637282040484, 0.026210746300912465, 0.022948869892315895, 0.0254249283532648, 0.02513241907858042, 0.02547277655529869, 0.024997253107730558, 0.02678582271373102, 0.025994602792539838, 0.025194243609474122, 0.025160928533166572, 0.02643916141849774, 0.024568370694034797, 0.025540607363726254, 0.02523463865664708, 0.02817382014878868, 0.0262981737889239, 0.024399230591068438, 0.02562607989096545, 0.026157262881537947], 0.0004: [0.026278809181384158, 0.023705060013434486, 0.025593750923595643, 0.027324727791684846, 0.02460185573394865, 0.025028450420065518, 0.02551170178752627, 0.025140118417647895, 0.026120831898361084, 0.025370592288728188, 0.02511751862762368, 0.025592705450443558, 0.0258336896284066, 0.02403530665156527, 0.026249751076931352, 0.02577598462605336, 0.02570996779771605, 0.027508587698045945, 0.03280113583281415, 0.027151143507208403, 0.02668410157563735, 0.025600523190456772, 0.026449868263461202, 0.025828051571499346, 0.025364522461555572, 0.028217100775980285, 0.02708445101954288, 0.026910354294855637, 0.027114719188376945, 0.02553808566276814, 0.02528065429497454, 0.0245330685511983, 0.026881638546792118, 0.02627113357860628, 0.026417797403714076, 0.02682299967109052, 0.02556626528407549, 0.026438970473879622, 0.02763970633029099, 0.025504754230280566, 0.025093239730704868, 0.026834253668066584, 0.02918990797194646, 0.026541325412220824, 0.026575941802486094, 0.027197788508183364, 0.026163435080959176, 0.026935687168426724], 0.0005: [0.02527916379610091, 0.02514258610880562, 0.028413676432933573, 0.02686609791105376, 0.028326619591961904, 0.027051072881366347, 0.027109326608202682, 0.02745808831496215, 0.02541812302721583, 0.02671324428673888, 0.027157794324216892, 0.025804830315724597, 0.02650717484905694, 0.02520277152716759, 0.027384185609449174, 0.02732416913079137, 0.02587174626553669, 0.026525488015518534, 0.028676950220759247, 0.027252156064964454, 0.02789947874042639, 0.02610189392084935, 0.029667815724139065, 0.027844984816022496, 0.027360009778434693, 0.027961524121894406, 0.026830828751406976, 0.027943416313442347, 0.026379798540364655, 0.02617802257782252, 0.02818457393019905, 0.02879811234866168, 0.027944417011258077, 0.02999386070945607, 0.028367818327681413, 0.028316541648129967, 0.02731681890365632, 0.027186335417516635, 0.028839515169767408, 0.027411237748710102, 0.027570377518519627, 0.02827075477661868, 0.028481300578256437, 0.028955331076356658, 0.027331789138862292, 0.02941816027527704, 0.026254913110551305, 0.027651364048694206]}\n",
      "avg_val_losses_lr 0.0005: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05], 0.0003: [7.147101700592104e-05, 7.175913446052961e-05, 7.356194255871746e-05, 7.294296242889691e-05, 6.941043457798035e-05, 7.279371268674175e-05, 7.795851725973521e-05, 7.283193435988389e-05, 7.224721181434947e-05, 6.883762457064956e-05, 7.399633995263816e-05, 7.02345769795554e-05, 6.57084739924276e-05, 7.590978487808853e-05, 7.045950961923684e-05, 7.14319060869694e-05, 7.099130575700965e-05, 7.431145560559152e-05, 7.205841880238586e-05, 7.08982383737029e-05, 7.290821930121144e-05, 7.681089674952094e-05, 8.573729864139874e-05, 7.378158848371398e-05, 7.468032236188085e-05, 7.376013737412684e-05, 7.307717505787047e-05, 8.066825395875063e-05, 7.647073377460085e-05, 7.777669525493313e-05, 6.809753677245073e-05, 7.544489125597864e-05, 7.45769112124048e-05, 7.558687405133142e-05, 7.417582524549127e-05, 7.948315345320777e-05, 7.713531985916866e-05, 7.476036679369176e-05, 7.466150900049428e-05, 7.845448492135828e-05, 7.290317713363441e-05, 7.578815241461797e-05, 7.48802334025136e-05, 8.360184020412072e-05, 7.803612400274154e-05, 7.240127771830397e-05, 7.60417800918856e-05, 7.76179907464034e-05], 0.0004: [7.797866225930017e-05, 7.034142437220916e-05, 7.594584843796927e-05, 8.108227831360489e-05, 7.30025392698773e-05, 7.426839887259797e-05, 7.570237919147262e-05, 7.459975791586913e-05, 7.750988693875693e-05, 7.528365664311035e-05, 7.453269622440262e-05, 7.594274614374943e-05, 7.665783272524214e-05, 7.132138472274561e-05, 7.789243643006336e-05, 7.64866012642533e-05, 7.629070563120489e-05, 8.162785667075949e-05, 9.733274727838027e-05, 8.056719141604868e-05, 7.918131031346394e-05, 7.596594418533167e-05, 7.848625597466232e-05, 7.664110258605147e-05, 7.526564528651505e-05, 8.373026936492666e-05, 8.036929085917769e-05, 7.985268336752414e-05, 8.045910738390785e-05, 7.578066962245739e-05, 7.501677832336659e-05, 7.279842300058843e-05, 7.976747343261756e-05, 7.795588598992961e-05, 7.839109021873613e-05, 7.95934708340965e-05, 7.586428867678186e-05, 7.845391832011757e-05, 8.20169327308338e-05, 7.568176329460109e-05, 7.446065201989575e-05, 7.96268654838771e-05, 8.661693760221501e-05, 7.875764217276209e-05, 7.886036143170947e-05, 8.070560388184974e-05, 7.763630587821714e-05, 7.99278550991891e-05], 0.0005: [7.501235547804425e-05, 7.46070804415597e-05, 8.431357991968419e-05, 7.972135878650968e-05, 8.405525101472374e-05, 8.027024593877254e-05, 8.04431056623225e-05, 8.147800686932388e-05, 7.542469741013599e-05, 7.926778720100558e-05, 8.058692677809167e-05, 7.657219678256557e-05, 7.865630519007994e-05, 7.478567218744092e-05, 8.125871100726758e-05, 8.10806205661465e-05, 7.677076043185961e-05, 7.871064693032205e-05, 8.509480777673367e-05, 8.08669319435147e-05, 8.2787770743105e-05, 7.745369115979035e-05, 8.803506149596162e-05, 8.262606770333085e-05, 8.118697263630473e-05, 8.297188166734245e-05, 7.96167025264302e-05, 8.291814929804851e-05, 7.82783339476696e-05, 7.767959221905793e-05, 8.363375053471529e-05, 8.545433931353614e-05, 8.292111872776877e-05, 8.900255403399427e-05, 8.417750245602793e-05, 8.402534613688418e-05, 8.105880980313448e-05, 8.067161844960426e-05, 8.557719634945819e-05, 8.13389844175374e-05, 8.181120925376744e-05, 8.38894800493136e-05, 8.451424503933661e-05, 8.59208637280613e-05, 8.110323186606022e-05, 8.729424414028795e-05, 7.790775403724424e-05, 8.205152536704512e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.001 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.004736827686429024\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "Validation loss decreased (inf --> 0.000078).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.008608536794781685\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000076\n",
      "Validation loss decreased (0.000078 --> 0.000076).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.013670423068106174\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000075\n",
      "Validation loss decreased (0.000076 --> 0.000075).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.010394702665507793\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "Validation loss decreased (0.000075 --> 0.000073).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.013784577138721943\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000073 --> 0.000070).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.015156515873968601\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000070 --> 0.000070).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.012526123784482479\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.009030526503920555\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.011463147588074207\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.011608261615037918\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.008408275432884693\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.009983038529753685\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.007925284095108509\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.00892383698374033\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.007968355901539326\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.009503206238150597\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.007806042209267616\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.009157678112387657\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.012117591686546803\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000070 --> 0.000069).  Saving model ...\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.0075485254637897015\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.008443964645266533\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.012542149983346462\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.007677587680518627\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.008812514133751392\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.007059855852276087\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.008322757668793201\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.011410537175834179\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.005884489975869656\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.006186502054333687\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.007946806028485298\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.013075204566121101\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.004845512565225363\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.006502171047031879\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.008531400002539158\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.010239987634122372\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.00726688327267766\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.00772106135264039\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.01031386200338602\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.004366779699921608\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.003439014544710517\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.008306560106575489\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.011311445385217667\n",
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.005095872096717358\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.005709898192435503\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.007955365814268589\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.0070915836840868\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.00645607290789485\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.007315904833376408\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000006 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.001: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706], 0.0003: [0.012649923188811448, 0.012886979752303365, 0.013064661253121746, 0.012776669194170703, 0.012717449952885684, 0.012622895056708876, 0.012493407813530033, 0.012345176194295551, 0.01228418961892159, 0.011904442857485261, 0.011872770472282787, 0.01176221937861382, 0.011755546346345597, 0.01147983250398066, 0.011406897356209809, 0.011273715252866086, 0.011117836107760454, 0.011100410664983509, 0.010980995508448586, 0.010884919475453591, 0.010663047040171014, 0.010613769883916362, 0.010488587144036075, 0.010372721744629191, 0.010250856866020638, 0.010216511461337724, 0.010188323876458802, 0.010019190751855981, 0.009914184962759651, 0.00978488870394012, 0.009763344133500036, 0.009616679967231392, 0.009524154194791464, 0.009595767747669012, 0.009324085868554112, 0.009346448879417322, 0.009228157250444862, 0.009081872637103684, 0.009101319909816443, 0.00890715618897092, 0.009007162021589459, 0.008798028554294773, 0.008824452554801524, 0.008752015123588669, 0.008672068164395634, 0.008579593918035412, 0.00853371461804878, 0.008454515820111341], 0.0004: [0.010461589537520737, 0.010591399666043883, 0.010611402501776234, 0.010521375114053853, 0.010463566356477154, 0.010185533027868687, 0.010282703834014856, 0.010240158361349514, 0.010141924851701513, 0.010020689212112863, 0.009825635712851577, 0.009849205724181282, 0.009697626133856817, 0.009704186213812567, 0.009605649036876409, 0.009570111616215262, 0.009373299329170205, 0.009318887470899855, 0.009483160142172861, 0.009012243430507142, 0.009241844184413947, 0.00910941833526956, 0.009012784126493248, 0.008988859170217653, 0.008882108767076187, 0.00878490531457437, 0.008673132161458773, 0.008606571863354128, 0.008647260404922297, 0.008644623380990073, 0.008487314101720194, 0.008351002751234257, 0.008405402253196217, 0.008290212967473209, 0.008252372652523708, 0.008173055193938574, 0.008112938052363471, 0.008104167956411225, 0.008026105214369085, 0.00802645246984044, 0.007950051948206398, 0.007910032287500585, 0.00782730131357398, 0.007763153648002547, 0.0077853449440139924, 0.007676174854644897, 0.007547849814727667, 0.007528017229993745], 0.0005: [0.008789883170504168, 0.009048614191366747, 0.00888007563688525, 0.008791782076774059, 0.008777922894729446, 0.008899526938919376, 0.008736818974977623, 0.008692446014936751, 0.008698188055339926, 0.008553287297781063, 0.008638999422795952, 0.008446284491496734, 0.00851975236929277, 0.008296450813282886, 0.008188020724663249, 0.008312198863374465, 0.00826485762098898, 0.008049722087889668, 0.008124918190463048, 0.008022059768076625, 0.007893119166380665, 0.007898263378592679, 0.00785035024212751, 0.007894374405753168, 0.007769841016159352, 0.007724415458055559, 0.007632197650396154, 0.007695974487606223, 0.0075009479171837656, 0.007611211390515877, 0.007501576964430218, 0.007417386832544461, 0.007427872373747431, 0.0073592548024386025, 0.00727199624331358, 0.0073228126162749015, 0.007132597597297883, 0.007277253147289485, 0.007088739172883966, 0.00717156446141113, 0.007080962069137184, 0.007078796061423851, 0.006964098615363976, 0.0069505628889601195, 0.0069488317671389825, 0.006881832833078949, 0.006857989698757943, 0.006811914339085058], 0.001: [0.012026074351437974, 0.012518779589819508, 0.012471754391819027, 0.012412033371219867, 0.01262411367081638, 0.01227698710932772, 0.012317956126588824, 0.012083887493991793, 0.012040229196524425, 0.011823293321607973, 0.011787003017074995, 0.011472040396169865, 0.01149483757485758, 0.011310782743315772, 0.011205762037813365, 0.010989914981889469, 0.010919607644956121, 0.010772406999811718, 0.010721892584461728, 0.010589818101531525, 0.010517213644648313, 0.010321376116605389, 0.010320563853816748, 0.010097683627377374, 0.010002121891635147, 0.00990553206654676, 0.009928920673181506, 0.009756113239702591, 0.00960016198881757, 0.009605895377240985, 0.009467301187257924, 0.009343275784582744, 0.009226579565923487, 0.009255779547813632, 0.00909139030530546, 0.009040015634950348, 0.008909890486739597, 0.008830375428217298, 0.008904297698228172, 0.008682574463732448, 0.00867909464674692, 0.008603272445422283, 0.008476578401834021, 0.008541997164727602, 0.008297473587022283, 0.008366975311045759, 0.008235776918376193, 0.008248005784393863]}\n",
      "avg_train_losses_lr for LR 0.001: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06], 0.0003: [9.412145229770422e-06, 9.588526601416194e-06, 9.720730099048918e-06, 9.506450293281773e-06, 9.462388357801848e-06, 9.392035012432199e-06, 9.29569033744794e-06, 9.185398954088952e-06, 9.140022037888088e-06, 8.857472364200343e-06, 8.833906601400884e-06, 8.751651323373378e-06, 8.746686269602378e-06, 8.541542041652277e-06, 8.487274818608488e-06, 8.388180991715838e-06, 8.272199484940813e-06, 8.259234125731777e-06, 8.170383562833768e-06, 8.098898419236304e-06, 7.933814762032005e-06, 7.897150211247294e-06, 7.804008291693509e-06, 7.717798917134816e-06, 7.627125644360593e-06, 7.601571027781045e-06, 7.58059812236518e-06, 7.454755023702367e-06, 7.376625716339026e-06, 7.280423142812589e-06, 7.264392956473241e-06, 7.155267832761452e-06, 7.0864242520769825e-06, 7.139708145587062e-06, 6.937563890293238e-06, 6.954203035280746e-06, 6.866188430390522e-06, 6.757345712130717e-06, 6.771815409089615e-06, 6.627348354889077e-06, 6.701757456539776e-06, 6.546152198135992e-06, 6.565812912798753e-06, 6.511916014574903e-06, 6.452431669937227e-06, 6.383626427109681e-06, 6.349490043191056e-06, 6.290562366154271e-06], 0.0004: [7.783920786845786e-06, 7.880505703901698e-06, 7.895388766202554e-06, 7.82840410271864e-06, 7.785391634283596e-06, 7.578521598116582e-06, 7.650821305070577e-06, 7.619165447432674e-06, 7.546075038468388e-06, 7.455869949488737e-06, 7.310740857776471e-06, 7.3282780685872635e-06, 7.215495635310132e-06, 7.220376647181969e-06, 7.147060295294947e-06, 7.120618762064927e-06, 6.974181048489736e-06, 6.9336960348957255e-06, 7.055922724830998e-06, 6.705538266746385e-06, 6.876372161022282e-06, 6.777841023266041e-06, 6.7059405703074755e-06, 6.688139263554801e-06, 6.60871188026502e-06, 6.536387882867836e-06, 6.45322333441873e-06, 6.403699303090869e-06, 6.4339735155671854e-06, 6.4320114441890425e-06, 6.314965849494192e-06, 6.213543713715965e-06, 6.254019533628138e-06, 6.168313219846138e-06, 6.14015822360395e-06, 6.081142257394772e-06, 6.036412241341869e-06, 6.029886872329781e-06, 5.971804474976998e-06, 5.972062849583661e-06, 5.915217223367856e-06, 5.885440690104602e-06, 5.82388490593302e-06, 5.7761559880971325e-06, 5.79266736905803e-06, 5.7114396239917395e-06, 5.615959683577133e-06, 5.6012032961262985e-06], 0.0005: [6.5400916447203634e-06, 6.732599844766925e-06, 6.60719913458724e-06, 6.54150452140927e-06, 6.531192630007028e-06, 6.621671829553107e-06, 6.50060935638216e-06, 6.467593761113654e-06, 6.471866112604112e-06, 6.364053048944243e-06, 6.427826951485084e-06, 6.284437865696975e-06, 6.339101465247597e-06, 6.1729544741688135e-06, 6.09227732489825e-06, 6.184671773344096e-06, 6.149447634664419e-06, 5.989376553489337e-06, 6.04532603457072e-06, 5.968794470295108e-06, 5.872856522604662e-06, 5.876684061452886e-06, 5.841034406344873e-06, 5.873790480471107e-06, 5.781131708451899e-06, 5.747332930100862e-06, 5.678718489878091e-06, 5.72617149375463e-06, 5.581062438380778e-06, 5.663103713181456e-06, 5.5815304794867695e-06, 5.518889012309867e-06, 5.526690754276362e-06, 5.4756360137191985e-06, 5.4107114905607e-06, 5.448521291871206e-06, 5.306992259894259e-06, 5.414622877447534e-06, 5.274359503633903e-06, 5.335985462359472e-06, 5.268572968108024e-06, 5.26696135522608e-06, 5.181620993574387e-06, 5.171549768571517e-06, 5.170261731502219e-06, 5.120411334136122e-06, 5.102670906813946e-06, 5.068388645152573e-06], 0.001: [8.947971987677063e-06, 9.314568147187134e-06, 9.279579160579633e-06, 9.235143877395734e-06, 9.392941719357425e-06, 9.134663027773602e-06, 9.165145927521446e-06, 8.990987718743894e-06, 8.958503866461625e-06, 8.797093245244028e-06, 8.770091530561753e-06, 8.535744342388292e-06, 8.55270652891189e-06, 8.415760969728997e-06, 8.337620563849229e-06, 8.177020075810617e-06, 8.124708069163781e-06, 8.015183779621814e-06, 7.977598649153072e-06, 7.87932894459191e-06, 7.825307771315708e-06, 7.6795953248552e-06, 7.67899096266127e-06, 7.51315746084626e-06, 7.442054978895199e-06, 7.370187549513958e-06, 7.387589786593382e-06, 7.259012827159666e-06, 7.142977670251169e-06, 7.147243584256685e-06, 7.044122907185955e-06, 6.951842101624065e-06, 6.865014557978785e-06, 6.886740734980381e-06, 6.764427310495133e-06, 6.726202109338056e-06, 6.629382802633629e-06, 6.57021981266168e-06, 6.625221501657866e-06, 6.460248856943786e-06, 6.4576597074009825e-06, 6.401244379034437e-06, 6.306977977555075e-06, 6.355652652327085e-06, 6.173715466534437e-06, 6.2254280588138086e-06, 6.1278102071251435e-06, 6.136909065769244e-06]}\n",
      "val_losses_lr for LR 0.001: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397], 0.0003: [0.02408573273099539, 0.024182828313198478, 0.024790374642287782, 0.02458177833853826, 0.02339131645277938, 0.024531481175431972, 0.026272020316530764, 0.02454436187928087, 0.02434731038143577, 0.023198279480308902, 0.024936766564039058, 0.023669052442110167, 0.022143755735448103, 0.025581597503915834, 0.02374485474168281, 0.024072552351308688, 0.023924070040112252, 0.025042960539084343, 0.024283687136404036, 0.023892706331937876, 0.024570069904508256, 0.02588527220458856, 0.028893469642151375, 0.024864395319011613, 0.025167268635953845, 0.024857166295080748, 0.024627007994502348, 0.027185201584098963, 0.025770637282040484, 0.026210746300912465, 0.022948869892315895, 0.0254249283532648, 0.02513241907858042, 0.02547277655529869, 0.024997253107730558, 0.02678582271373102, 0.025994602792539838, 0.025194243609474122, 0.025160928533166572, 0.02643916141849774, 0.024568370694034797, 0.025540607363726254, 0.02523463865664708, 0.02817382014878868, 0.0262981737889239, 0.024399230591068438, 0.02562607989096545, 0.026157262881537947], 0.0004: [0.026278809181384158, 0.023705060013434486, 0.025593750923595643, 0.027324727791684846, 0.02460185573394865, 0.025028450420065518, 0.02551170178752627, 0.025140118417647895, 0.026120831898361084, 0.025370592288728188, 0.02511751862762368, 0.025592705450443558, 0.0258336896284066, 0.02403530665156527, 0.026249751076931352, 0.02577598462605336, 0.02570996779771605, 0.027508587698045945, 0.03280113583281415, 0.027151143507208403, 0.02668410157563735, 0.025600523190456772, 0.026449868263461202, 0.025828051571499346, 0.025364522461555572, 0.028217100775980285, 0.02708445101954288, 0.026910354294855637, 0.027114719188376945, 0.02553808566276814, 0.02528065429497454, 0.0245330685511983, 0.026881638546792118, 0.02627113357860628, 0.026417797403714076, 0.02682299967109052, 0.02556626528407549, 0.026438970473879622, 0.02763970633029099, 0.025504754230280566, 0.025093239730704868, 0.026834253668066584, 0.02918990797194646, 0.026541325412220824, 0.026575941802486094, 0.027197788508183364, 0.026163435080959176, 0.026935687168426724], 0.0005: [0.02527916379610091, 0.02514258610880562, 0.028413676432933573, 0.02686609791105376, 0.028326619591961904, 0.027051072881366347, 0.027109326608202682, 0.02745808831496215, 0.02541812302721583, 0.02671324428673888, 0.027157794324216892, 0.025804830315724597, 0.02650717484905694, 0.02520277152716759, 0.027384185609449174, 0.02732416913079137, 0.02587174626553669, 0.026525488015518534, 0.028676950220759247, 0.027252156064964454, 0.02789947874042639, 0.02610189392084935, 0.029667815724139065, 0.027844984816022496, 0.027360009778434693, 0.027961524121894406, 0.026830828751406976, 0.027943416313442347, 0.026379798540364655, 0.02617802257782252, 0.02818457393019905, 0.02879811234866168, 0.027944417011258077, 0.02999386070945607, 0.028367818327681413, 0.028316541648129967, 0.02731681890365632, 0.027186335417516635, 0.028839515169767408, 0.027411237748710102, 0.027570377518519627, 0.02827075477661868, 0.028481300578256437, 0.028955331076356658, 0.027331789138862292, 0.02941816027527704, 0.026254913110551305, 0.027651364048694206], 0.001: [0.02620126713117036, 0.02559806210975486, 0.025223773967706857, 0.02470194388357841, 0.023714631248696615, 0.02349511381979336, 0.026456863251832517, 0.024494181601302994, 0.02612646533043652, 0.025087478348172817, 0.02475468637504634, 0.02474065638459579, 0.02611848174867935, 0.02689167099814166, 0.02361199321800109, 0.024557756891512815, 0.02790197314838228, 0.02829739106100505, 0.02311581571086223, 0.026050826187801013, 0.024667522752552514, 0.025131096632305523, 0.025651760577591744, 0.025477266241727213, 0.025543272007499944, 0.025221387890253206, 0.026001260313274505, 0.026269552843694956, 0.02521556090060566, 0.027088156077756543, 0.026643078358296726, 0.02552265261275036, 0.026377228471431178, 0.027135710969832583, 0.026522723758919304, 0.026243876002574173, 0.025778985496621446, 0.027086113813092783, 0.02770619933805719, 0.027138577875037293, 0.027910162560719133, 0.02627040803824891, 0.02687691568151028, 0.026064836208705817, 0.027261187245057855, 0.02519379653042121, 0.024953027142526858, 0.025451373999789126]}\n",
      "avg_val_losses_lr 0.001: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05], 0.0003: [7.147101700592104e-05, 7.175913446052961e-05, 7.356194255871746e-05, 7.294296242889691e-05, 6.941043457798035e-05, 7.279371268674175e-05, 7.795851725973521e-05, 7.283193435988389e-05, 7.224721181434947e-05, 6.883762457064956e-05, 7.399633995263816e-05, 7.02345769795554e-05, 6.57084739924276e-05, 7.590978487808853e-05, 7.045950961923684e-05, 7.14319060869694e-05, 7.099130575700965e-05, 7.431145560559152e-05, 7.205841880238586e-05, 7.08982383737029e-05, 7.290821930121144e-05, 7.681089674952094e-05, 8.573729864139874e-05, 7.378158848371398e-05, 7.468032236188085e-05, 7.376013737412684e-05, 7.307717505787047e-05, 8.066825395875063e-05, 7.647073377460085e-05, 7.777669525493313e-05, 6.809753677245073e-05, 7.544489125597864e-05, 7.45769112124048e-05, 7.558687405133142e-05, 7.417582524549127e-05, 7.948315345320777e-05, 7.713531985916866e-05, 7.476036679369176e-05, 7.466150900049428e-05, 7.845448492135828e-05, 7.290317713363441e-05, 7.578815241461797e-05, 7.48802334025136e-05, 8.360184020412072e-05, 7.803612400274154e-05, 7.240127771830397e-05, 7.60417800918856e-05, 7.76179907464034e-05], 0.0004: [7.797866225930017e-05, 7.034142437220916e-05, 7.594584843796927e-05, 8.108227831360489e-05, 7.30025392698773e-05, 7.426839887259797e-05, 7.570237919147262e-05, 7.459975791586913e-05, 7.750988693875693e-05, 7.528365664311035e-05, 7.453269622440262e-05, 7.594274614374943e-05, 7.665783272524214e-05, 7.132138472274561e-05, 7.789243643006336e-05, 7.64866012642533e-05, 7.629070563120489e-05, 8.162785667075949e-05, 9.733274727838027e-05, 8.056719141604868e-05, 7.918131031346394e-05, 7.596594418533167e-05, 7.848625597466232e-05, 7.664110258605147e-05, 7.526564528651505e-05, 8.373026936492666e-05, 8.036929085917769e-05, 7.985268336752414e-05, 8.045910738390785e-05, 7.578066962245739e-05, 7.501677832336659e-05, 7.279842300058843e-05, 7.976747343261756e-05, 7.795588598992961e-05, 7.839109021873613e-05, 7.95934708340965e-05, 7.586428867678186e-05, 7.845391832011757e-05, 8.20169327308338e-05, 7.568176329460109e-05, 7.446065201989575e-05, 7.96268654838771e-05, 8.661693760221501e-05, 7.875764217276209e-05, 7.886036143170947e-05, 8.070560388184974e-05, 7.763630587821714e-05, 7.99278550991891e-05], 0.0005: [7.501235547804425e-05, 7.46070804415597e-05, 8.431357991968419e-05, 7.972135878650968e-05, 8.405525101472374e-05, 8.027024593877254e-05, 8.04431056623225e-05, 8.147800686932388e-05, 7.542469741013599e-05, 7.926778720100558e-05, 8.058692677809167e-05, 7.657219678256557e-05, 7.865630519007994e-05, 7.478567218744092e-05, 8.125871100726758e-05, 8.10806205661465e-05, 7.677076043185961e-05, 7.871064693032205e-05, 8.509480777673367e-05, 8.08669319435147e-05, 8.2787770743105e-05, 7.745369115979035e-05, 8.803506149596162e-05, 8.262606770333085e-05, 8.118697263630473e-05, 8.297188166734245e-05, 7.96167025264302e-05, 8.291814929804851e-05, 7.82783339476696e-05, 7.767959221905793e-05, 8.363375053471529e-05, 8.545433931353614e-05, 8.292111872776877e-05, 8.900255403399427e-05, 8.417750245602793e-05, 8.402534613688418e-05, 8.105880980313448e-05, 8.067161844960426e-05, 8.557719634945819e-05, 8.13389844175374e-05, 8.181120925376744e-05, 8.38894800493136e-05, 8.451424503933661e-05, 8.59208637280613e-05, 8.110323186606022e-05, 8.729424414028795e-05, 7.790775403724424e-05, 8.205152536704512e-05], 0.001: [7.774856715480819e-05, 7.595864127523697e-05, 7.484799396945654e-05, 7.329953674652347e-05, 7.036982566378817e-05, 6.971843863440167e-05, 7.85070126167137e-05, 7.268303145787238e-05, 7.752660335441103e-05, 7.444355592929619e-05, 7.345604265592386e-05, 7.341441063678275e-05, 7.750291320082894e-05, 7.979724331792777e-05, 7.006526177448395e-05, 7.287168217066117e-05, 8.279517254712842e-05, 8.396851946885772e-05, 6.859292495804817e-05, 7.730215485994366e-05, 7.319739689184722e-05, 7.457298703948227e-05, 7.611798390976779e-05, 7.560019656298877e-05, 7.579605936943603e-05, 7.48409136209294e-05, 7.71550751135742e-05, 7.795119538188415e-05, 7.48236228504619e-05, 8.038028509720042e-05, 7.905957969820988e-05, 7.573487422181116e-05, 7.827070763035958e-05, 8.05213975365952e-05, 7.870244438848458e-05, 7.787500297499755e-05, 7.649550592469272e-05, 8.037422496466701e-05, 8.221424135922015e-05, 8.052990467370116e-05, 8.281947347394402e-05, 7.795373305118371e-05, 7.975345899557947e-05, 7.734372762227246e-05, 8.089373069750105e-05, 7.475904014961784e-05, 7.40445909273794e-05, 7.552336498453747e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.002 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.007038480136543512\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000110\n",
      "Validation loss decreased (inf --> 0.000110).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.01418260671198368\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000110 --> 0.000071).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.01456630602478981\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.013326971791684628\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.012040587142109871\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.012957925908267498\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000071 --> 0.000068).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.009730270132422447\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.01714034005999565\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.015634668990969658\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.010599764063954353\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.019354432821273804\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.012437378987669945\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.016436299309134483\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.012760919518768787\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000070\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.013559853658080101\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.014270855113863945\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.01199597492814064\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.009407172910869122\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.01500871405005455\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.009068683721125126\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.013306597247719765\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.010564770549535751\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.008489619009196758\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.006862113252282143\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000071\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.010538389906287193\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.007352043874561787\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.008018880151212215\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.008411343209445477\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.00822476390749216\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.01149060670286417\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.008448568172752857\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.010962151922285557\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.008624715730547905\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.00853784754872322\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.00965671706944704\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.008263522759079933\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000072\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "############# Epoch 37: Training Start   #############\n",
      "Epoch: 37, Training Loss:  0.01032700389623642\n",
      "############# Epoch 37: Training End     #############\n",
      "############# Epoch 37: Validation Start   #############\n",
      "############# Epoch 37: Validation End     #############\n",
      "Epoch: 37 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 37  Done   #############\n",
      "\n",
      "############# Epoch 38: Training Start   #############\n",
      "Epoch: 38, Training Loss:  0.010935742408037186\n",
      "############# Epoch 38: Training End     #############\n",
      "############# Epoch 38: Validation Start   #############\n",
      "############# Epoch 38: Validation End     #############\n",
      "Epoch: 38 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 38  Done   #############\n",
      "\n",
      "############# Epoch 39: Training Start   #############\n",
      "Epoch: 39, Training Loss:  0.006551187951117754\n",
      "############# Epoch 39: Training End     #############\n",
      "############# Epoch 39: Validation Start   #############\n",
      "############# Epoch 39: Validation End     #############\n",
      "Epoch: 39 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 39  Done   #############\n",
      "\n",
      "############# Epoch 40: Training Start   #############\n",
      "Epoch: 40, Training Loss:  0.013845125213265419\n",
      "############# Epoch 40: Training End     #############\n",
      "############# Epoch 40: Validation Start   #############\n",
      "############# Epoch 40: Validation End     #############\n",
      "Epoch: 40 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 40  Done   #############\n",
      "\n",
      "############# Epoch 41: Training Start   #############\n",
      "Epoch: 41, Training Loss:  0.014903848990797997\n",
      "############# Epoch 41: Training End     #############\n",
      "############# Epoch 41: Validation Start   #############\n",
      "############# Epoch 41: Validation End     #############\n",
      "Epoch: 41 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 41  Done   #############\n",
      "\n",
      "############# Epoch 42: Training Start   #############\n",
      "Epoch: 42, Training Loss:  0.006701566744595766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 42: Training End     #############\n",
      "############# Epoch 42: Validation Start   #############\n",
      "############# Epoch 42: Validation End     #############\n",
      "Epoch: 42 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 42  Done   #############\n",
      "\n",
      "############# Epoch 43: Training Start   #############\n",
      "Epoch: 43, Training Loss:  0.010005778633058071\n",
      "############# Epoch 43: Training End     #############\n",
      "############# Epoch 43: Validation Start   #############\n",
      "############# Epoch 43: Validation End     #############\n",
      "Epoch: 43 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 43  Done   #############\n",
      "\n",
      "############# Epoch 44: Training Start   #############\n",
      "Epoch: 44, Training Loss:  0.006141210440546274\n",
      "############# Epoch 44: Training End     #############\n",
      "############# Epoch 44: Validation Start   #############\n",
      "############# Epoch 44: Validation End     #############\n",
      "Epoch: 44 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 44  Done   #############\n",
      "\n",
      "############# Epoch 45: Training Start   #############\n",
      "Epoch: 45, Training Loss:  0.010839027352631092\n",
      "############# Epoch 45: Training End     #############\n",
      "############# Epoch 45: Validation Start   #############\n",
      "############# Epoch 45: Validation End     #############\n",
      "Epoch: 45 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 45  Done   #############\n",
      "\n",
      "############# Epoch 46: Training Start   #############\n",
      "Epoch: 46, Training Loss:  0.006917737424373627\n",
      "############# Epoch 46: Training End     #############\n",
      "############# Epoch 46: Validation Start   #############\n",
      "############# Epoch 46: Validation End     #############\n",
      "Epoch: 46 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 46  Done   #############\n",
      "\n",
      "############# Epoch 47: Training Start   #############\n",
      "Epoch: 47, Training Loss:  0.006644065957516432\n",
      "############# Epoch 47: Training End     #############\n",
      "############# Epoch 47: Validation Start   #############\n",
      "############# Epoch 47: Validation End     #############\n",
      "Epoch: 47 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000088\n",
      "############# Epoch 47  Done   #############\n",
      "\n",
      "############# Epoch 48: Training Start   #############\n",
      "Epoch: 48, Training Loss:  0.008929365314543247\n",
      "############# Epoch 48: Training End     #############\n",
      "############# Epoch 48: Validation Start   #############\n",
      "############# Epoch 48: Validation End     #############\n",
      "Epoch: 48 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 48  Done   #############\n",
      "\n",
      "train_losses_lr for LR 0.002: \n",
      " {1e-05: [0.13964022392229658, 0.06065570294491151, 0.05406757868504859, 0.05080964959259808, 0.04864223342987568, 0.046853033304657905, 0.04545395249234776, 0.04413121883823934, 0.043106021888975284, 0.04206315056182491, 0.04109347571197544, 0.04032163242144253, 0.039370355387550904, 0.03856576665670473, 0.03785253563269399, 0.03708079868062792, 0.036315675768058886, 0.03564451538286887, 0.03487366718694655, 0.034122175943117505, 0.033528990063738745, 0.03286272523386983, 0.032126810290806364, 0.03155370980745096, 0.030932757073536602, 0.030259046648141177, 0.029638649443984925, 0.0291492653362054, 0.028427220979383883, 0.027945689779395835, 0.02733821056262102, 0.02677649236284199, 0.026333938088057434, 0.025730526140597218, 0.02519958774930619, 0.02480403110287926, 0.02427531187311702, 0.02372145648058945, 0.0232940815109205, 0.022935157774121553, 0.02251347460592765, 0.02212012514826796, 0.021630988749426004, 0.02125358774459786, 0.020798907789174426, 0.020470826085546026, 0.020125630322062174, 0.01978948735909182], 2e-05: [0.02178240589827985, 0.02150940257083023, 0.021123746126596368, 0.02073381000289895, 0.020212067918258792, 0.019827512516972744, 0.01939307587079903, 0.01886495557569322, 0.018489181593481813, 0.01806525156592084, 0.017705853477569436, 0.01739162643137942, 0.016965991254047713, 0.01669353260824988, 0.01637281963901066, 0.01589962125927701, 0.01575552232333436, 0.01543065604484928, 0.015105951357067425, 0.014865280194499063, 0.014659404956286636, 0.014444177474485653, 0.014146656644048301, 0.013937599774743884, 0.013693893494616662, 0.013569362808041104, 0.013329547232258075, 0.013230369231938061, 0.012914144504369068, 0.012765024407902597, 0.012640270096155636, 0.012455612049268977, 0.012252303787265995, 0.012126508474812857, 0.011928667668793682, 0.011862640957198365, 0.011667988761300443, 0.011489675243085782, 0.011447819084250592, 0.011267618593375954, 0.011137907993023484, 0.011022238776994124, 0.011000862970485319, 0.010759517379310185, 0.010681316580476491, 0.010551299174992547, 0.01043697433765158, 0.010320276186879104], 3e-05: [0.012059614737587416, 0.01214892999684838, 0.012111463446109118, 0.01213669021498867, 0.012013924614057911, 0.011816467732950564, 0.011682346102870283, 0.011677014732932926, 0.011394445958320005, 0.011411754950781228, 0.01121641078324977, 0.011194553439314149, 0.011032917405481436, 0.010792243116330138, 0.0107928460856783, 0.010797032136386936, 0.010610258071946651, 0.010492788940609898, 0.010406712949458913, 0.010244372958758113, 0.010154668216966378, 0.010109998663773724, 0.010029124021544005, 0.009981501978472796, 0.009827565369916887, 0.009773836203315863, 0.009616585645181608, 0.009614726970453154, 0.009536699035379577, 0.00942774826918531, 0.00941483836348717, 0.009288264379609331, 0.009206275624386582, 0.009128799356923463, 0.009135408839245506, 0.008967262699332517, 0.008937555638979003, 0.008887834187459828, 0.008889112941514666, 0.008719081592668458, 0.008692794363187645, 0.008576314766590553, 0.008592452589904756, 0.008469578159877408, 0.008382343731465779, 0.008475791206798209, 0.00839788082729293, 0.008249215592687036], 4e-05: [0.009463309960188966, 0.009675847566367822, 0.009519185095823123, 0.009577815794424785, 0.009552444957426768, 0.009434076528123118, 0.009426469206768427, 0.009364824737401387, 0.009276013545743238, 0.009314526168704365, 0.009185199980926153, 0.009141288947527451, 0.009172407473670324, 0.008979592006133841, 0.00891595046327675, 0.008951088946646682, 0.008825972085192224, 0.008806665304118286, 0.008740256341601088, 0.008651528905396659, 0.008661501620303009, 0.008573490385537109, 0.00852492057795947, 0.008375720243098591, 0.008423464821957867, 0.008305643797419708, 0.008355128991665682, 0.008171353616316566, 0.008180563246631722, 0.008173690000451943, 0.008137582714837102, 0.008039532698603588, 0.007990631550845671, 0.007896267929027086, 0.007988883547070473, 0.007834190015446867, 0.00778265837098367, 0.0077857859637117015, 0.007653376515182095, 0.007654208466105745, 0.007657962299812679, 0.007560504429940411, 0.007575055810385874, 0.007522460281463622, 0.0074606985938208675, 0.0074450295623613594, 0.007388548371853258, 0.007316307616704194], 5e-05: [0.008291527594251212, 0.008303494704921937, 0.00843515341832592, 0.008400268621092737, 0.008249995868113931, 0.008297464345170227, 0.008320059753064655, 0.008273100129805418, 0.008185952708507617, 0.008158324628658148, 0.008176257527555311, 0.008045277347264368, 0.008022264346696567, 0.007983134273672489, 0.007962631476749221, 0.007852961054249175, 0.0077781778324736255, 0.007911646871174795, 0.00782530058980531, 0.007685242114391815, 0.007753988282361421, 0.007606832825764462, 0.007686574813435852, 0.007579089124365478, 0.0074109342006392675, 0.007491715231437756, 0.007455418334540719, 0.00741090036639694, 0.007357638546853274, 0.007311311426400655, 0.007295096929391593, 0.007268135824969722, 0.007212660249933836, 0.007244740203965374, 0.007078603888484868, 0.00707273300161302, 0.0070857644882164, 0.007127275353836708, 0.00699668288715266, 0.006989521737913101, 0.0069592000793255405, 0.006920767775833753, 0.006871942090150526, 0.006898841699391285, 0.006946817567603868, 0.006790366511094831, 0.006738000615206108, 0.006670786506306493], 0.0001: [0.011245088873866283, 0.011709389121235647, 0.01180335737306105, 0.011760002770253252, 0.011731541324393002, 0.011643699645425127, 0.011561199048758557, 0.011501562388037284, 0.011558090551434792, 0.01122187777183994, 0.011191483137996065, 0.011125363794452, 0.0110090918017141, 0.01099429714668077, 0.010799180215967458, 0.010705913205345575, 0.010736895935094382, 0.010588881245071421, 0.010345733072123659, 0.010360301441063969, 0.010379244030142799, 0.010064400149747679, 0.010110044227552682, 0.010073012126667889, 0.009794127179789239, 0.009838217460214979, 0.009779435398307949, 0.009818805623667905, 0.009541544622888533, 0.00957021133113553, 0.0094354659034122, 0.009423958967610283, 0.009220738639885992, 0.009233873941411737, 0.009158780509529788, 0.009110389727408913, 0.009076003556228465, 0.00886536196152778, 0.008881036132238696, 0.008892693791581434, 0.008624320813916198, 0.008776066402788283, 0.008696499576693729, 0.008520685824589154, 0.008521364672647228, 0.00845615755402141, 0.008412697779325136, 0.008387401815568985], 0.0002: [0.01470564263304585, 0.015362603183824666, 0.015253341147659458, 0.015220393466214382, 0.01495860301899182, 0.015098191790803829, 0.014716639045564943, 0.014673781321367395, 0.014336798120071072, 0.014153991516823644, 0.013995580780222864, 0.013863001996166224, 0.01362811190345036, 0.013368173829457238, 0.013278744465510712, 0.013115410026027067, 0.012840014853974848, 0.012814998591368606, 0.012484641231172388, 0.012391469486049442, 0.012250216354524508, 0.01217390878168989, 0.011917103773157587, 0.01187379493820204, 0.01164768430386347, 0.011559835144518213, 0.011373721632740585, 0.01141107811452544, 0.011184396751992794, 0.011070416008546347, 0.01101085570588197, 0.010793656862612508, 0.010678176185701034, 0.010665295387817802, 0.010481297006966398, 0.010292797560049674, 0.0102147485803081, 0.01024800092030395, 0.010093643372653923, 0.010026392641607571, 0.009826513980355936, 0.00986557418807587, 0.009702976289623808, 0.009594398939779711, 0.009511534962560604, 0.009413945781748865, 0.009414330328928331, 0.009253891142309706], 0.0003: [0.012649923188811448, 0.012886979752303365, 0.013064661253121746, 0.012776669194170703, 0.012717449952885684, 0.012622895056708876, 0.012493407813530033, 0.012345176194295551, 0.01228418961892159, 0.011904442857485261, 0.011872770472282787, 0.01176221937861382, 0.011755546346345597, 0.01147983250398066, 0.011406897356209809, 0.011273715252866086, 0.011117836107760454, 0.011100410664983509, 0.010980995508448586, 0.010884919475453591, 0.010663047040171014, 0.010613769883916362, 0.010488587144036075, 0.010372721744629191, 0.010250856866020638, 0.010216511461337724, 0.010188323876458802, 0.010019190751855981, 0.009914184962759651, 0.00978488870394012, 0.009763344133500036, 0.009616679967231392, 0.009524154194791464, 0.009595767747669012, 0.009324085868554112, 0.009346448879417322, 0.009228157250444862, 0.009081872637103684, 0.009101319909816443, 0.00890715618897092, 0.009007162021589459, 0.008798028554294773, 0.008824452554801524, 0.008752015123588669, 0.008672068164395634, 0.008579593918035412, 0.00853371461804878, 0.008454515820111341], 0.0004: [0.010461589537520737, 0.010591399666043883, 0.010611402501776234, 0.010521375114053853, 0.010463566356477154, 0.010185533027868687, 0.010282703834014856, 0.010240158361349514, 0.010141924851701513, 0.010020689212112863, 0.009825635712851577, 0.009849205724181282, 0.009697626133856817, 0.009704186213812567, 0.009605649036876409, 0.009570111616215262, 0.009373299329170205, 0.009318887470899855, 0.009483160142172861, 0.009012243430507142, 0.009241844184413947, 0.00910941833526956, 0.009012784126493248, 0.008988859170217653, 0.008882108767076187, 0.00878490531457437, 0.008673132161458773, 0.008606571863354128, 0.008647260404922297, 0.008644623380990073, 0.008487314101720194, 0.008351002751234257, 0.008405402253196217, 0.008290212967473209, 0.008252372652523708, 0.008173055193938574, 0.008112938052363471, 0.008104167956411225, 0.008026105214369085, 0.00802645246984044, 0.007950051948206398, 0.007910032287500585, 0.00782730131357398, 0.007763153648002547, 0.0077853449440139924, 0.007676174854644897, 0.007547849814727667, 0.007528017229993745], 0.0005: [0.008789883170504168, 0.009048614191366747, 0.00888007563688525, 0.008791782076774059, 0.008777922894729446, 0.008899526938919376, 0.008736818974977623, 0.008692446014936751, 0.008698188055339926, 0.008553287297781063, 0.008638999422795952, 0.008446284491496734, 0.00851975236929277, 0.008296450813282886, 0.008188020724663249, 0.008312198863374465, 0.00826485762098898, 0.008049722087889668, 0.008124918190463048, 0.008022059768076625, 0.007893119166380665, 0.007898263378592679, 0.00785035024212751, 0.007894374405753168, 0.007769841016159352, 0.007724415458055559, 0.007632197650396154, 0.007695974487606223, 0.0075009479171837656, 0.007611211390515877, 0.007501576964430218, 0.007417386832544461, 0.007427872373747431, 0.0073592548024386025, 0.00727199624331358, 0.0073228126162749015, 0.007132597597297883, 0.007277253147289485, 0.007088739172883966, 0.00717156446141113, 0.007080962069137184, 0.007078796061423851, 0.006964098615363976, 0.0069505628889601195, 0.0069488317671389825, 0.006881832833078949, 0.006857989698757943, 0.006811914339085058], 0.001: [0.012026074351437974, 0.012518779589819508, 0.012471754391819027, 0.012412033371219867, 0.01262411367081638, 0.01227698710932772, 0.012317956126588824, 0.012083887493991793, 0.012040229196524425, 0.011823293321607973, 0.011787003017074995, 0.011472040396169865, 0.01149483757485758, 0.011310782743315772, 0.011205762037813365, 0.010989914981889469, 0.010919607644956121, 0.010772406999811718, 0.010721892584461728, 0.010589818101531525, 0.010517213644648313, 0.010321376116605389, 0.010320563853816748, 0.010097683627377374, 0.010002121891635147, 0.00990553206654676, 0.009928920673181506, 0.009756113239702591, 0.00960016198881757, 0.009605895377240985, 0.009467301187257924, 0.009343275784582744, 0.009226579565923487, 0.009255779547813632, 0.00909139030530546, 0.009040015634950348, 0.008909890486739597, 0.008830375428217298, 0.008904297698228172, 0.008682574463732448, 0.00867909464674692, 0.008603272445422283, 0.008476578401834021, 0.008541997164727602, 0.008297473587022283, 0.008366975311045759, 0.008235776918376193, 0.008248005784393863], 0.002: [0.015073084877041669, 0.01563143432438314, 0.015452069228991791, 0.015522474988496734, 0.015149079394870498, 0.015004806518499232, 0.014844083262557576, 0.014594416227260971, 0.014521604602729584, 0.014114945790511462, 0.013976186664617012, 0.013553918877386497, 0.013438867282751576, 0.013175110829511795, 0.013142276118395827, 0.012832454851983744, 0.012733998787076593, 0.01244064329373596, 0.012198620908228425, 0.012148371746228643, 0.01202325104289534, 0.011816131234116362, 0.011660598087820247, 0.011499567943073576, 0.011368849859753118, 0.011200890755433297, 0.011077418490285283, 0.010919713054179395, 0.010807581045983608, 0.010691974165854925, 0.010539724990805887, 0.01061270220331303, 0.010278440793280453, 0.010146149590409121, 0.010249639394584965, 0.009945498008511048, 0.009922580529140135, 0.009780983247591881, 0.009870497543445152, 0.00955589382803217, 0.009597562818574561, 0.009423093568329934, 0.009383105939161324, 0.009282715031336661, 0.009232639425179162, 0.009180031467015695, 0.009117004409442122, 0.009002641216717342]}\n",
      "avg_train_losses_lr for LR 0.002: \n",
      " {1e-05: [0.00010389897613266115, 4.513073135782106e-05, 4.02288531882802e-05, 3.780479880401643e-05, 3.6192137968657497e-05, 3.4860887875489515e-05, 3.381990512823494e-05, 3.2835728302261415e-05, 3.207293295310661e-05, 3.129698702516734e-05, 3.057550276188649e-05, 3.0001214599287596e-05, 2.929341918716585e-05, 2.8694766857667207e-05, 2.8164089012421122e-05, 2.7589879970705298e-05, 2.702059208932953e-05, 2.6521216802729814e-05, 2.594766903790666e-05, 2.5388523767200524e-05, 2.494716522599609e-05, 2.4451432465676954e-05, 2.390387670446902e-05, 2.3477462654353392e-05, 2.3015444251143304e-05, 2.2514171613200282e-05, 2.2052566550584023e-05, 2.1688441470390924e-05, 2.1151206085851104e-05, 2.0792923943002853e-05, 2.034093047814064e-05, 1.992298538901934e-05, 1.959370393456654e-05, 1.9144736711753883e-05, 1.874969326585282e-05, 1.8455380284880404e-05, 1.8061988000831117e-05, 1.764989321472429e-05, 1.7331905886101562e-05, 1.706484953431663e-05, 1.67510971770295e-05, 1.645842644960414e-05, 1.6094485676656253e-05, 1.5813681357587693e-05, 1.547537781932621e-05, 1.5231269408888412e-05, 1.4974427322962926e-05, 1.4724320951705224e-05], 2e-05: [1.6207147245743938e-05, 1.6004019769962968e-05, 1.5717073010860394e-05, 1.542694196644267e-05, 1.503874101060922e-05, 1.4752613479890435e-05, 1.442937192767785e-05, 1.4036425279533648e-05, 1.3756831542769205e-05, 1.3441407415119672e-05, 1.3173998123191544e-05, 1.2940198237633496e-05, 1.2623505397356929e-05, 1.2420783190662114e-05, 1.218215746950198e-05, 1.1830075341723966e-05, 1.1722858871528541e-05, 1.1481142890512857e-05, 1.1239547140675167e-05, 1.1060476335192755e-05, 1.0907295354379938e-05, 1.0747155858992302e-05, 1.0525786193488319e-05, 1.0370237927636818e-05, 1.0188908850161208e-05, 1.0096252089316298e-05, 9.9178178811444e-06, 9.844024726144391e-06, 9.6087384705127e-06, 9.49778601778467e-06, 9.40496286916342e-06, 9.267568489039417e-06, 9.116297460763388e-06, 9.02269975804528e-06, 8.875496777376252e-06, 8.826369759820212e-06, 8.681539256919973e-06, 8.54886550824835e-06, 8.517722532924548e-06, 8.383644786738061e-06, 8.287133923380568e-06, 8.201070518596818e-06, 8.18516590065872e-06, 8.005593288177222e-06, 7.94740816999739e-06, 7.850669029012311e-06, 7.76560590599076e-06, 7.678776924761238e-06], 3e-05: [8.972927632133494e-06, 9.039382438131235e-06, 9.01150554025976e-06, 9.030275457580855e-06, 8.938932004507375e-06, 8.792014682254884e-06, 8.692221802730866e-06, 8.688255009622712e-06, 8.478010385654765e-06, 8.490889100283651e-06, 8.345543737537031e-06, 8.329280832823028e-06, 8.209015926697497e-06, 8.029942794888495e-06, 8.030391432796354e-06, 8.033506053859327e-06, 7.894537255912687e-06, 7.807134628429984e-06, 7.743089992156929e-06, 7.6223013085997865e-06, 7.555556709052364e-06, 7.522320434355449e-06, 7.462145849363099e-06, 7.426712781601783e-06, 7.312176614521493e-06, 7.272199556038589e-06, 7.155197652664887e-06, 7.153814710158597e-06, 7.095758210847899e-06, 7.014693652667641e-06, 7.005088068070811e-06, 6.910910996733133e-06, 6.849907458620969e-06, 6.792261426282339e-06, 6.797179195867191e-06, 6.672070460812885e-06, 6.649966993287949e-06, 6.612971865669515e-06, 6.6139233195793645e-06, 6.4874118993068885e-06, 6.467852948800331e-06, 6.381186582284638e-06, 6.393193891298181e-06, 6.3017694641945005e-06, 6.236862895435848e-06, 6.306392266962953e-06, 6.248423234592954e-06, 6.137809220749283e-06], 4e-05: [7.041153244188219e-06, 7.199291344023677e-06, 7.082727005820776e-06, 7.126351037518441e-06, 7.107473926656821e-06, 7.019402178663034e-06, 7.013741969321746e-06, 6.967875548661747e-06, 6.901795792963719e-06, 6.930451018381224e-06, 6.8342261762843394e-06, 6.801554276434115e-06, 6.824707941718991e-06, 6.681244052182917e-06, 6.633891713747581e-06, 6.660036418635925e-06, 6.566943515768024e-06, 6.552578351278486e-06, 6.503166920834143e-06, 6.437149483182038e-06, 6.444569657963548e-06, 6.37908510828654e-06, 6.3429468586007965e-06, 6.231934704686452e-06, 6.267458944909127e-06, 6.179794492127759e-06, 6.216613833084584e-06, 6.079876202616493e-06, 6.086728606124793e-06, 6.0816145836696e-06, 6.054749043777606e-06, 5.981795162651479e-06, 5.9454103800935055e-06, 5.875199351954677e-06, 5.944109782046482e-06, 5.829010428159871e-06, 5.7906684307914205e-06, 5.792995508714064e-06, 5.6944765737962016e-06, 5.695095584900108e-06, 5.697888615932053e-06, 5.625375319896139e-06, 5.636202239870442e-06, 5.59706866180329e-06, 5.551115025164336e-06, 5.539456519614107e-06, 5.497431824295579e-06, 5.443681262428716e-06], 5e-05: [6.169291364770248e-06, 6.178195464971679e-06, 6.276155817206786e-06, 6.250199866884477e-06, 6.138389782822865e-06, 6.173708590156419e-06, 6.190520649601678e-06, 6.15558045372427e-06, 6.090738622401501e-06, 6.070182015370646e-06, 6.083524946097702e-06, 5.986069454809798e-06, 5.968946686530183e-06, 5.939832048863459e-06, 5.924576991628885e-06, 5.842976974887779e-06, 5.787334696780971e-06, 5.886642017243151e-06, 5.82239627217657e-06, 5.7181860970177195e-06, 5.769336519614153e-06, 5.65984585250332e-06, 5.719177688568343e-06, 5.639203217533838e-06, 5.5140879469042165e-06, 5.5741928805340445e-06, 5.547186260818987e-06, 5.514062772616771e-06, 5.474433442599162e-06, 5.439963858929059e-06, 5.427899501035412e-06, 5.4078391554834245e-06, 5.3665626859626754e-06, 5.390431699378998e-06, 5.266818369408384e-06, 5.262450150009687e-06, 5.272146196589583e-06, 5.303032257318979e-06, 5.205865243417158e-06, 5.200537007375819e-06, 5.17797624949817e-06, 5.14938078559059e-06, 5.113052150409617e-06, 5.133066740618515e-06, 5.16876307113383e-06, 5.052356035040797e-06, 5.013393314885497e-06, 4.963382817192331e-06], 0.0001: [8.36688160257908e-06, 8.712343096157476e-06, 8.782259950194233e-06, 8.750002061200336e-06, 8.728825390173364e-06, 8.663466998084171e-06, 8.602082625564404e-06, 8.557710110146789e-06, 8.59976975553184e-06, 8.349611437380908e-06, 8.326996382437548e-06, 8.277800442300595e-06, 8.191288542942039e-06, 8.180280615089858e-06, 8.03510432735674e-06, 7.96570923016784e-06, 7.988761856469034e-06, 7.878631878773378e-06, 7.697718059615817e-06, 7.708557619839263e-06, 7.722651808141964e-06, 7.488392968562261e-06, 7.522354335976698e-06, 7.4948006894850366e-06, 7.287297008771755e-06, 7.320102276945669e-06, 7.276365623741033e-06, 7.305658946181477e-06, 7.0993635586968255e-06, 7.120692954713937e-06, 7.020435940038839e-06, 7.011874231852889e-06, 6.860668630867554e-06, 6.870441920693257e-06, 6.814568831495378e-06, 6.778563785274489e-06, 6.752978836479512e-06, 6.5962514594700745e-06, 6.607913788868077e-06, 6.61658764254571e-06, 6.416905367497171e-06, 6.529811311598424e-06, 6.470609804087596e-06, 6.339796000438359e-06, 6.340301095719663e-06, 6.291783894361168e-06, 6.259447752474059e-06, 6.24062635086978e-06], 0.0002: [1.0941698387682925e-05, 1.1430508321298115e-05, 1.1349212163437097e-05, 1.1324697519504748e-05, 1.112991296055939e-05, 1.1233773653871897e-05, 1.094988024223582e-05, 1.0917992054588835e-05, 1.0667260506005263e-05, 1.053124368811283e-05, 1.041337855671344e-05, 1.0314733628099869e-05, 1.0139964213876755e-05, 9.94655790882235e-06, 9.880018203504995e-06, 9.75848960269871e-06, 9.553582480636047e-06, 9.534969190006403e-06, 9.28916758271755e-06, 9.219843367596311e-06, 9.114744311402164e-06, 9.057967843519263e-06, 8.866892688361299e-06, 8.834668852828898e-06, 8.666431773707938e-06, 8.601067815861765e-06, 8.46259050055103e-06, 8.490385501879047e-06, 8.321723773804162e-06, 8.236916673025556e-06, 8.19260097163837e-06, 8.03099468944383e-06, 7.94507156674184e-06, 7.93548763974539e-06, 7.798584082564284e-06, 7.658331517894103e-06, 7.600259360348288e-06, 7.625000684749962e-06, 7.5101513189389304e-06, 7.460113572624681e-06, 7.311394330621976e-06, 7.340456985175498e-06, 7.219476405970095e-06, 7.1386896873360945e-06, 7.077034942381402e-06, 7.00442394475362e-06, 7.004710066166913e-06, 6.885335671361389e-06], 0.0003: [9.412145229770422e-06, 9.588526601416194e-06, 9.720730099048918e-06, 9.506450293281773e-06, 9.462388357801848e-06, 9.392035012432199e-06, 9.29569033744794e-06, 9.185398954088952e-06, 9.140022037888088e-06, 8.857472364200343e-06, 8.833906601400884e-06, 8.751651323373378e-06, 8.746686269602378e-06, 8.541542041652277e-06, 8.487274818608488e-06, 8.388180991715838e-06, 8.272199484940813e-06, 8.259234125731777e-06, 8.170383562833768e-06, 8.098898419236304e-06, 7.933814762032005e-06, 7.897150211247294e-06, 7.804008291693509e-06, 7.717798917134816e-06, 7.627125644360593e-06, 7.601571027781045e-06, 7.58059812236518e-06, 7.454755023702367e-06, 7.376625716339026e-06, 7.280423142812589e-06, 7.264392956473241e-06, 7.155267832761452e-06, 7.0864242520769825e-06, 7.139708145587062e-06, 6.937563890293238e-06, 6.954203035280746e-06, 6.866188430390522e-06, 6.757345712130717e-06, 6.771815409089615e-06, 6.627348354889077e-06, 6.701757456539776e-06, 6.546152198135992e-06, 6.565812912798753e-06, 6.511916014574903e-06, 6.452431669937227e-06, 6.383626427109681e-06, 6.349490043191056e-06, 6.290562366154271e-06], 0.0004: [7.783920786845786e-06, 7.880505703901698e-06, 7.895388766202554e-06, 7.82840410271864e-06, 7.785391634283596e-06, 7.578521598116582e-06, 7.650821305070577e-06, 7.619165447432674e-06, 7.546075038468388e-06, 7.455869949488737e-06, 7.310740857776471e-06, 7.3282780685872635e-06, 7.215495635310132e-06, 7.220376647181969e-06, 7.147060295294947e-06, 7.120618762064927e-06, 6.974181048489736e-06, 6.9336960348957255e-06, 7.055922724830998e-06, 6.705538266746385e-06, 6.876372161022282e-06, 6.777841023266041e-06, 6.7059405703074755e-06, 6.688139263554801e-06, 6.60871188026502e-06, 6.536387882867836e-06, 6.45322333441873e-06, 6.403699303090869e-06, 6.4339735155671854e-06, 6.4320114441890425e-06, 6.314965849494192e-06, 6.213543713715965e-06, 6.254019533628138e-06, 6.168313219846138e-06, 6.14015822360395e-06, 6.081142257394772e-06, 6.036412241341869e-06, 6.029886872329781e-06, 5.971804474976998e-06, 5.972062849583661e-06, 5.915217223367856e-06, 5.885440690104602e-06, 5.82388490593302e-06, 5.7761559880971325e-06, 5.79266736905803e-06, 5.7114396239917395e-06, 5.615959683577133e-06, 5.6012032961262985e-06], 0.0005: [6.5400916447203634e-06, 6.732599844766925e-06, 6.60719913458724e-06, 6.54150452140927e-06, 6.531192630007028e-06, 6.621671829553107e-06, 6.50060935638216e-06, 6.467593761113654e-06, 6.471866112604112e-06, 6.364053048944243e-06, 6.427826951485084e-06, 6.284437865696975e-06, 6.339101465247597e-06, 6.1729544741688135e-06, 6.09227732489825e-06, 6.184671773344096e-06, 6.149447634664419e-06, 5.989376553489337e-06, 6.04532603457072e-06, 5.968794470295108e-06, 5.872856522604662e-06, 5.876684061452886e-06, 5.841034406344873e-06, 5.873790480471107e-06, 5.781131708451899e-06, 5.747332930100862e-06, 5.678718489878091e-06, 5.72617149375463e-06, 5.581062438380778e-06, 5.663103713181456e-06, 5.5815304794867695e-06, 5.518889012309867e-06, 5.526690754276362e-06, 5.4756360137191985e-06, 5.4107114905607e-06, 5.448521291871206e-06, 5.306992259894259e-06, 5.414622877447534e-06, 5.274359503633903e-06, 5.335985462359472e-06, 5.268572968108024e-06, 5.26696135522608e-06, 5.181620993574387e-06, 5.171549768571517e-06, 5.170261731502219e-06, 5.120411334136122e-06, 5.102670906813946e-06, 5.068388645152573e-06], 0.001: [8.947971987677063e-06, 9.314568147187134e-06, 9.279579160579633e-06, 9.235143877395734e-06, 9.392941719357425e-06, 9.134663027773602e-06, 9.165145927521446e-06, 8.990987718743894e-06, 8.958503866461625e-06, 8.797093245244028e-06, 8.770091530561753e-06, 8.535744342388292e-06, 8.55270652891189e-06, 8.415760969728997e-06, 8.337620563849229e-06, 8.177020075810617e-06, 8.124708069163781e-06, 8.015183779621814e-06, 7.977598649153072e-06, 7.87932894459191e-06, 7.825307771315708e-06, 7.6795953248552e-06, 7.67899096266127e-06, 7.51315746084626e-06, 7.442054978895199e-06, 7.370187549513958e-06, 7.387589786593382e-06, 7.259012827159666e-06, 7.142977670251169e-06, 7.147243584256685e-06, 7.044122907185955e-06, 6.951842101624065e-06, 6.865014557978785e-06, 6.886740734980381e-06, 6.764427310495133e-06, 6.726202109338056e-06, 6.629382802633629e-06, 6.57021981266168e-06, 6.625221501657866e-06, 6.460248856943786e-06, 6.4576597074009825e-06, 6.401244379034437e-06, 6.306977977555075e-06, 6.355652652327085e-06, 6.173715466534437e-06, 6.2254280588138086e-06, 6.1278102071251435e-06, 6.136909065769244e-06], 0.002: [1.1215092914465527e-05, 1.1630531491356504e-05, 1.1497075319190321e-05, 1.1549460556917213e-05, 1.127163645451674e-05, 1.1164290564359547e-05, 1.1044704808450577e-05, 1.085894064528346e-05, 1.0804765329411892e-05, 1.0502191808416266e-05, 1.0398948411173372e-05, 1.0084761069484e-05, 9.999157204428256e-06, 9.802909843386752e-06, 9.7784792547588e-06, 9.547957479154571e-06, 9.474701478479608e-06, 9.256431022124971e-06, 9.076354842431863e-06, 9.038967073086787e-06, 8.945871311678081e-06, 8.791764311098484e-06, 8.676040243913875e-06, 8.55622614812022e-06, 8.458965669459165e-06, 8.333996097792631e-06, 8.242126852890835e-06, 8.124786498645383e-06, 8.04135494492828e-06, 7.95533792102301e-06, 7.84205728482581e-06, 7.89635580603648e-06, 7.647649399762243e-06, 7.549218445244882e-06, 7.6262197876376225e-06, 7.3999241134754826e-06, 7.382872417514981e-06, 7.277517297315387e-06, 7.344120196015738e-06, 7.110040050619174e-06, 7.141043763820358e-06, 7.01123033357882e-06, 6.981477633304557e-06, 6.906782017363587e-06, 6.869523381829733e-06, 6.8303805558152495e-06, 6.783485423692055e-06, 6.698393762438499e-06]}\n",
      "val_losses_lr for LR 0.002: \n",
      " {1e-05: [0.0588055351632993, 0.04996724116673986, 0.04560996868954391, 0.043622880422396884, 0.04210123730939467, 0.0406937339627778, 0.038976992754050116, 0.03806619574965285, 0.03735150520079035, 0.036506322331350746, 0.036268368225202005, 0.03523141663046194, 0.03426137590333159, 0.033697570516598, 0.03300508090712194, 0.03298288096814729, 0.03217584111946096, 0.031626864532644426, 0.031057798765179633, 0.030459358304385238, 0.02996185313059069, 0.02966135102172189, 0.02959331016965454, 0.028630163213818477, 0.02843403270142016, 0.027373063411198842, 0.027146155509312875, 0.02667835830138771, 0.02665074833444921, 0.02607519580852736, 0.026292720317663713, 0.025660224991245352, 0.024883383437327884, 0.025132874731604526, 0.024487257349230258, 0.02410173559014185, 0.02387947253938156, 0.023568026030585408, 0.023786302064531646, 0.023507483350527575, 0.023267001999702393, 0.02306314539551205, 0.02270197445521194, 0.022878871022969562, 0.022214191037199786, 0.022343941638107996, 0.022186981638456827, 0.02200367532045932], 2e-05: [0.022660463155753006, 0.02351184445778765, 0.023247875950301956, 0.022579657288340046, 0.02305923297293817, 0.02221746528548819, 0.022234332937061865, 0.021670424152795756, 0.021733052818552315, 0.021663401969528893, 0.02110102408198025, 0.02150998708656616, 0.021337800515533717, 0.02131708563396636, 0.021284380690015033, 0.020999640624318, 0.021257901278211123, 0.021207104090225366, 0.021133927808029886, 0.02114770848176909, 0.020912959034491113, 0.020836305799023524, 0.021179775686696926, 0.0213782318925778, 0.021237106992412647, 0.020835033463153883, 0.02138037948098132, 0.020696789417305295, 0.02083120594293056, 0.021695801982716926, 0.021144902220310888, 0.02118956097415497, 0.02108777265647532, 0.020998766992616984, 0.02074697467937888, 0.021521214242383426, 0.020854631668227524, 0.02109706001798414, 0.02121235551423108, 0.021468447576600304, 0.02166379379793913, 0.02216072694898671, 0.021664097574912555, 0.02194929708663141, 0.021719645337099414, 0.02184101574686674, 0.02167555857560313, 0.021635115392594564], 3e-05: [0.022678307054470388, 0.023215672197456003, 0.022207706946968053, 0.023259829049957607, 0.02224472100716582, 0.02253544952399831, 0.02207518748535555, 0.021901300783581386, 0.022822043680218364, 0.022391339632729476, 0.021677084532442003, 0.02242906024204209, 0.02243997083799994, 0.022416673097013312, 0.02186045099536195, 0.02225734340303331, 0.022390694391999394, 0.02336876153067108, 0.022116017424244443, 0.02208331574821287, 0.02280832484823263, 0.022560754111064306, 0.022400123894888945, 0.022589129930533428, 0.022650313349468314, 0.022904376390193902, 0.02353573972027672, 0.023811562655587575, 0.022987534645779007, 0.022230755094195597, 0.02296856280764461, 0.0235688402524732, 0.024176620396036526, 0.023856002690950364, 0.023859699207664817, 0.023516678960181406, 0.02455071832304849, 0.0244177492371365, 0.023357829342952736, 0.023954020165050684, 0.0248227025795374, 0.023925789792190136, 0.02441027099598276, 0.02422160867363966, 0.024405995546909226, 0.024528297658550937, 0.02467954545406188, 0.023550597239197238], 4e-05: [0.0244260495943114, 0.02496572348872306, 0.024099931520208142, 0.023869902984369053, 0.02521676654880452, 0.025361571496197794, 0.023635547755988057, 0.02380673726467908, 0.024158475110832432, 0.023322728924033177, 0.02416727584455083, 0.024549628813814613, 0.024528349299282277, 0.0253444117637633, 0.02626870867577922, 0.025114756891398905, 0.02604046344994746, 0.024395245340865554, 0.02502578627373416, 0.024438879047782996, 0.025341786989991955, 0.02444060009321093, 0.025647540752242598, 0.023912796717501888, 0.024072423877155837, 0.025129727208821066, 0.026263078273956603, 0.02543770329673009, 0.024474121375943486, 0.02608054707749838, 0.024554682286174336, 0.02495658975536727, 0.026119479650454656, 0.026127825498039273, 0.025614595351580418, 0.024182198759864795, 0.025792911505735636, 0.025676919024393455, 0.026989781629371563, 0.025701982307714852, 0.026209073300122416, 0.0262374771642844, 0.02694871340179125, 0.026192018577071794, 0.02605002506937313, 0.02652024242126047, 0.027127574083259873, 0.027174204896074274], 5e-05: [0.027180909350492295, 0.02685560794679853, 0.026986125939545007, 0.026763868730968473, 0.025734352293349687, 0.026002108086265338, 0.025593269193539485, 0.024660336941160075, 0.025949472942851073, 0.026333329329980167, 0.026443862043962298, 0.027859023765271145, 0.02757010296006474, 0.02589469967072813, 0.026042528967276348, 0.025581817630435207, 0.02562855430104901, 0.02644406030374615, 0.025534578082939328, 0.02637414190951371, 0.0265261901422329, 0.026802194507886547, 0.026776942776831125, 0.026550676492636996, 0.02621815340919558, 0.026446580360381014, 0.02670528852029839, 0.026088547314591832, 0.02580659146467638, 0.026969278819212347, 0.026090441798122505, 0.029400616446205983, 0.03003343864059412, 0.028328833859745484, 0.027283468495125848, 0.027587314398174422, 0.02661272055726465, 0.02789905699362385, 0.027517394544086922, 0.027992742961045994, 0.028274997166504397, 0.027813627627106152, 0.02696180111349999, 0.028497722131153157, 0.027332247035904002, 0.027569335392048967, 0.02729142756955782, 0.028870202173984626], 0.0001: [0.026083037261339877, 0.02579439536306334, 0.023730114254359832, 0.023921388849472345, 0.027344610818415813, 0.02418146531846517, 0.02382041891423283, 0.026213454936550883, 0.024307531518834007, 0.02400471693324892, 0.02496326640137757, 0.02389807516753188, 0.022751798981784095, 0.02293038958246965, 0.023070268095291335, 0.02448704539320103, 0.02447953856016791, 0.024043597620898072, 0.023676475825047987, 0.024557932665521837, 0.024186723221657193, 0.023432356451889534, 0.025398322291263226, 0.02385599316354806, 0.024735837962025175, 0.025045171702689654, 0.0256058236311856, 0.024740305520560953, 0.024706750066304243, 0.02488290849735944, 0.02422235746113501, 0.02433220535594118, 0.024079334893117393, 0.0261275880052556, 0.024829133384714583, 0.02511586175225563, 0.024569862074486755, 0.025129013365958148, 0.025006784778364682, 0.02544055753373694, 0.026014372901244847, 0.026089203394728356, 0.026020519340062602, 0.02510379299173234, 0.025952451229537637, 0.02498809556346885, 0.025985378524680965, 0.02560727816152466], 0.0002: [0.025340698948212923, 0.024157623846072833, 0.023849358337237687, 0.023072955497051385, 0.023489720486546663, 0.023453174193475392, 0.0229873255306982, 0.023999857430232995, 0.028322090859074455, 0.02211648434019185, 0.02315371500145788, 0.023933807497108987, 0.02266533099507321, 0.023275232564337358, 0.02217667412499205, 0.023235648552657768, 0.022103791144463696, 0.02339565275424887, 0.024178057115867683, 0.022489212997788315, 0.023956964224941736, 0.024024306010255913, 0.024928344035029238, 0.02520284649310773, 0.023674356729035385, 0.02345268342516853, 0.02589686103828405, 0.023388312310146216, 0.02394444294113566, 0.023244658603888423, 0.023498790621901192, 0.022953941506120863, 0.02470071736076509, 0.02391417439921982, 0.02383042072866236, 0.024021444185871856, 0.02440951281097284, 0.024883748746535116, 0.026416351383093187, 0.024624277244789047, 0.024238203621803855, 0.024147159682492814, 0.024002820521438335, 0.024578611483902453, 0.02485340771207293, 0.026176641838906955, 0.025594645027811884, 0.02575580583393397], 0.0003: [0.02408573273099539, 0.024182828313198478, 0.024790374642287782, 0.02458177833853826, 0.02339131645277938, 0.024531481175431972, 0.026272020316530764, 0.02454436187928087, 0.02434731038143577, 0.023198279480308902, 0.024936766564039058, 0.023669052442110167, 0.022143755735448103, 0.025581597503915834, 0.02374485474168281, 0.024072552351308688, 0.023924070040112252, 0.025042960539084343, 0.024283687136404036, 0.023892706331937876, 0.024570069904508256, 0.02588527220458856, 0.028893469642151375, 0.024864395319011613, 0.025167268635953845, 0.024857166295080748, 0.024627007994502348, 0.027185201584098963, 0.025770637282040484, 0.026210746300912465, 0.022948869892315895, 0.0254249283532648, 0.02513241907858042, 0.02547277655529869, 0.024997253107730558, 0.02678582271373102, 0.025994602792539838, 0.025194243609474122, 0.025160928533166572, 0.02643916141849774, 0.024568370694034797, 0.025540607363726254, 0.02523463865664708, 0.02817382014878868, 0.0262981737889239, 0.024399230591068438, 0.02562607989096545, 0.026157262881537947], 0.0004: [0.026278809181384158, 0.023705060013434486, 0.025593750923595643, 0.027324727791684846, 0.02460185573394865, 0.025028450420065518, 0.02551170178752627, 0.025140118417647895, 0.026120831898361084, 0.025370592288728188, 0.02511751862762368, 0.025592705450443558, 0.0258336896284066, 0.02403530665156527, 0.026249751076931352, 0.02577598462605336, 0.02570996779771605, 0.027508587698045945, 0.03280113583281415, 0.027151143507208403, 0.02668410157563735, 0.025600523190456772, 0.026449868263461202, 0.025828051571499346, 0.025364522461555572, 0.028217100775980285, 0.02708445101954288, 0.026910354294855637, 0.027114719188376945, 0.02553808566276814, 0.02528065429497454, 0.0245330685511983, 0.026881638546792118, 0.02627113357860628, 0.026417797403714076, 0.02682299967109052, 0.02556626528407549, 0.026438970473879622, 0.02763970633029099, 0.025504754230280566, 0.025093239730704868, 0.026834253668066584, 0.02918990797194646, 0.026541325412220824, 0.026575941802486094, 0.027197788508183364, 0.026163435080959176, 0.026935687168426724], 0.0005: [0.02527916379610091, 0.02514258610880562, 0.028413676432933573, 0.02686609791105376, 0.028326619591961904, 0.027051072881366347, 0.027109326608202682, 0.02745808831496215, 0.02541812302721583, 0.02671324428673888, 0.027157794324216892, 0.025804830315724597, 0.02650717484905694, 0.02520277152716759, 0.027384185609449174, 0.02732416913079137, 0.02587174626553669, 0.026525488015518534, 0.028676950220759247, 0.027252156064964454, 0.02789947874042639, 0.02610189392084935, 0.029667815724139065, 0.027844984816022496, 0.027360009778434693, 0.027961524121894406, 0.026830828751406976, 0.027943416313442347, 0.026379798540364655, 0.02617802257782252, 0.02818457393019905, 0.02879811234866168, 0.027944417011258077, 0.02999386070945607, 0.028367818327681413, 0.028316541648129967, 0.02731681890365632, 0.027186335417516635, 0.028839515169767408, 0.027411237748710102, 0.027570377518519627, 0.02827075477661868, 0.028481300578256437, 0.028955331076356658, 0.027331789138862292, 0.02941816027527704, 0.026254913110551305, 0.027651364048694206], 0.001: [0.02620126713117036, 0.02559806210975486, 0.025223773967706857, 0.02470194388357841, 0.023714631248696615, 0.02349511381979336, 0.026456863251832517, 0.024494181601302994, 0.02612646533043652, 0.025087478348172817, 0.02475468637504634, 0.02474065638459579, 0.02611848174867935, 0.02689167099814166, 0.02361199321800109, 0.024557756891512815, 0.02790197314838228, 0.02829739106100505, 0.02311581571086223, 0.026050826187801013, 0.024667522752552514, 0.025131096632305523, 0.025651760577591744, 0.025477266241727213, 0.025543272007499944, 0.025221387890253206, 0.026001260313274505, 0.026269552843694956, 0.02521556090060566, 0.027088156077756543, 0.026643078358296726, 0.02552265261275036, 0.026377228471431178, 0.027135710969832583, 0.026522723758919304, 0.026243876002574173, 0.025778985496621446, 0.027086113813092783, 0.02770619933805719, 0.027138577875037293, 0.027910162560719133, 0.02627040803824891, 0.02687691568151028, 0.026064836208705817, 0.027261187245057855, 0.02519379653042121, 0.024953027142526858, 0.025451373999789126], 0.002: [0.03694450961376159, 0.023922965097730255, 0.024928258367170195, 0.02589219555988332, 0.025521162598882992, 0.02297273455381128, 0.025374142917353962, 0.02690496415629408, 0.025886752847195202, 0.025945479497976768, 0.025623845489203746, 0.02579839875540237, 0.023651566363329424, 0.02360936118848069, 0.025694805197417712, 0.026246089706589804, 0.025231302749000796, 0.02375926778483196, 0.0263137379605931, 0.024098401357218056, 0.02563479496027931, 0.029139724676151828, 0.024947428892059836, 0.024039231731282775, 0.02664520313758791, 0.02665750210751015, 0.026125820932594794, 0.02526074486234688, 0.0244606230946849, 0.024534699100813115, 0.026386077291931865, 0.02453937708021583, 0.025836319824299313, 0.025276193033229526, 0.025290520879708074, 0.024142269260672863, 0.025229725327608067, 0.02605718920195272, 0.025943683558461467, 0.02913721950824211, 0.02791266941855102, 0.024681742912440255, 0.02736730706250677, 0.026073603170926673, 0.02637371966388707, 0.02766974687780798, 0.029775092422475793, 0.028287209447429077]}\n",
      "avg_val_losses_lr 0.002: \n",
      " {1e-05: [0.00017449713698308397, 0.0001482707453018987, 0.00013534115338143592, 0.00012944474902788392, 0.00012492948756496935, 0.000120752919770854, 0.00011565873220786385, 0.00011295607047374733, 0.0001108353270053126, 0.00010832736596839984, 0.0001076212706979288, 0.00010454426299840339, 0.00010166580386745278, 9.999279085043916e-05, 9.793792554042118e-05, 9.787205035058544e-05, 9.54772733515162e-05, 9.384826270814369e-05, 9.215964025275855e-05, 9.038385253526777e-05, 8.890757605516526e-05, 8.801587840273558e-05, 8.781397676455354e-05, 8.495597392824474e-05, 8.437398427721115e-05, 8.12257074516286e-05, 8.055239023534978e-05, 7.916426795664009e-05, 7.908233927136264e-05, 7.737446827456189e-05, 7.801994159544129e-05, 7.614310086422953e-05, 7.38379330484507e-05, 7.457826329852975e-05, 7.266248471581679e-05, 7.15185032348423e-05, 7.085896895958919e-05, 6.99347953429834e-05, 7.058249870780904e-05, 6.97551434733756e-05, 6.904154896054123e-05, 6.8436633221104e-05, 6.736490936264671e-05, 6.788982499397496e-05, 6.591748082255129e-05, 6.630249744245697e-05, 6.58367407669342e-05, 6.529280510522053e-05], 2e-05: [6.72417304325015e-05, 6.976808444447374e-05, 6.898479510475358e-05, 6.700195041050459e-05, 6.842502365857024e-05, 6.59271966928433e-05, 6.597724907140019e-05, 6.430392923678265e-05, 6.448977097493268e-05, 6.428309189771185e-05, 6.261431478332418e-05, 6.38278548562794e-05, 6.331691547636118e-05, 6.325544698506339e-05, 6.315839967363511e-05, 6.231347366266468e-05, 6.307982575136832e-05, 6.292909225586162e-05, 6.271195195261094e-05, 6.27528441595522e-05, 6.20562582625849e-05, 6.182880059057425e-05, 6.284799907031728e-05, 6.34368898889549e-05, 6.301812163920667e-05, 6.182502511321627e-05, 6.344326255484071e-05, 6.141480539259732e-05, 6.181366748644082e-05, 6.43792343700799e-05, 6.274451697421628e-05, 6.287703553161713e-05, 6.257499304592083e-05, 6.231088128372992e-05, 6.156372308421033e-05, 6.386116985870453e-05, 6.188318002441401e-05, 6.260255198214879e-05, 6.29446751164127e-05, 6.370459221543117e-05, 6.428425459329119e-05, 6.575883367651843e-05, 6.42851560086426e-05, 6.513144536092407e-05, 6.444998616349974e-05, 6.481013574737905e-05, 6.431916491276893e-05, 6.419915546763966e-05], 3e-05: [6.729467968685576e-05, 6.888923500728784e-05, 6.589824019871825e-05, 6.902026424319765e-05, 6.600807420523982e-05, 6.687077010088519e-05, 6.550500737494228e-05, 6.498902309668067e-05, 6.772119786414945e-05, 6.644314431076996e-05, 6.432369297460535e-05, 6.655507490220204e-05, 6.658745055786332e-05, 6.651831779529173e-05, 6.486780710789896e-05, 6.604552938585551e-05, 6.644122964984983e-05, 6.93435060257302e-05, 6.562616446363336e-05, 6.552912684929634e-05, 6.768048916389505e-05, 6.694585789633325e-05, 6.646921037059034e-05, 6.703005914104875e-05, 6.721161231296235e-05, 6.796550857624303e-05, 6.983899026788345e-05, 7.065745595129844e-05, 6.821226897857272e-05, 6.596663232698991e-05, 6.815597272298103e-05, 6.993721143167122e-05, 7.174071334135468e-05, 7.078932549243432e-05, 7.080029438476206e-05, 6.978243014890624e-05, 7.285079621082638e-05, 7.245622919031603e-05, 6.931106629956301e-05, 7.108017853130767e-05, 7.365787115589733e-05, 7.099640887890248e-05, 7.243403856374706e-05, 7.187420971406427e-05, 7.2421351771244e-05, 7.278426604911258e-05, 7.323307256398184e-05, 6.98830778611194e-05], 4e-05: [7.24808593303009e-05, 7.408226554517229e-05, 7.151314991159687e-05, 7.08305726539141e-05, 7.482720044155644e-05, 7.52568887127531e-05, 7.013515654595863e-05, 7.064313728391418e-05, 7.16868697650814e-05, 6.920691075380764e-05, 7.171298470193125e-05, 7.284756324574069e-05, 7.278441928570408e-05, 7.520596962541038e-05, 7.794869043257928e-05, 7.452450116142109e-05, 7.727140489598653e-05, 7.238945205004616e-05, 7.426049339386992e-05, 7.25189289251721e-05, 7.519818097920461e-05, 7.252403588489889e-05, 7.61054621728267e-05, 7.09578537611332e-05, 7.143152485802919e-05, 7.456892346831177e-05, 7.79319830087733e-05, 7.548279910008929e-05, 7.262350556659788e-05, 7.739034741097442e-05, 7.28625587126835e-05, 7.405516247883463e-05, 7.750587433369335e-05, 7.75306394600572e-05, 7.600770134000124e-05, 7.175726634974716e-05, 7.653682939387429e-05, 7.619263805457999e-05, 8.00883727874527e-05, 7.626700981517761e-05, 7.777173086089738e-05, 7.785601532428605e-05, 7.996650861065652e-05, 7.772112337410028e-05, 7.72997776539262e-05, 7.869508136872543e-05, 8.049725247258122e-05, 8.063562283701565e-05], 5e-05: [8.065551736051126e-05, 7.969023129613806e-05, 8.007752504316026e-05, 7.941800810376402e-05, 7.63630631850139e-05, 7.715759076043128e-05, 7.594441897192725e-05, 7.317607400937708e-05, 7.700140339124947e-05, 7.814044311566815e-05, 7.846843336487329e-05, 8.266772630644257e-05, 8.181039454025145e-05, 7.683887142649296e-05, 7.727753402752626e-05, 7.591043807250803e-05, 7.604912255504157e-05, 7.846902167283724e-05, 7.57702613737072e-05, 7.826154869291902e-05, 7.87127303923825e-05, 7.95317344447672e-05, 7.94568034920805e-05, 7.878539018586646e-05, 7.779867480473466e-05, 7.847649958570034e-05, 7.924417958545515e-05, 7.741408698691938e-05, 7.657742274384683e-05, 8.00275335881672e-05, 7.741970859977005e-05, 8.724218530031448e-05, 8.911999596615465e-05, 8.406182154227146e-05, 8.095984716654554e-05, 8.18614670568974e-05, 7.896949720256572e-05, 8.27865192689135e-05, 8.165398974506505e-05, 8.30645191722433e-05, 8.390206874333649e-05, 8.253301966500342e-05, 8.000534455044508e-05, 8.456297368294705e-05, 8.11045906109911e-05, 8.180811689035301e-05, 8.098346459809442e-05, 8.56682557091532e-05], 0.0001: [7.739773668053376e-05, 7.654123253134522e-05, 7.041576930077102e-05, 7.098334970169835e-05, 8.114127839292526e-05, 7.175508996577202e-05, 7.068373565054253e-05, 7.778473274940915e-05, 7.212917364639171e-05, 7.123061404524902e-05, 7.407497448479991e-05, 7.091416963659311e-05, 6.751275662250473e-05, 6.804269905777344e-05, 6.845776882875767e-05, 7.266185576617517e-05, 7.263958029723415e-05, 7.134598700563226e-05, 7.025660482210085e-05, 7.287220375525768e-05, 7.177069205239523e-05, 6.953221499077013e-05, 7.536594151710157e-05, 7.078929722121086e-05, 7.340011264695898e-05, 7.431801692192776e-05, 7.59816724960997e-05, 7.34133694972135e-05, 7.331379841633307e-05, 7.383652373103692e-05, 7.187643163541545e-05, 7.220238978024089e-05, 7.145203232379049e-05, 7.752993473369615e-05, 7.367695366384149e-05, 7.452777968028377e-05, 7.290760259491618e-05, 7.456680524023189e-05, 7.42041091346133e-05, 7.549126864610368e-05, 7.719398487016275e-05, 7.741603381225031e-05, 7.721222356101663e-05, 7.449196733451733e-05, 7.701024103720367e-05, 7.414865152364644e-05, 7.71079481444539e-05, 7.598598860986545e-05], 0.0002: [7.519495236858434e-05, 7.168434375689268e-05, 7.076960930931065e-05, 6.846574331469254e-05, 6.970243467818001e-05, 6.959398870467475e-05, 6.82116484590451e-05, 7.121619415499405e-05, 8.404181263820313e-05, 6.562754997089569e-05, 6.870538576100261e-05, 7.102020028815723e-05, 6.725617505956441e-05, 6.906597200100107e-05, 6.580615467356691e-05, 6.89485120256907e-05, 6.558988470167269e-05, 6.942330194139131e-05, 7.174497660494861e-05, 6.673356972637483e-05, 7.108891461407043e-05, 7.128874187019559e-05, 7.397134728495322e-05, 7.478589463830187e-05, 7.02503167033691e-05, 6.959253241889771e-05, 7.684528498007136e-05, 6.940152020814901e-05, 7.105175946924527e-05, 6.897524808275496e-05, 6.972934902641303e-05, 6.811258607157526e-05, 7.329589721295279e-05, 7.096194183744754e-05, 7.071341462511087e-05, 7.128024980970878e-05, 7.243178875659597e-05, 7.383901705203299e-05, 7.83867993563596e-05, 7.30690719429942e-05, 7.192345288369097e-05, 7.165329282638816e-05, 7.122498671049952e-05, 7.293356523413191e-05, 7.374898430882175e-05, 7.767549507094052e-05, 7.594850156620738e-05, 7.642672354283077e-05], 0.0003: [7.147101700592104e-05, 7.175913446052961e-05, 7.356194255871746e-05, 7.294296242889691e-05, 6.941043457798035e-05, 7.279371268674175e-05, 7.795851725973521e-05, 7.283193435988389e-05, 7.224721181434947e-05, 6.883762457064956e-05, 7.399633995263816e-05, 7.02345769795554e-05, 6.57084739924276e-05, 7.590978487808853e-05, 7.045950961923684e-05, 7.14319060869694e-05, 7.099130575700965e-05, 7.431145560559152e-05, 7.205841880238586e-05, 7.08982383737029e-05, 7.290821930121144e-05, 7.681089674952094e-05, 8.573729864139874e-05, 7.378158848371398e-05, 7.468032236188085e-05, 7.376013737412684e-05, 7.307717505787047e-05, 8.066825395875063e-05, 7.647073377460085e-05, 7.777669525493313e-05, 6.809753677245073e-05, 7.544489125597864e-05, 7.45769112124048e-05, 7.558687405133142e-05, 7.417582524549127e-05, 7.948315345320777e-05, 7.713531985916866e-05, 7.476036679369176e-05, 7.466150900049428e-05, 7.845448492135828e-05, 7.290317713363441e-05, 7.578815241461797e-05, 7.48802334025136e-05, 8.360184020412072e-05, 7.803612400274154e-05, 7.240127771830397e-05, 7.60417800918856e-05, 7.76179907464034e-05], 0.0004: [7.797866225930017e-05, 7.034142437220916e-05, 7.594584843796927e-05, 8.108227831360489e-05, 7.30025392698773e-05, 7.426839887259797e-05, 7.570237919147262e-05, 7.459975791586913e-05, 7.750988693875693e-05, 7.528365664311035e-05, 7.453269622440262e-05, 7.594274614374943e-05, 7.665783272524214e-05, 7.132138472274561e-05, 7.789243643006336e-05, 7.64866012642533e-05, 7.629070563120489e-05, 8.162785667075949e-05, 9.733274727838027e-05, 8.056719141604868e-05, 7.918131031346394e-05, 7.596594418533167e-05, 7.848625597466232e-05, 7.664110258605147e-05, 7.526564528651505e-05, 8.373026936492666e-05, 8.036929085917769e-05, 7.985268336752414e-05, 8.045910738390785e-05, 7.578066962245739e-05, 7.501677832336659e-05, 7.279842300058843e-05, 7.976747343261756e-05, 7.795588598992961e-05, 7.839109021873613e-05, 7.95934708340965e-05, 7.586428867678186e-05, 7.845391832011757e-05, 8.20169327308338e-05, 7.568176329460109e-05, 7.446065201989575e-05, 7.96268654838771e-05, 8.661693760221501e-05, 7.875764217276209e-05, 7.886036143170947e-05, 8.070560388184974e-05, 7.763630587821714e-05, 7.99278550991891e-05], 0.0005: [7.501235547804425e-05, 7.46070804415597e-05, 8.431357991968419e-05, 7.972135878650968e-05, 8.405525101472374e-05, 8.027024593877254e-05, 8.04431056623225e-05, 8.147800686932388e-05, 7.542469741013599e-05, 7.926778720100558e-05, 8.058692677809167e-05, 7.657219678256557e-05, 7.865630519007994e-05, 7.478567218744092e-05, 8.125871100726758e-05, 8.10806205661465e-05, 7.677076043185961e-05, 7.871064693032205e-05, 8.509480777673367e-05, 8.08669319435147e-05, 8.2787770743105e-05, 7.745369115979035e-05, 8.803506149596162e-05, 8.262606770333085e-05, 8.118697263630473e-05, 8.297188166734245e-05, 7.96167025264302e-05, 8.291814929804851e-05, 7.82783339476696e-05, 7.767959221905793e-05, 8.363375053471529e-05, 8.545433931353614e-05, 8.292111872776877e-05, 8.900255403399427e-05, 8.417750245602793e-05, 8.402534613688418e-05, 8.105880980313448e-05, 8.067161844960426e-05, 8.557719634945819e-05, 8.13389844175374e-05, 8.181120925376744e-05, 8.38894800493136e-05, 8.451424503933661e-05, 8.59208637280613e-05, 8.110323186606022e-05, 8.729424414028795e-05, 7.790775403724424e-05, 8.205152536704512e-05], 0.001: [7.774856715480819e-05, 7.595864127523697e-05, 7.484799396945654e-05, 7.329953674652347e-05, 7.036982566378817e-05, 6.971843863440167e-05, 7.85070126167137e-05, 7.268303145787238e-05, 7.752660335441103e-05, 7.444355592929619e-05, 7.345604265592386e-05, 7.341441063678275e-05, 7.750291320082894e-05, 7.979724331792777e-05, 7.006526177448395e-05, 7.287168217066117e-05, 8.279517254712842e-05, 8.396851946885772e-05, 6.859292495804817e-05, 7.730215485994366e-05, 7.319739689184722e-05, 7.457298703948227e-05, 7.611798390976779e-05, 7.560019656298877e-05, 7.579605936943603e-05, 7.48409136209294e-05, 7.71550751135742e-05, 7.795119538188415e-05, 7.48236228504619e-05, 8.038028509720042e-05, 7.905957969820988e-05, 7.573487422181116e-05, 7.827070763035958e-05, 8.05213975365952e-05, 7.870244438848458e-05, 7.787500297499755e-05, 7.649550592469272e-05, 8.037422496466701e-05, 8.221424135922015e-05, 8.052990467370116e-05, 8.281947347394402e-05, 7.795373305118371e-05, 7.975345899557947e-05, 7.734372762227246e-05, 8.089373069750105e-05, 7.475904014961784e-05, 7.40445909273794e-05, 7.552336498453747e-05], 0.002: [0.0001096276249666516, 7.098802699623221e-05, 7.397109307765637e-05, 7.68314408305143e-05, 7.573045281567654e-05, 6.816835179172486e-05, 7.529419263309782e-05, 7.983668889108036e-05, 7.681529034776024e-05, 7.698955340645925e-05, 7.603514981959568e-05, 7.655311203383492e-05, 7.018268950542856e-05, 7.005745159786556e-05, 7.624571275198134e-05, 7.788157182964334e-05, 7.487033456676795e-05, 7.050227829326991e-05, 7.808230848840682e-05, 7.150860936859957e-05, 7.606764083168935e-05, 8.646802574525765e-05, 7.402797890818943e-05, 7.133303184356907e-05, 7.906588468126976e-05, 7.910238014097968e-05, 7.752469119464331e-05, 7.495769988826968e-05, 7.258345131953976e-05, 7.280326142674515e-05, 7.829696525795806e-05, 7.28171426712636e-05, 7.666563746082882e-05, 7.500354015795111e-05, 7.504605602287262e-05, 7.163878118894024e-05, 7.486565379112186e-05, 7.732103620757483e-05, 7.698422420908447e-05, 8.646059201258786e-05, 8.282691222121965e-05, 7.323959321198888e-05, 8.120862629823968e-05, 7.736974234696343e-05, 7.82602957385373e-05, 8.210607382138866e-05, 8.835338997767298e-05, 8.393830696566492e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.003 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.010062042623758316\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "Validation loss decreased (inf --> 0.000074).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.00952606089413166\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.01020109560340643\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000084\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.010089388117194176\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.012119420804083347\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000079\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.012577247805893421\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000075\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.015229995362460613\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.011169394478201866\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000073\n",
      "Validation loss decreased (0.000074 --> 0.000073).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.009004822932183743\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.006476923357695341\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.009210809133946896\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.012128157541155815\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.008629834279417992\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.010650191456079483\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.00855796504765749\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.01496156957000494\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.006900001782923937\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.011942654848098755\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.008381891064345837\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.00630469573661685\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000074\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.010556202381849289\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.009240982122719288\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000081\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.005975303705781698\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.009390819817781448\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.014046347700059414\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000076\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.011938007548451424\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000083\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.010394366458058357\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000080\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.0071949828416109085\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.01104680635035038\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000078\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.005528537090867758\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000082\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.008918346837162971\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000007 \tAverage Validation Loss: 0.000086\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n"
     ]
    }
   ],
   "source": [
    "#For hyperparameter tuning\n",
    "for lr in learning_rate:\n",
    "    print('\\n')\n",
    "    print(f'##########################################################')\n",
    "    print(f'##########################################################')    \n",
    "    print(f'############### Training for learning rate {lr} START! ###############')\n",
    "    print(f'##########################################################')\n",
    "    print(f'##########################################################')\n",
    "    print('\\n')\n",
    "    optimizer = make_optimizer(model, lr)\n",
    "    train_model(EPOCHS,\n",
    "               train_data_loader,\n",
    "               val_data_loader,\n",
    "               model,\n",
    "               optimizer,\n",
    "               os.path.join(checkpoint_path, f\"curr_ckpt_{lr}\"),\n",
    "               best_model_path,\n",
    "               metric_path,\n",
    "               date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584f93a",
   "metadata": {},
   "source": [
    "### Check & Visualize validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['dodgerblue', 'green', 'violet', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb57324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate):\n",
    "    plt.title(\"Losses per epoch\")\n",
    "    plt.plot(epoch_list, \n",
    "             train_losses_lr[lr], \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Train\")    \n",
    "    plt.plot(epoch_list, \n",
    "             val_losses_lr[lr], \n",
    "             '--', \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Val\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate):\n",
    "    plt.title(\"Gap of loss per epoch\")\n",
    "    plt.plot(epoch_list, \n",
    "             np.array(train_losses_lr[lr]) - np.array(val_losses_lr[lr]), \n",
    "             color=color[index], \n",
    "             label=lr)    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Gap of loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate):\n",
    "    plt.title(\"Average losses per epoch\")\n",
    "    plt.plot(epoch_list, \n",
    "             avg_train_losses_lr[lr], \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Train\")    \n",
    "    plt.plot(epoch_list, \n",
    "             avg_val_losses_lr[lr], \n",
    "             '--', \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Val\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537da752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate):\n",
    "    plt.title(\"Gap of Average loss per epoch\")\n",
    "    plt.plot(epoch_list, \n",
    "             np.array(avg_train_losses_lr[lr]) - np.array(avg_val_losses_lr[lr]), \n",
    "             color=color[index], \n",
    "             label=lr)    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Gap of average loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c96b1",
   "metadata": {},
   "source": [
    "- Let's see losses in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e6cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate[1:]):\n",
    "    plt.title(\"Losses per epoch: lr = [1e-05, 1e-04, 1e-03]\")\n",
    "    plt.plot(epoch_list, \n",
    "             train_losses_lr[lr], \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Train\")    \n",
    "    plt.plot(epoch_list, \n",
    "             val_losses_lr[lr], \n",
    "             '--', \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Val\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1710b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate[1:]):\n",
    "    plt.title(\"Gap of loss per epoch: lr = [1e-05, 1e-04, 1e-03]\")\n",
    "    plt.plot(epoch_list, \n",
    "             np.array(train_losses_lr[lr]) - np.array(val_losses_lr[lr]), \n",
    "             color=color[index], \n",
    "             label=lr)    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Gap of loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate[1:]):\n",
    "    plt.title(\"Average losses per epoch: lr = [1e-05, 1e-04, 1e-03]\")\n",
    "    plt.plot(epoch_list,\n",
    "             avg_train_losses_lr[lr], \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Train\")    \n",
    "    plt.plot(epoch_list, \n",
    "             avg_val_losses_lr[lr], \n",
    "             '--', \n",
    "             color=color[index], \n",
    "             label=f\"{lr} - Val\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c11807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.xticks(epoch_list)\n",
    "for index, lr in enumerate(learning_rate[1:]):\n",
    "    plt.title(\"Gap of Average loss per epoch: lr = [1e-05, 1e-04, 1e-03]\")\n",
    "    plt.plot(epoch_list, \n",
    "             np.array(avg_train_losses_lr[lr]) - np.array(avg_val_losses_lr[lr]), \n",
    "             color=color[index], \n",
    "             label=lr)    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Gap of average loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75dce5",
   "metadata": {},
   "source": [
    "__Minimum validation loss:  Learning rate = 0000, epoch = 00__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe9f1b",
   "metadata": {},
   "source": [
    "## 5-5. Evaluate model\n",
    " - Test with validation dataset\n",
    " - 1st important index: Precision\n",
    " - 2nd importand index: Recall\n",
    "   - __First, get high & stable <u>Precision</u>, then improve <u>Recall</u>.__\n",
    " - And other indexes: F1 score, confusion matrix\n",
    " \n",
    " - For metrics reference here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef66290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        transformed_image = Image.open(urlopen(image)).convert('RGB') #OR open(image)\n",
    "    except:\n",
    "        return \"Cannot open image\"\n",
    "    \n",
    "    transformed_image = transform(transformed_image)\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "def inference(image, model, device):\n",
    "    preprocessed_image = preprocessing(image)\n",
    "    \n",
    "    if preprocessed_image == \"Cannot open image\":\n",
    "        return \"Cannot open image.\"\n",
    "\n",
    "    #for gpu computation\n",
    "    if device.type == 'cuda':\n",
    "        model.cuda()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(preprocessed_image)\n",
    "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist() #1*46 list in a list\n",
    "        return final_output\n",
    "    #     print(final_output)\n",
    "    #     print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
    "#         result_pair = zip(train_df.columns[1:].to_list(), final_output[0])\n",
    "#         result_dict = {}\n",
    "#         for label, score in result_pair:\n",
    "#             if score > 0.1: #Set prediction threshold\n",
    "#                 result_dict[label] = score\n",
    "#     return sorted(result_dict.items(), key=(lambda x: x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf56d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "#from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "\n",
    "def get_metrics(models_path, average_param, label_names):\n",
    "    #Load model\n",
    "    load_model = ResNeXt50Class()\n",
    "    load_model = load_model.cuda() #for GPU computation\n",
    "    load_model = nn.DataParallel(load_model) # Distributed\n",
    "    \n",
    "\n",
    "    y_true = valid_dataset.labels\n",
    "    y_evaluations = {}\n",
    "\n",
    "    for file in os.listdir(models_path):\n",
    "        y_pred = []\n",
    "\n",
    "        if file.split('_')[0] == 'curr':\n",
    "            model = os.path.join(models_path, file)\n",
    "            # optimizer\n",
    "            val_lr = float(file.split('_')[2])\n",
    "            val_optimizer = make_optimizer(load_model, val_lr)            \n",
    "            print(f\"===========================Start evaluating model {file}, at lr {val_lr}===========================\")\n",
    "            #Load model --> This isn't necessary if you evaluate model right after training.\n",
    "            prediction_model = load_ckp(model,\n",
    "                                                        load_model,\n",
    "                                                        val_optimizer)[0] #load_ckp: [model, optimizer, checkpoint['epoch'], valid_loss_min.item()]\n",
    "            \n",
    "            #Evaluate.\n",
    "            for i in tqdm(range(len(valid_dataset))):\n",
    "                prediction = np.array(inference(valid_dataset.feed_text[i],\n",
    "                                                            prediction_model, \n",
    "                                                            tokenizer,\n",
    "                                                            device))[0]\n",
    "                prediction_over_threshold = np.where(prediction > 0.1, 1, 0).tolist()  #Threshold for each class is 0.1\n",
    "                y_pred.append(prediction_over_threshold)\n",
    "                if i != 0 and i % 5000 == 0:\n",
    "                    _precision_score = precision_score(y_true[:i+1], y_pred, average=average_param) #average: ['micro', 'macro', 'weighted', 'samples']\n",
    "                    _recall_score = recall_score(y_true[:i+1], y_pred, average=average_param)\n",
    "                    _f1_score = f1_score(y_true[:i+1], y_pred, average=average_param)\n",
    "                    _average_precision_score = average_precision_score(y_true[:i+1], y_pred, average=average_param)\n",
    "                    print(f'current index number: {i}')\n",
    "                    print(f'current Precision: {_precision_score}')\n",
    "                    print(f'current Recall: {_recall_score}')\n",
    "                    print(f'current f1 score: {_f1_score}')\n",
    "                    print(f'current AP: {_average_precision_score}')\n",
    "                    print('\\n')\n",
    "                \n",
    "                \n",
    "            #Append evaluation result into y_preds\n",
    "            _precision_score = precision_score(y_true, y_pred, average=average_param) #average: ['micro', 'macro', 'weighted', 'samples']\n",
    "            _recall_score = recall_score(y_true, y_pred, average=average_param)\n",
    "            _f1_score = f1_score(y_true, y_pred, average=average_param)\n",
    "            _average_precision_score = average_precision_score(y_true, y_pred, average=average_param)            \n",
    "            _classification_report = classification_report(y_true, y_pred, target_names = label_names)\n",
    "            result_dict = {}\n",
    "            result_dict['precision'] = _precision_score\n",
    "            result_dict['recall'] = _recall_score\n",
    "            result_dict['f1score'] = _f1_score\n",
    "            result_dict['AP'] = _average_precision_score\n",
    "            result_dict['classification_report'] = _classification_report\n",
    "            y_evaluations[model.split('/')[-1]] = result_dict\n",
    "            print(f\"Number of finished model: {len(y_evaluations)}\")\n",
    "            print(f\"===========================Done evaluating!===========================\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return y_evaluations\n",
    "\n",
    "models_path = '/home/ubuntu/Desktop/Project/autolabeler_classifier/resnext50_model/20211207/'\n",
    "bert_evaluations_for_20211207 = get_metrics(models_path, 'weighted', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc51a24",
   "metadata": {},
   "source": [
    "# 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb4b67",
   "metadata": {},
   "source": [
    "## 6-1. Define preprocessing function for input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        transformed_image = Image.open(urlopen(image)).convert('RGB') #OR open(image)\n",
    "    except:\n",
    "        return \"Cannot open image\"\n",
    "    \n",
    "    transformed_image = transform(transformed_image)\n",
    "    \n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af390d",
   "metadata": {},
   "source": [
    "## 6-2. Load saved model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "load_model = ResNeXt50Class()\n",
    "load_model = load_model.cuda() #for GPU computation\n",
    "load_model = nn.DataParallel(load_model) # Distributed\n",
    "best_model_path = os.path.join(checkpoint_path, \"20211126/curr_ckpt_1e-05_16\") #currently best model state.\n",
    "\n",
    "best_optimizer = make_optimizer(\"loaded_model or best_model_path\", \"LEARNING_RATE\") # <-- Parameter shoud be changed!\n",
    "\n",
    "predicton_model = load_ckp(best_model_path,  #Path to the saved checkpoint\n",
    "                        load_model,\n",
    "                        best_optimizer)[0] #load_ckp: [model, optimizer, checkpoint['epoch'], valid_loss_min.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654031",
   "metadata": {},
   "source": [
    "## 6-3. Define inference function\n",
    " - Return dictionaries of predicted labels: {label1: score1, label2: score2, ....}\n",
    " - __Labeles which has lower score than threshold will be ignored.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b871f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, model, device):\n",
    "    preprocessed_image = preprocessing(image)\n",
    "    \n",
    "    if preprocessed_image == \"Cannot open image\":\n",
    "        return \"Cannot open image.\"\n",
    "\n",
    "    #for gpu computation\n",
    "    if device.type == 'cuda':\n",
    "        model.cuda()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(preprocessed_image)\n",
    "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist() #1*46 list in a list\n",
    "    #     print(final_output)\n",
    "    #     print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
    "        result_pair = zip(train_df.columns[1:].to_list(), final_output[0])\n",
    "        result_dict = {}\n",
    "        for label, score in result_pair:\n",
    "            if score > 0.1: #Set prediction threshold\n",
    "                result_dict[label] = score\n",
    "    return sorted(result_dict.items(), key=(lambda x: x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b32e3",
   "metadata": {},
   "source": [
    "- Reference code for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14471026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for sample_id in [1,2,3,4,6]:\n",
    "#     test_img, test_labels = test_dataset[sample_id]\n",
    "#     test_img_path = os.path.join(img_folder, test_dataset.imgs[sample_id])\n",
    "#     with torch.no_grad():\n",
    "#         raw_pred = model(test_img.unsqueeze(0)).cpu().numpy()[0]\n",
    "#         raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
    "\n",
    "#     predicted_labels = np.array(dataset_val.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
    "#     if not len(predicted_labels):\n",
    "#         predicted_labels = ['no predictions']\n",
    "#     img_labels = np.array(dataset_val.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
    "#     plt.imshow(Image.open(test_img_path))\n",
    "#     plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a04e3",
   "metadata": {},
   "source": [
    "## 6-4. Demo test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = \"\"\n",
    "\n",
    "inference(test_image, #Input sentence\n",
    "         predicton_model,\n",
    "          device) #CPU or GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-bert-text-classification",
   "language": "python",
   "name": "tf-bert-text-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
