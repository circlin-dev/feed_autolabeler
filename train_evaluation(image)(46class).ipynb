{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da93c77",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b130ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import printoptions\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "#import requests\n",
    "#import tarfile\n",
    "import random\n",
    "#import json\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5dcd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True #Enable processing cutted images(prevent OSError: image file is truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296158fb",
   "metadata": {},
   "source": [
    "# 2. Set paths\n",
    " - To load dataset\n",
    " - To save checkpoints & best checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d82f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date today: 20211201\n"
     ]
    }
   ],
   "source": [
    "#Set directories as you want.\n",
    "path = \"/home/ubuntu/Desktop/Project\"\n",
    "dataset_path = os.path.join(path, \"datasets/circlin_feeds_dataset/image_dataset\")\n",
    "\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "print(f\"Date today: {date}\")\n",
    "checkpoint_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "model_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "metric_path = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}\")\n",
    "\n",
    "# Save path for logs\n",
    "# logdir = os.path.join(path, f\"autolabeler_classifier/resnext50_model/{date}/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590f6ed",
   "metadata": {},
   "source": [
    "# 3. Training settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9dde8",
   "metadata": {},
   "source": [
    "## 3-1. Set seed number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52637839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all seeds to make experiments reproducible\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed(2020)\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c10801",
   "metadata": {},
   "source": [
    "## 3-2. Hyperparameters\n",
    " - __Adjust: <u>mean</u>, <u>std</u>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643f6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training parameters.\n",
    "NUM_WORKERS = 8 # Number of CPU processes for data preprocessing\n",
    "LEARNING_RATE = 1e-4 # Learning rate\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "save_freq = 1 # Save checkpoint frequency (epochs)\n",
    "test_freq = 200 # Test model frequency (iterations)\n",
    "EPOCHS = 36 # Number of epochs for training \n",
    "# Note: on the small subset of data overfitting happens after 30-35 epochs\n",
    "\n",
    "\n",
    "#For normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Run tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943551f",
   "metadata": {},
   "source": [
    "## 3-3. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe33abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "#criterion = nn.BCELoss() #BCEWithLogitsLoss\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets) #BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030b9d7",
   "metadata": {},
   "source": [
    "## 3-4. Check GPU status & Enable distributed processing\n",
    " - __Should be improved!__ \n",
    "   - As is : Using DatParallel\n",
    "   - To be: Use DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf1ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Device check(for GPU computing)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4527158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For multiple GPU utilization: This should be improved...\n",
    "\n",
    "# dist.init_process_group(\n",
    "#     backend='nccl',\n",
    "#     init_method='tcp://localhost:9999', #FREEPORT\n",
    "#     world_size=2,\n",
    "#     rank=0,\n",
    "# )\n",
    "\n",
    "# dist.init_process_group(\n",
    "#     backend=\"nccl\",\n",
    "#     init_method='tcp://127.0.0.1:9999',\n",
    "#     rank=0,\n",
    "#     world_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761908f2",
   "metadata": {},
   "source": [
    "## 3-5. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534520d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "def make_optimizer(model, lr):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params =  model.parameters(), \n",
    "        lr=lr)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68000b",
   "metadata": {},
   "source": [
    "# 4. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e0e93",
   "metadata": {},
   "source": [
    "## 4-1. Define target labels(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fb22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define taret labels\n",
    "labels = ['간편식', '건강간식', '건강식', '건강음료', '걷기/산책', '격투기', '골프', \n",
    "          '기타식단', '기타운동', '농구', '달리기/조깅', '당구', '등산/등반', '루틴기록', '맨몸', '무술', \n",
    "          '배구', '배드민턴', '보조제', '보충제', '볼링', '수상스포츠', '스키/스노보드', '승마', '신체기록', \n",
    "          '야구', '온라인클래스', '요가', '운동기구', '운동용품', '웨이트', '유산소기록', '의류', '일반간식', \n",
    "          '일반식', '일반음료', '일상생활', '자전거', '종합운동', '줄넘기', '축구/풋살', '탁구', '테니스', \n",
    "          '폴댄스', '필라테스', '홈트'] #46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08841c18",
   "metadata": {},
   "source": [
    "## 4-2. Create custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a67ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, df, transforms):\n",
    "#         # !!!!!!!!!!Broken image files cannot be opened by both PIL.Image, cv2... So remove them from list by try~except.\n",
    "#         broken_urls = []\n",
    "#         broken_index = []\n",
    "#         for url in df['url']:\n",
    "#             if df[df['url']==url].index[0] % 500 == 0:\n",
    "#                 print(f\"Now Doing: {df[df['url']==url].index[0]}, and {len(broken_urls)} urls({len(broken_index)} indexes) seem to be broken...\")\n",
    "#             try:\n",
    "#                 image = Image.open(urlopen(url))\n",
    "#             except:\n",
    "#                 drop_index = df[df['url'] == url].index\n",
    "#                 broken_urls.append(url)\n",
    "#                 broken_index.append(drop_index)\n",
    "\n",
    "#         print(f\"{len(broken_urls)} files are broken... Cannot open below files: \\n {broken_urls}\")\n",
    "#         df = df.drop(broken_index)\n",
    "#         print(f\"Removed broken file rows. Now you can use {len(df)} files. Data is as below: \\n {df}\")\n",
    "\n",
    "#         self.transforms = transforms\n",
    "#         self.df = df\n",
    "#         self.feed_image = df['url'] #Series of file name\n",
    "#         self.labels = self.df[labels].values #df.values: np.array #one-hot encoded: [0, 1, 0, ...., 1, 1]\n",
    "        \n",
    "#         #self.image_list = self.feed_image.tolist()\n",
    "#         #self.label_list = self.labels.tolist()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.feed_image)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         label = torch.FloatTensor(self.labels[index])\n",
    "#         image_url = self.feed_image[index]\n",
    "\n",
    "#         image = Image.open(urlopen(image_url)) #Input(image_url) is URL address. \n",
    "#         if self.transforms is not None:\n",
    "#             image = self.transforms(image)\n",
    "            \n",
    "#         return image, label\n",
    "\n",
    "    \n",
    "# # train_annotations = os.path.join(img_folder, 'small_train.json')\n",
    "# # train_dataset = CustomDataset(img_folder, train_annotations, train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161492d",
   "metadata": {},
   "source": [
    "- At Image.open in __ __getitem__ __  needs .convert('RGB') because Image.open returns grayscale.\n",
    "    - https://stackoverflow.com/questions/59218671/runtimeerror-output-with-shape-1-224-224-doesnt-match-the-broadcast-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "        self.feed_image = df['url'] #Series of file name\n",
    "        self.labels = self.df[labels].values #df.values: np.array #one-hot encoded: [0, 1, 0, ...., 1, 1]\n",
    "        \n",
    "        #self.image_list = self.feed_image.tolist()\n",
    "        #self.label_list = self.labels.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feed_image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = torch.FloatTensor(self.labels[index])\n",
    "        image_url = self.feed_image[index]\n",
    "        \n",
    "        #Needs .convert('RGB') because Image.open returns grayscale.\n",
    "        image = Image.open(image_url).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "    \n",
    "# train_annotations = os.path.join(img_folder, 'small_train.json')\n",
    "# train_dataset = CustomDataset(img_folder, train_annotations, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd65ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use the .tar.gz archive from this(https://github.com/thuml/HashNet/tree/master/pytorch#datasets) \n",
    "# # github repository to speed up image loading(instead of loading it from Flickr).\n",
    "# # Let's download and extract it.\n",
    "# img_folder = '/home/ubuntu/Desktop/ml_dl_tutorials/dataset_for_tutorial/nus_wide_images'\n",
    "# if not os.path.exists(img_folder):\n",
    "#     def download_file_from_google_drive(id, destination):\n",
    "#         def get_confirm_token(response):\n",
    "#             for key, value in response.cookies.items():\n",
    "#                 if key.startswith('download_warning'):\n",
    "#                     return value\n",
    "#             return None\n",
    "\n",
    "#         def save_response_content(response, destination):\n",
    "#             CHUNK_SIZE = 32768\n",
    "#             with open(destination, \"wb\") as f:\n",
    "#                 for chunk in tqdm(response.iter_content(CHUNK_SIZE), desc='Downloading'):\n",
    "#                     if chunk:  # filter out keep-alive new chunks\n",
    "#                         f.write(chunk)\n",
    "\n",
    "#         URL = \"https://docs.google.com/uc?export=download\"\n",
    "#         session = requests.Session()\n",
    "#         response = session.get(URL, params={'id': id}, stream=True)\n",
    "#         token = get_confirm_token(response)\n",
    "\n",
    "#         if token:\n",
    "#             params = {'id': id, 'confirm': token}\n",
    "#             response = session.get(URL, params=params, stream=True)\n",
    "#         save_response_content(response, destination)\n",
    "\n",
    "#     file_id = '0B7IzDz-4yH_HMFdiSE44R1lselE'\n",
    "#     path_to_tar_file = str(time.time()) + '.tar.gz'\n",
    "#     download_file_from_google_drive(file_id, path_to_tar_file)\n",
    "#     print('Extraction')\n",
    "#     with tarfile.open(path_to_tar_file) as tar_ref:\n",
    "#         tar_ref.extractall(os.path.dirname(img_folder))\n",
    "#     os.remove(path_to_tar_file)\n",
    "# # Also, copy our pre-processed annotations to the dataset folder. \n",
    "# # Note: you can find script for generating such annotations in attachments\n",
    "# copyfile('nus_wide/small_test.json', os.path.join(img_folder, 'small_test.json'))\n",
    "# copyfile('nus_wide/small_train.json', os.path.join(img_folder, 'small_train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34167c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
    "# dataset_val = CustomDataset(img_folder, os.path.join(img_folder, 'small_test.json'), None)\n",
    "# dataset_train = CustomDataset(img_folder, os.path.join(img_folder, 'small_train.json'), None)\n",
    "\n",
    "# # A simple function for visualization.\n",
    "# def show_sample(img, binary_img_labels):\n",
    "#     # Convert the binary labels back to the text representation.    \n",
    "#     img_labels = np.array(dataset_val.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(\"{}\".format(', '.join(img_labels)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# for sample_id in range(5):\n",
    "#     show_sample(*dataset_val[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeece538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate label distribution for the entire dataset (train + test)\n",
    "# samples = dataset_val.annos + dataset_train.annos\n",
    "# samples = np.array(samples)\n",
    "# with printoptions(precision=3, suppress=True):\n",
    "#     class_counts = np.sum(samples, axis=0)\n",
    "#     # Sort labels according to their frequency in the dataset.\n",
    "#     sorted_ids = np.array([i[0] for i in sorted(enumerate(class_counts), key=lambda x: x[1])], dtype=int)\n",
    "#     print('Label distribution (count, class name):', list(zip(class_counts[sorted_ids].astype(int), np.array(dataset_val.classes)[sorted_ids])))\n",
    "#     plt.barh(range(len(dataset_val.classes)), width=class_counts[sorted_ids])\n",
    "#     plt.yticks(range(len(dataset_val.classes)), np.array(dataset_val.classes)[sorted_ids])\n",
    "#     plt.gca().margins(y=0)\n",
    "#     plt.grid()\n",
    "#     plt.title('Label distribution')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bff5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'seq', 'url', 'deidentification_x', '간편식', '건강간식', '건강식',\n",
      "       '건강음료', '걷기/산책', '격투기', '골프', '기타식단', '기타운동', '농구', '달리기/조깅', '당구',\n",
      "       '등산/등반', '루틴기록', '맨몸', '무술', '배구', '배드민턴', '보조제', '보충제', '볼링', '수상스포츠',\n",
      "       '스키/스노보드', '승마', '신체기록', '야구', '온라인클래스', '요가', '운동기구', '운동용품', '웨이트',\n",
      "       '유산소기록', '의류', '일반간식', '일반식', '일반음료', '일상생활', '자전거', '종합운동', '줄넘기',\n",
      "       '축구/풋살', '탁구', '테니스', '폴댄스', '필라테스', '홈트'],\n",
      "      dtype='object')\n",
      "['n']\n",
      "215145\n"
     ]
    }
   ],
   "source": [
    "#Get image dataset\n",
    "dataset = os.path.join(dataset_path, \"20211201_image_dataset(change_url).csv\")\n",
    "whole_df = pd.read_csv(dataset)\n",
    "print(whole_df.columns)\n",
    "print(whole_df['deidentification_x'].unique())\n",
    "print(len(whole_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0505f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', '간편식', '건강간식', '건강식', '건강음료', '걷기/산책', '격투기', '골프', '기타식단',\n",
      "       '기타운동', '농구', '달리기/조깅', '당구', '등산/등반', '루틴기록', '맨몸', '무술', '배구', '배드민턴',\n",
      "       '보조제', '보충제', '볼링', '수상스포츠', '스키/스노보드', '승마', '신체기록', '야구', '온라인클래스',\n",
      "       '요가', '운동기구', '운동용품', '웨이트', '유산소기록', '의류', '일반간식', '일반식', '일반음료',\n",
      "       '일상생활', '자전거', '종합운동', '줄넘기', '축구/풋살', '탁구', '테니스', '폴댄스', '필라테스',\n",
      "       '홈트'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>간편식</th>\n",
       "      <th>건강간식</th>\n",
       "      <th>건강식</th>\n",
       "      <th>건강음료</th>\n",
       "      <th>걷기/산책</th>\n",
       "      <th>격투기</th>\n",
       "      <th>골프</th>\n",
       "      <th>기타식단</th>\n",
       "      <th>기타운동</th>\n",
       "      <th>...</th>\n",
       "      <th>일상생활</th>\n",
       "      <th>자전거</th>\n",
       "      <th>종합운동</th>\n",
       "      <th>줄넘기</th>\n",
       "      <th>축구/풋살</th>\n",
       "      <th>탁구</th>\n",
       "      <th>테니스</th>\n",
       "      <th>폴댄스</th>\n",
       "      <th>필라테스</th>\n",
       "      <th>홈트</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/ubuntu/Desktop/Project/datasets/circlin_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  간편식  건강간식  건강식  건강음료  \\\n",
       "0  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "1  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "2  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "3  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "4  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "5  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "6  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "7  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     1    0     0   \n",
       "8  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    0     0   \n",
       "9  /home/ubuntu/Desktop/Project/datasets/circlin_...    0     0    1     0   \n",
       "\n",
       "   걷기/산책  격투기  골프  기타식단  기타운동  ...  일상생활  자전거  종합운동  줄넘기  축구/풋살  탁구  테니스  폴댄스  \\\n",
       "0      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "1      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "2      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "3      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "4      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "5      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "6      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "7      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "8      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "9      0    0   0     0     0  ...     0    0     0    0      0   0    0    0   \n",
       "\n",
       "   필라테스  홈트  \n",
       "0     0   0  \n",
       "1     0   0  \n",
       "2     0   0  \n",
       "3     0   0  \n",
       "4     0   0  \n",
       "5     0   1  \n",
       "6     0   0  \n",
       "7     0   0  \n",
       "8     0   0  \n",
       "9     0   0  \n",
       "\n",
       "[10 rows x 47 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop useless features/columns\n",
    "copy_df = whole_df.copy()\n",
    "copy_df.drop(labels=['index', 'seq', 'deidentification_x'], axis=1, inplace=True)\n",
    "print(copy_df.columns)\n",
    "copy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b464ec3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123.675, 116.28, 103.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torchvision/transforms/transforms.py:1362: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torchvision/transforms/transforms.py:1376: UserWarning: Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\n",
      "  \"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"\n"
     ]
    }
   ],
   "source": [
    "#Train - validation split\n",
    "train_size = 0.8\n",
    "train_df = copy_df.copy().sample(frac=train_size, random_state=200).reset_index(drop=True)\n",
    "val_df = copy_df.drop(train_df.index).reset_index(drop=True)\n",
    "# train_dataset = CustomDataset(splitted_train_df, tokenizer, MAX_LEN)\n",
    "# valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "# Train preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomAffine(degrees=20, \n",
    "                            translate=(0.2, 0.2),\n",
    "                            scale=(0.5, 1.5),\n",
    "                            shear=None,\n",
    "                            resample=False, \n",
    "                            fillcolor=tuple(np.array(np.array(mean)*255).astype(int).tolist())),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "# Test preprocessing\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(tuple(np.array(np.array(mean)*255).tolist()))\n",
    "\n",
    "\n",
    "# valid_annotations = os.path.join(img_folder, 'small_test.json')\n",
    "# train_annotations = os.path.join(img_folder, 'small_train.json')\n",
    "# train_dataset = CustomDataset(img_folder, train_annotations, train_transform)\n",
    "# valid_dataset = CustomDataset(img_folder, valid_annotations, val_transform)\n",
    "\n",
    "train_dataset = CustomDataset(train_df, train_transform)\n",
    "valid_dataset = CustomDataset(val_df, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd955c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader =  torch.utils.data.DataLoader(train_dataset, \n",
    "                              batch_size=TRAIN_BATCH_SIZE, \n",
    "                              num_workers=NUM_WORKERS,  #0?\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "val_data_loader =  torch.utils.data.DataLoader(valid_dataset, \n",
    "                             batch_size=VALID_BATCH_SIZE, \n",
    "                             num_workers=NUM_WORKERS) #0?\n",
    "\n",
    "#num_train_batches = int(np.ceil(len(train_dataset) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41255b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To explore file shape.\n",
    "# batchlist = []\n",
    "# datalist = []\n",
    "# for batch_idx, data in enumerate(train_data_loader):\n",
    "#     #print(batch_idx, data)\n",
    "#     batchlist.append(batch_idx)\n",
    "#     datalist.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d76fb",
   "metadata": {},
   "source": [
    "# 5. Make feed image classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5095c0",
   "metadata": {},
   "source": [
    "## 5-1. Define functions that save checkpoint of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d59b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min #valid_loss_min.item()\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c09f1",
   "metadata": {},
   "source": [
    "## 5-2. Define Resnext50 model as a class\n",
    " - Use pytorch implemented pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a7eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class ResNeXt50Class(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "#         super().__init__()\n",
    "#         resnet = models.resnext50_32x4d(pretrained=True)\n",
    "#         resnet.fc = nn.Sequential(\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
    "#         )\n",
    "#         self.base_model = resnet\n",
    "        super(ResNeXt50Class, self).__init__()\n",
    "        self.resnext_model = models.resnext50_32x4d(pretrained=True)\n",
    "        self.resnext_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=self.resnext_model.fc.in_features, \n",
    "                      out_features=n_classes)\n",
    "        )\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.sigm(self.resnext_model(x))\n",
    "        \n",
    "        return output\n",
    "        #return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f8b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNeXt50Class(\n",
       "    (resnext_model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Sequential(\n",
       "        (0): Dropout(p=0.2, inplace=False)\n",
       "        (1): Linear(in_features=2048, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (sigm): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNeXt50Class(len(labels)) #or len(labels) #train_dataset.classes\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model) #Distributed\n",
    "#model = nn.parallel.DistributedDataParallel(model, device_ids=[0, 1]) #Distributed DataParallel  ===> Should use this!!!!!!!!!!!!!!!!!\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c183807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = make_optimizer(model, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec6432a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets = []\n",
    "val_outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d56e37",
   "metadata": {},
   "source": [
    "## 5-3. Training\n",
    " - __<u>Add Train Loss!!!!!!!!!!</u>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "832aef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.000001, 0.00001, 0.0001, 0.001]\n",
    "train_losses_lr = {}\n",
    "avg_train_losses_lr = {}\n",
    "val_losses_lr = {}\n",
    "avg_val_losses_lr = {}\n",
    "epoch_list = [int(x) for x in np.linspace(1, EPOCHS, EPOCHS).tolist()]\n",
    "print(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85d1c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs,\n",
    "                       training_loader,\n",
    "                       validation_loader,\n",
    "                       model,\n",
    "                       optimizer,\n",
    "                       checkpoint_path,\n",
    "                       best_model_path,\n",
    "                       metric_path,\n",
    "                       date):\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    train_loss_epoch = []\n",
    "    avg_train_loss_epoch = []    \n",
    "    val_loss_epoch = [] #append to val_loss_list\n",
    "    avg_val_loss_epoch = [] #append to avg_val_list\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        print(f'############# Epoch {epoch}: Training Start   #############')\n",
    "#         for images, targets in enumerate(training_loader):\n",
    "        for batch_idx, data in enumerate(training_loader):\n",
    "            images, targets = data[0], data[1]\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if batch_idx%5000==0:\n",
    "                print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "        print('############# Epoch {}: Training End     #############'.format(epoch))\n",
    "        train_loss_epoch.append(train_loss)\n",
    "        print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "\n",
    "        model.eval()\n",
    "   \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(validation_loader, 0):\n",
    "                images, targets = data[0], data[1]\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
    "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "            print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
    "           # calculate average losses\n",
    "#             print('before calculate avg train loss', train_loss)\n",
    "            val_loss_epoch.append(valid_loss) \n",
    "            avg_train_loss = train_loss/len(training_loader)\n",
    "            avg_valid_loss = valid_loss/len(validation_loader)\n",
    "            #Print training/validation statistics\n",
    "            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                avg_train_loss,\n",
    "                avg_valid_loss\n",
    "            ))\n",
    "            avg_train_loss_epoch.append(avg_train_loss)\n",
    "            avg_val_loss_epoch.append(avg_valid_loss) \n",
    "            \n",
    "\n",
    "            # create checkpoint variable and add important data\n",
    "            checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'valid_loss_min': avg_valid_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "              }\n",
    "\n",
    "            # save checkpoint\n",
    "            #save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "            save_ckp(checkpoint, False,  f\"{checkpoint_path}_{epoch}\", best_model_path)\n",
    "            \n",
    "\n",
    "            ## TODO: save the model if validation loss has decreased\n",
    "            if avg_valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,avg_valid_loss))\n",
    "                # save checkpoint as best model\n",
    "                #save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "                save_ckp(checkpoint, True,  f\"{checkpoint_path}_{epoch}\", best_model_path)\n",
    "                valid_loss_min = avg_valid_loss\n",
    "\n",
    "        now = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_text = f\"[{now}]: [Learning Rate {lr}, Epoch {epoch}] - train_loss = {train_loss}, avg_train_loss = {avg_train_loss}, validation_loss = {valid_loss}, avg_validation_loss = {avg_valid_loss}\\n\"\n",
    "        if os.path.isfile(os.path.join(metric_path, f\"metric_logs_{date}.txt\")):\n",
    "            with open(os.path.join(metric_path, f\"metric_logs_{date}.txt\"), 'a', encoding='utf-8') as f:\n",
    "                f.write(log_text)\n",
    "        else:\n",
    "            with open(os.path.join(metric_path, f\"metric_logs_{date}.txt\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(log_text)       \n",
    "        print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
    "#     train_loss_lr.append(train_loss_epoch)\n",
    "#     avg_train_losse_lr.append(avg_train_loss_epoch)        \n",
    "#     val_losses_lr.append(val_loss_epoch)\n",
    "#     avg_val_losses_lr.append(avg_val_loss_epoch)\n",
    "\n",
    "    train_losses_lr[lr] = train_loss_epoch\n",
    "    avg_train_losses_lr[lr] = avg_train_loss_epoch\n",
    "    val_losses_lr[lr] = val_loss_epoch\n",
    "    avg_val_losses_lr[lr] = avg_val_loss_epoch\n",
    "    print(f\"train_losses_lr for LR {lr}: \\n {train_losses_lr}\")\n",
    "    print(f\"avg_train_losses_lr for LR {lr}: \\n {avg_train_losses_lr}\")\n",
    "    print(f\"val_losses_lr for LR {lr}: \\n {val_losses_lr}\")\n",
    "    print(f\"avg_val_losses_lr {lr}: \\n {avg_val_losses_lr}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4feba3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# model = model.to(device)\n",
    "\n",
    "# # If more than one GPU is available we can use both to speed up the training.\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "# os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "# # # Loss function\n",
    "# # criterion = nn.BCELoss() #BCEWithLogitsLoss\n",
    "# # Tensoboard logger\n",
    "# logger = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "248e1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use threshold to define predicted labels and invoke sklearn's metrics with different averaging strategies.\n",
    "# def calculate_metrics(pred, target, threshold=0.5):\n",
    "#     pred = np.array(pred > threshold, dtype=float)\n",
    "#     return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "#             'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "#             'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "#             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187d033",
   "metadata": {},
   "source": [
    "### Set checkpoint path, best model's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02029cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(checkpoint_path, \"curr_ckpt\")\n",
    "best_model_path = os.path.join(checkpoint_path, \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd769d99",
   "metadata": {},
   "source": [
    "### Training start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33a3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 1e-06 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n",
      "Epoch: 1, Training Loss:  0.6826050877571106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf-bert-text-classification/lib/python3.7/site-packages/torch/cuda/nccl.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000126 \tAverage Validation Loss: 0.000217\n",
      "Validation loss decreased (inf --> 0.000217).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.16546469926834106\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000045 \tAverage Validation Loss: 0.000122\n",
      "Validation loss decreased (0.000217 --> 0.000122).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.10442551225423813\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000031 \tAverage Validation Loss: 0.000097\n",
      "Validation loss decreased (0.000122 --> 0.000097).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.07537321746349335\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000026 \tAverage Validation Loss: 0.000088\n",
      "Validation loss decreased (0.000097 --> 0.000088).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.06865069270133972\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000024 \tAverage Validation Loss: 0.000083\n",
      "Validation loss decreased (0.000088 --> 0.000083).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.0563991479575634\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000023 \tAverage Validation Loss: 0.000081\n",
      "Validation loss decreased (0.000083 --> 0.000081).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.056859321892261505\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000023 \tAverage Validation Loss: 0.000079\n",
      "Validation loss decreased (0.000081 --> 0.000079).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.07677916437387466\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000022 \tAverage Validation Loss: 0.000077\n",
      "Validation loss decreased (0.000079 --> 0.000077).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.06080915778875351\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000022 \tAverage Validation Loss: 0.000075\n",
      "Validation loss decreased (0.000077 --> 0.000075).  Saving model ...\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.06093287467956543\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000021 \tAverage Validation Loss: 0.000074\n",
      "Validation loss decreased (0.000075 --> 0.000074).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.0566367581486702\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000021 \tAverage Validation Loss: 0.000073\n",
      "Validation loss decreased (0.000074 --> 0.000073).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.055342674255371094\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000021 \tAverage Validation Loss: 0.000073\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.04999132826924324\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000073 --> 0.000071).  Saving model ...\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.06271031498908997\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000071\n",
      "Validation loss decreased (0.000071 --> 0.000071).  Saving model ...\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.06242982670664787\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000071 --> 0.000070).  Saving model ...\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.046316348016262054\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000070\n",
      "Validation loss decreased (0.000070 --> 0.000070).  Saving model ...\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.06198061630129814\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000020 \tAverage Validation Loss: 0.000069\n",
      "Validation loss decreased (0.000070 --> 0.000069).  Saving model ...\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.05204596742987633\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000069 --> 0.000068).  Saving model ...\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n",
      "Epoch: 19, Training Loss:  0.050728071480989456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000068\n",
      "Validation loss decreased (0.000068 --> 0.000068).  Saving model ...\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.040961649268865585\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000067\n",
      "Validation loss decreased (0.000068 --> 0.000067).  Saving model ...\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.05180855467915535\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000067 --> 0.000066).  Saving model ...\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.04826030507683754\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000066\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.04826390743255615\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000066\n",
      "Validation loss decreased (0.000066 --> 0.000066).  Saving model ...\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.043752919882535934\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000065\n",
      "Validation loss decreased (0.000066 --> 0.000065).  Saving model ...\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.037599194794893265\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000019 \tAverage Validation Loss: 0.000065\n",
      "Validation loss decreased (0.000065 --> 0.000065).  Saving model ...\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.04199705272912979\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000065 --> 0.000064).  Saving model ...\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.053642388433218\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000064 --> 0.000064).  Saving model ...\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.05184389278292656\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000064\n",
      "Validation loss decreased (0.000064 --> 0.000064).  Saving model ...\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.06200254708528519\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000064\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.05427024886012077\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000063\n",
      "Validation loss decreased (0.000064 --> 0.000063).  Saving model ...\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.04328250512480736\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000063 --> 0.000062).  Saving model ...\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.04289558157324791\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.04222312569618225\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.04391084238886833\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000062\n",
      "Validation loss decreased (0.000062 --> 0.000062).  Saving model ...\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.04085920378565788\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000061\n",
      "Validation loss decreased (0.000062 --> 0.000061).  Saving model ...\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.04010822996497154\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000061\n",
      "Validation loss decreased (0.000061 --> 0.000061).  Saving model ...\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "train_losses_lr for LR 1e-06: \n",
      " {1e-06: [0.33936317726088605, 0.1213365623463531, 0.0830273938269344, 0.0710118369318133, 0.06581515137642327, 0.06283728286136304, 0.0609481826413516, 0.05945030035843816, 0.05843809462859318, 0.057416790035820245, 0.056564377312568866, 0.055776344396813955, 0.055058868322763965, 0.05454401281624533, 0.05389113375254164, 0.05337280319853318, 0.05290001034930582, 0.052346642827701795, 0.05209596332213874, 0.05152972416544255, 0.05121221266515121, 0.050816509529100364, 0.050474771825952, 0.05014117995631016, 0.04997764244625764, 0.049506752757655495, 0.049231246312579496, 0.04900216373489237, 0.04872369645984251, 0.04836945551526885, 0.04817073963313753, 0.04791046419538015, 0.04768841113866091, 0.04740075082315545, 0.04719765532682847, 0.046954586904519556]}\n",
      "avg_train_losses_lr for LR 1e-06: \n",
      " {1e-06: [0.00012620423103788995, 4.512330321545299e-05, 3.08766804860299e-05, 2.6408269591600334e-05, 2.4475697797107946e-05, 2.3368271796713666e-05, 2.2665742893771514e-05, 2.2108702253045058e-05, 2.1732277660317286e-05, 2.1352469332770637e-05, 2.1035469435689425e-05, 2.0742411452887304e-05, 2.0475592533567855e-05, 2.0284125257064087e-05, 2.0041329026605295e-05, 1.9848569430469757e-05, 1.967274464459123e-05, 1.946695530966969e-05, 1.9373731246611656e-05, 1.9163155137762197e-05, 1.9045077227650136e-05, 1.8897920985161905e-05, 1.877083370247378e-05, 1.8646775736820436e-05, 1.8585958514785286e-05, 1.841084148666995e-05, 1.8308384645808665e-05, 1.8223192166192774e-05, 1.8119634235716813e-05, 1.798789717934877e-05, 1.7913997632256427e-05, 1.78172049815471e-05, 1.773462667856486e-05, 1.7627649990016904e-05, 1.755212172808794e-05, 1.7461728116221478e-05]}\n",
      "val_losses_lr for LR 1e-06: \n",
      " {1e-06: [0.14613198764611152, 0.0819366471883446, 0.06544178859905006, 0.058948262573882686, 0.055987989532735025, 0.05424439567203505, 0.05286044122604168, 0.05183559753842106, 0.05061721248222856, 0.04989088045496818, 0.049076575550622954, 0.049096814881218635, 0.048077993230537165, 0.04756927373715265, 0.047071912504433704, 0.04697707751530598, 0.046247575088870514, 0.04597514845917961, 0.045621619843511145, 0.045263804625415335, 0.04457331534314561, 0.04458316672665317, 0.04412414957208756, 0.04390929248543766, 0.043693150865553386, 0.04324363160430061, 0.04301613357809291, 0.04277135255188858, 0.04306749104180364, 0.04221094544163251, 0.041959734711059835, 0.04179363360387695, 0.04156281110203728, 0.04147626279146142, 0.04135078839393112, 0.04089249535166225]}\n",
      "avg_val_losses_lr 1e-06: \n",
      " {1e-06: [0.00021713519709674816, 0.00012174836134969481, 9.7238913222957e-05, 8.759028614247056e-05, 8.319166349589157e-05, 8.060088509960633e-05, 7.854448919174097e-05, 7.702169025025418e-05, 7.521131126631286e-05, 7.413206605493043e-05, 7.292210334416487e-05, 7.295217664371268e-05, 7.143832575116964e-05, 7.06824275440604e-05, 6.99434063958896e-05, 6.980249259332241e-05, 6.871853653621176e-05, 6.83137421384541e-05, 6.778843958916961e-05, 6.725676764549084e-05, 6.623078059902766e-05, 6.624541861315479e-05, 6.556337232108107e-05, 6.524411959203219e-05, 6.492295819547308e-05, 6.425502467206628e-05, 6.391698897190626e-05, 6.355327273683296e-05, 6.399330021070379e-05, 6.272057272159362e-05, 6.234730269102501e-05, 6.21004956966968e-05, 6.175752021105094e-05, 6.162891945239438e-05, 6.144247904001653e-05, 6.07615086948919e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 1e-05 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.04195743799209595\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000018 \tAverage Validation Loss: 0.000061\n",
      "Validation loss decreased (inf --> 0.000061).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.047795865684747696\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000058\n",
      "Validation loss decreased (0.000061 --> 0.000058).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.0471530556678772\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000017 \tAverage Validation Loss: 0.000057\n",
      "Validation loss decreased (0.000058 --> 0.000057).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.05188098922371864\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000055\n",
      "Validation loss decreased (0.000057 --> 0.000055).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.04564160108566284\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000054\n",
      "Validation loss decreased (0.000055 --> 0.000054).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.041055262088775635\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000053\n",
      "Validation loss decreased (0.000054 --> 0.000053).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.052259787917137146\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000051\n",
      "Validation loss decreased (0.000053 --> 0.000051).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.04371563345193863\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000050\n",
      "Validation loss decreased (0.000051 --> 0.000050).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.036573976278305054\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000048\n",
      "Validation loss decreased (0.000050 --> 0.000048).  Saving model ...\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.04659096524119377\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000048\n",
      "Validation loss decreased (0.000048 --> 0.000048).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.04511266201734543\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000046\n",
      "Validation loss decreased (0.000048 --> 0.000046).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.04571138694882393\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000046\n",
      "Validation loss decreased (0.000046 --> 0.000046).  Saving model ...\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.02346760407090187\n",
      "############# Epoch 13: Training End     #############\n",
      "############# Epoch 13: Validation Start   #############\n",
      "############# Epoch 13: Validation End     #############\n",
      "Epoch: 13 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000044\n",
      "Validation loss decreased (0.000046 --> 0.000044).  Saving model ...\n",
      "############# Epoch 13  Done   #############\n",
      "\n",
      "############# Epoch 14: Training Start   #############\n",
      "Epoch: 14, Training Loss:  0.03410204499959946\n",
      "############# Epoch 14: Training End     #############\n",
      "############# Epoch 14: Validation Start   #############\n",
      "############# Epoch 14: Validation End     #############\n",
      "Epoch: 14 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000044\n",
      "############# Epoch 14  Done   #############\n",
      "\n",
      "############# Epoch 15: Training Start   #############\n",
      "Epoch: 15, Training Loss:  0.02698303386569023\n",
      "############# Epoch 15: Training End     #############\n",
      "############# Epoch 15: Validation Start   #############\n",
      "############# Epoch 15: Validation End     #############\n",
      "Epoch: 15 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000044\n",
      "Validation loss decreased (0.000044 --> 0.000044).  Saving model ...\n",
      "############# Epoch 15  Done   #############\n",
      "\n",
      "############# Epoch 16: Training Start   #############\n",
      "Epoch: 16, Training Loss:  0.0316336564719677\n",
      "############# Epoch 16: Training End     #############\n",
      "############# Epoch 16: Validation Start   #############\n",
      "############# Epoch 16: Validation End     #############\n",
      "Epoch: 16 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000043\n",
      "Validation loss decreased (0.000044 --> 0.000043).  Saving model ...\n",
      "############# Epoch 16  Done   #############\n",
      "\n",
      "############# Epoch 17: Training Start   #############\n",
      "Epoch: 17, Training Loss:  0.02867109887301922\n",
      "############# Epoch 17: Training End     #############\n",
      "############# Epoch 17: Validation Start   #############\n",
      "############# Epoch 17: Validation End     #############\n",
      "Epoch: 17 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000042\n",
      "Validation loss decreased (0.000043 --> 0.000042).  Saving model ...\n",
      "############# Epoch 17  Done   #############\n",
      "\n",
      "############# Epoch 18: Training Start   #############\n",
      "Epoch: 18, Training Loss:  0.02543179877102375\n",
      "############# Epoch 18: Training End     #############\n",
      "############# Epoch 18: Validation Start   #############\n",
      "############# Epoch 18: Validation End     #############\n",
      "Epoch: 18 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000041\n",
      "Validation loss decreased (0.000042 --> 0.000041).  Saving model ...\n",
      "############# Epoch 18  Done   #############\n",
      "\n",
      "############# Epoch 19: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss:  0.025881508365273476\n",
      "############# Epoch 19: Training End     #############\n",
      "############# Epoch 19: Validation Start   #############\n",
      "############# Epoch 19: Validation End     #############\n",
      "Epoch: 19 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000040\n",
      "Validation loss decreased (0.000041 --> 0.000040).  Saving model ...\n",
      "############# Epoch 19  Done   #############\n",
      "\n",
      "############# Epoch 20: Training Start   #############\n",
      "Epoch: 20, Training Loss:  0.031863532960414886\n",
      "############# Epoch 20: Training End     #############\n",
      "############# Epoch 20: Validation Start   #############\n",
      "############# Epoch 20: Validation End     #############\n",
      "Epoch: 20 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000039\n",
      "Validation loss decreased (0.000040 --> 0.000039).  Saving model ...\n",
      "############# Epoch 20  Done   #############\n",
      "\n",
      "############# Epoch 21: Training Start   #############\n",
      "Epoch: 21, Training Loss:  0.02346322126686573\n",
      "############# Epoch 21: Training End     #############\n",
      "############# Epoch 21: Validation Start   #############\n",
      "############# Epoch 21: Validation End     #############\n",
      "Epoch: 21 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000039\n",
      "############# Epoch 21  Done   #############\n",
      "\n",
      "############# Epoch 22: Training Start   #############\n",
      "Epoch: 22, Training Loss:  0.022065280005335808\n",
      "############# Epoch 22: Training End     #############\n",
      "############# Epoch 22: Validation Start   #############\n",
      "############# Epoch 22: Validation End     #############\n",
      "Epoch: 22 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000039\n",
      "Validation loss decreased (0.000039 --> 0.000039).  Saving model ...\n",
      "############# Epoch 22  Done   #############\n",
      "\n",
      "############# Epoch 23: Training Start   #############\n",
      "Epoch: 23, Training Loss:  0.02254408970475197\n",
      "############# Epoch 23: Training End     #############\n",
      "############# Epoch 23: Validation Start   #############\n",
      "############# Epoch 23: Validation End     #############\n",
      "Epoch: 23 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000038\n",
      "Validation loss decreased (0.000039 --> 0.000038).  Saving model ...\n",
      "############# Epoch 23  Done   #############\n",
      "\n",
      "############# Epoch 24: Training Start   #############\n",
      "Epoch: 24, Training Loss:  0.02679668366909027\n",
      "############# Epoch 24: Training End     #############\n",
      "############# Epoch 24: Validation Start   #############\n",
      "############# Epoch 24: Validation End     #############\n",
      "Epoch: 24 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000037\n",
      "Validation loss decreased (0.000038 --> 0.000037).  Saving model ...\n",
      "############# Epoch 24  Done   #############\n",
      "\n",
      "############# Epoch 25: Training Start   #############\n",
      "Epoch: 25, Training Loss:  0.02424260601401329\n",
      "############# Epoch 25: Training End     #############\n",
      "############# Epoch 25: Validation Start   #############\n",
      "############# Epoch 25: Validation End     #############\n",
      "Epoch: 25 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000037\n",
      "Validation loss decreased (0.000037 --> 0.000037).  Saving model ...\n",
      "############# Epoch 25  Done   #############\n",
      "\n",
      "############# Epoch 26: Training Start   #############\n",
      "Epoch: 26, Training Loss:  0.027089212089776993\n",
      "############# Epoch 26: Training End     #############\n",
      "############# Epoch 26: Validation Start   #############\n",
      "############# Epoch 26: Validation End     #############\n",
      "Epoch: 26 \tAvgerage Training Loss: 0.000010 \tAverage Validation Loss: 0.000037\n",
      "############# Epoch 26  Done   #############\n",
      "\n",
      "############# Epoch 27: Training Start   #############\n",
      "Epoch: 27, Training Loss:  0.022419968619942665\n",
      "############# Epoch 27: Training End     #############\n",
      "############# Epoch 27: Validation Start   #############\n",
      "############# Epoch 27: Validation End     #############\n",
      "Epoch: 27 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000036\n",
      "Validation loss decreased (0.000037 --> 0.000036).  Saving model ...\n",
      "############# Epoch 27  Done   #############\n",
      "\n",
      "############# Epoch 28: Training Start   #############\n",
      "Epoch: 28, Training Loss:  0.018864966928958893\n",
      "############# Epoch 28: Training End     #############\n",
      "############# Epoch 28: Validation Start   #############\n",
      "############# Epoch 28: Validation End     #############\n",
      "Epoch: 28 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000035\n",
      "Validation loss decreased (0.000036 --> 0.000035).  Saving model ...\n",
      "############# Epoch 28  Done   #############\n",
      "\n",
      "############# Epoch 29: Training Start   #############\n",
      "Epoch: 29, Training Loss:  0.02095479518175125\n",
      "############# Epoch 29: Training End     #############\n",
      "############# Epoch 29: Validation Start   #############\n",
      "############# Epoch 29: Validation End     #############\n",
      "Epoch: 29 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000035\n",
      "Validation loss decreased (0.000035 --> 0.000035).  Saving model ...\n",
      "############# Epoch 29  Done   #############\n",
      "\n",
      "############# Epoch 30: Training Start   #############\n",
      "Epoch: 30, Training Loss:  0.017421793192625046\n",
      "############# Epoch 30: Training End     #############\n",
      "############# Epoch 30: Validation Start   #############\n",
      "############# Epoch 30: Validation End     #############\n",
      "Epoch: 30 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000035\n",
      "Validation loss decreased (0.000035 --> 0.000035).  Saving model ...\n",
      "############# Epoch 30  Done   #############\n",
      "\n",
      "############# Epoch 31: Training Start   #############\n",
      "Epoch: 31, Training Loss:  0.027028586715459824\n",
      "############# Epoch 31: Training End     #############\n",
      "############# Epoch 31: Validation Start   #############\n",
      "############# Epoch 31: Validation End     #############\n",
      "Epoch: 31 \tAvgerage Training Loss: 0.000009 \tAverage Validation Loss: 0.000035\n",
      "############# Epoch 31  Done   #############\n",
      "\n",
      "############# Epoch 32: Training Start   #############\n",
      "Epoch: 32, Training Loss:  0.021035712212324142\n",
      "############# Epoch 32: Training End     #############\n",
      "############# Epoch 32: Validation Start   #############\n",
      "############# Epoch 32: Validation End     #############\n",
      "Epoch: 32 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000034\n",
      "Validation loss decreased (0.000035 --> 0.000034).  Saving model ...\n",
      "############# Epoch 32  Done   #############\n",
      "\n",
      "############# Epoch 33: Training Start   #############\n",
      "Epoch: 33, Training Loss:  0.017290811985731125\n",
      "############# Epoch 33: Training End     #############\n",
      "############# Epoch 33: Validation Start   #############\n",
      "############# Epoch 33: Validation End     #############\n",
      "Epoch: 33 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000034\n",
      "############# Epoch 33  Done   #############\n",
      "\n",
      "############# Epoch 34: Training Start   #############\n",
      "Epoch: 34, Training Loss:  0.013464215211570263\n",
      "############# Epoch 34: Training End     #############\n",
      "############# Epoch 34: Validation Start   #############\n",
      "############# Epoch 34: Validation End     #############\n",
      "Epoch: 34 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000034\n",
      "Validation loss decreased (0.000034 --> 0.000034).  Saving model ...\n",
      "############# Epoch 34  Done   #############\n",
      "\n",
      "############# Epoch 35: Training Start   #############\n",
      "Epoch: 35, Training Loss:  0.017203787341713905\n",
      "############# Epoch 35: Training End     #############\n",
      "############# Epoch 35: Validation Start   #############\n",
      "############# Epoch 35: Validation End     #############\n",
      "Epoch: 35 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000034\n",
      "Validation loss decreased (0.000034 --> 0.000034).  Saving model ...\n",
      "############# Epoch 35  Done   #############\n",
      "\n",
      "############# Epoch 36: Training Start   #############\n",
      "Epoch: 36, Training Loss:  0.023033281788229942\n",
      "############# Epoch 36: Training End     #############\n",
      "############# Epoch 36: Validation Start   #############\n",
      "############# Epoch 36: Validation End     #############\n",
      "Epoch: 36 \tAvgerage Training Loss: 0.000008 \tAverage Validation Loss: 0.000033\n",
      "Validation loss decreased (0.000034 --> 0.000033).  Saving model ...\n",
      "############# Epoch 36  Done   #############\n",
      "\n",
      "train_losses_lr for LR 1e-05: \n",
      " {1e-06: [0.33936317726088605, 0.1213365623463531, 0.0830273938269344, 0.0710118369318133, 0.06581515137642327, 0.06283728286136304, 0.0609481826413516, 0.05945030035843816, 0.05843809462859318, 0.057416790035820245, 0.056564377312568866, 0.055776344396813955, 0.055058868322763965, 0.05454401281624533, 0.05389113375254164, 0.05337280319853318, 0.05290001034930582, 0.052346642827701795, 0.05209596332213874, 0.05152972416544255, 0.05121221266515121, 0.050816509529100364, 0.050474771825952, 0.05014117995631016, 0.04997764244625764, 0.049506752757655495, 0.049231246312579496, 0.04900216373489237, 0.04872369645984251, 0.04836945551526885, 0.04817073963313753, 0.04791046419538015, 0.04768841113866091, 0.04740075082315545, 0.04719765532682847, 0.046954586904519556], 1e-05: [0.04765121977916204, 0.04595591642957235, 0.04462174570424131, 0.04343502953933949, 0.042247211911108515, 0.04115811217813168, 0.04011044016349658, 0.03911155595954302, 0.038220250663199, 0.037417173879508575, 0.036482947139092184, 0.0356548163408116, 0.03485584802656823, 0.03396771763970573, 0.033264584453515406, 0.03242440027620836, 0.03163213133687255, 0.030907722210803193, 0.030201822033379833, 0.02956649132229472, 0.028852856599207326, 0.02824979933798204, 0.02752705575055281, 0.026983768159337243, 0.026273522522562334, 0.025715238397733858, 0.02518617064204085, 0.024586745965553942, 0.02405146121086576, 0.023464684137383916, 0.02305443962539277, 0.02249564010651301, 0.022006244267388595, 0.021620698873398396, 0.021058492255478557, 0.020747907265782514]}\n",
      "avg_train_losses_lr for LR 1e-05: \n",
      " {1e-06: [0.00012620423103788995, 4.512330321545299e-05, 3.08766804860299e-05, 2.6408269591600334e-05, 2.4475697797107946e-05, 2.3368271796713666e-05, 2.2665742893771514e-05, 2.2108702253045058e-05, 2.1732277660317286e-05, 2.1352469332770637e-05, 2.1035469435689425e-05, 2.0742411452887304e-05, 2.0475592533567855e-05, 2.0284125257064087e-05, 2.0041329026605295e-05, 1.9848569430469757e-05, 1.967274464459123e-05, 1.946695530966969e-05, 1.9373731246611656e-05, 1.9163155137762197e-05, 1.9045077227650136e-05, 1.8897920985161905e-05, 1.877083370247378e-05, 1.8646775736820436e-05, 1.8585958514785286e-05, 1.841084148666995e-05, 1.8308384645808665e-05, 1.8223192166192774e-05, 1.8119634235716813e-05, 1.798789717934877e-05, 1.7913997632256427e-05, 1.78172049815471e-05, 1.773462667856486e-05, 1.7627649990016904e-05, 1.755212172808794e-05, 1.7461728116221478e-05], 1e-05: [1.772079575275643e-05, 1.7090337087977818e-05, 1.6594178395032098e-05, 1.6152855909014315e-05, 1.571112380480049e-05, 1.5306103450402262e-05, 1.4916489462066412e-05, 1.454501895111306e-05, 1.4213555471624768e-05, 1.391490289308612e-05, 1.3567477552656074e-05, 1.3259507750394794e-05, 1.2962383051903396e-05, 1.2632100275085804e-05, 1.2370615267205431e-05, 1.2058162988549037e-05, 1.1763529690172016e-05, 1.1494132469618146e-05, 1.1231618457932254e-05, 1.0995348204646605e-05, 1.0729957827894134e-05, 1.050568960133211e-05, 1.0236911770380367e-05, 1.0034871015000833e-05, 9.770740990168216e-06, 9.56312324199846e-06, 9.366370636683098e-06, 9.143453315564872e-06, 8.944388698722856e-06, 8.726174837256942e-06, 8.573610868498611e-06, 8.365801452775385e-06, 8.183802256373595e-06, 8.040423530456822e-06, 7.831347064142267e-06, 7.715845022604133e-06]}\n",
      "val_losses_lr for LR 1e-05: \n",
      " {1e-06: [0.14613198764611152, 0.0819366471883446, 0.06544178859905006, 0.058948262573882686, 0.055987989532735025, 0.05424439567203505, 0.05286044122604168, 0.05183559753842106, 0.05061721248222856, 0.04989088045496818, 0.049076575550622954, 0.049096814881218635, 0.048077993230537165, 0.04756927373715265, 0.047071912504433704, 0.04697707751530598, 0.046247575088870514, 0.04597514845917961, 0.045621619843511145, 0.045263804625415335, 0.04457331534314561, 0.04458316672665317, 0.04412414957208756, 0.04390929248543766, 0.043693150865553386, 0.04324363160430061, 0.04301613357809291, 0.04277135255188858, 0.04306749104180364, 0.04221094544163251, 0.041959734711059835, 0.04179363360387695, 0.04156281110203728, 0.04147626279146142, 0.04135078839393112, 0.04089249535166225], 1e-05: [0.04085804145633729, 0.03905622936220611, 0.0382135359689394, 0.03720090285297844, 0.036418203383484865, 0.035434235997199116, 0.034464043151327096, 0.03395446049627939, 0.03251521058386138, 0.03199549152877014, 0.031261745665947485, 0.03071449943202342, 0.029831783497293696, 0.029886900852773377, 0.02935967535156906, 0.028669837080385056, 0.028077453042741687, 0.02775333560781793, 0.027084066013613208, 0.0262748264094585, 0.026308932084241458, 0.026076009101268994, 0.025272419315003073, 0.0252074628672597, 0.02465504229367666, 0.024808208365408476, 0.02429505172330901, 0.02358577100291605, 0.023584239771603458, 0.023326063611687195, 0.023410202248262752, 0.022934700380374214, 0.023166713019747147, 0.022817612851224002, 0.022731030029819, 0.02226976936414595]}\n",
      "avg_val_losses_lr 1e-05: \n",
      " {1e-06: [0.00021713519709674816, 0.00012174836134969481, 9.7238913222957e-05, 8.759028614247056e-05, 8.319166349589157e-05, 8.060088509960633e-05, 7.854448919174097e-05, 7.702169025025418e-05, 7.521131126631286e-05, 7.413206605493043e-05, 7.292210334416487e-05, 7.295217664371268e-05, 7.143832575116964e-05, 7.06824275440604e-05, 6.99434063958896e-05, 6.980249259332241e-05, 6.871853653621176e-05, 6.83137421384541e-05, 6.778843958916961e-05, 6.725676764549084e-05, 6.623078059902766e-05, 6.624541861315479e-05, 6.556337232108107e-05, 6.524411959203219e-05, 6.492295819547308e-05, 6.425502467206628e-05, 6.391698897190626e-05, 6.355327273683296e-05, 6.399330021070379e-05, 6.272057272159362e-05, 6.234730269102501e-05, 6.21004956966968e-05, 6.175752021105094e-05, 6.162891945239438e-05, 6.144247904001653e-05, 6.07615086948919e-05], 1e-05: [6.071031419960965e-05, 5.803303025587832e-05, 5.678088554077177e-05, 5.527623009357866e-05, 5.4113229395965624e-05, 5.265116790074163e-05, 5.120957377611753e-05, 5.045239301081633e-05, 4.831383444853102e-05, 4.7541592167563355e-05, 4.645133085579121e-05, 4.563818637744936e-05, 4.4326572804299695e-05, 4.440847080649833e-05, 4.362507481659593e-05, 4.2600055097154614e-05, 4.171984107391038e-05, 4.1238240130487266e-05, 4.0243783081148896e-05, 3.904134681940342e-05, 3.909202389931866e-05, 3.8745927342153035e-05, 3.755188605498228e-05, 3.745536830202036e-05, 3.6634535354645856e-05, 3.686212238545093e-05, 3.6099631089612204e-05, 3.504572214400602e-05, 3.5043446911743625e-05, 3.465982706045646e-05, 3.478484732282727e-05, 3.407830665731681e-05, 3.442305054940141e-05, 3.390432815932244e-05, 3.377567612157355e-05, 3.3090296232014785e-05]}\n",
      "\n",
      "\n",
      "##########################################################\n",
      "##########################################################\n",
      "############### Training for learning rate 0.0001 START! ###############\n",
      "##########################################################\n",
      "##########################################################\n",
      "\n",
      "\n",
      "############# Epoch 1: Training Start   #############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.017691606655716896\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000016 \tAverage Validation Loss: 0.000053\n",
      "Validation loss decreased (inf --> 0.000053).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "Epoch: 2, Training Loss:  0.041801951825618744\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000054\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "Epoch: 3, Training Loss:  0.0452173575758934\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000050\n",
      "Validation loss decreased (0.000053 --> 0.000050).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "Epoch: 4, Training Loss:  0.036152515560388565\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000015 \tAverage Validation Loss: 0.000048\n",
      "Validation loss decreased (0.000050 --> 0.000048).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "Epoch: 5, Training Loss:  0.028370676562190056\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n",
      "############# Epoch 5: Validation End     #############\n",
      "Epoch: 5 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000048\n",
      "Validation loss decreased (0.000048 --> 0.000048).  Saving model ...\n",
      "############# Epoch 5  Done   #############\n",
      "\n",
      "############# Epoch 6: Training Start   #############\n",
      "Epoch: 6, Training Loss:  0.03736727684736252\n",
      "############# Epoch 6: Training End     #############\n",
      "############# Epoch 6: Validation Start   #############\n",
      "############# Epoch 6: Validation End     #############\n",
      "Epoch: 6 \tAvgerage Training Loss: 0.000014 \tAverage Validation Loss: 0.000046\n",
      "Validation loss decreased (0.000048 --> 0.000046).  Saving model ...\n",
      "############# Epoch 6  Done   #############\n",
      "\n",
      "############# Epoch 7: Training Start   #############\n",
      "Epoch: 7, Training Loss:  0.03012785315513611\n",
      "############# Epoch 7: Training End     #############\n",
      "############# Epoch 7: Validation Start   #############\n",
      "############# Epoch 7: Validation End     #############\n",
      "Epoch: 7 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000045\n",
      "Validation loss decreased (0.000046 --> 0.000045).  Saving model ...\n",
      "############# Epoch 7  Done   #############\n",
      "\n",
      "############# Epoch 8: Training Start   #############\n",
      "Epoch: 8, Training Loss:  0.026766067370772362\n",
      "############# Epoch 8: Training End     #############\n",
      "############# Epoch 8: Validation Start   #############\n",
      "############# Epoch 8: Validation End     #############\n",
      "Epoch: 8 \tAvgerage Training Loss: 0.000013 \tAverage Validation Loss: 0.000043\n",
      "Validation loss decreased (0.000045 --> 0.000043).  Saving model ...\n",
      "############# Epoch 8  Done   #############\n",
      "\n",
      "############# Epoch 9: Training Start   #############\n",
      "Epoch: 9, Training Loss:  0.030270742252469063\n",
      "############# Epoch 9: Training End     #############\n",
      "############# Epoch 9: Validation Start   #############\n",
      "############# Epoch 9: Validation End     #############\n",
      "Epoch: 9 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000042\n",
      "Validation loss decreased (0.000043 --> 0.000042).  Saving model ...\n",
      "############# Epoch 9  Done   #############\n",
      "\n",
      "############# Epoch 10: Training Start   #############\n",
      "Epoch: 10, Training Loss:  0.026186252012848854\n",
      "############# Epoch 10: Training End     #############\n",
      "############# Epoch 10: Validation Start   #############\n",
      "############# Epoch 10: Validation End     #############\n",
      "Epoch: 10 \tAvgerage Training Loss: 0.000012 \tAverage Validation Loss: 0.000042\n",
      "Validation loss decreased (0.000042 --> 0.000042).  Saving model ...\n",
      "############# Epoch 10  Done   #############\n",
      "\n",
      "############# Epoch 11: Training Start   #############\n",
      "Epoch: 11, Training Loss:  0.03634950891137123\n",
      "############# Epoch 11: Training End     #############\n",
      "############# Epoch 11: Validation Start   #############\n",
      "############# Epoch 11: Validation End     #############\n",
      "Epoch: 11 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000041\n",
      "Validation loss decreased (0.000042 --> 0.000041).  Saving model ...\n",
      "############# Epoch 11  Done   #############\n",
      "\n",
      "############# Epoch 12: Training Start   #############\n",
      "Epoch: 12, Training Loss:  0.03176363930106163\n",
      "############# Epoch 12: Training End     #############\n",
      "############# Epoch 12: Validation Start   #############\n",
      "############# Epoch 12: Validation End     #############\n",
      "Epoch: 12 \tAvgerage Training Loss: 0.000011 \tAverage Validation Loss: 0.000040\n",
      "Validation loss decreased (0.000041 --> 0.000040).  Saving model ...\n",
      "############# Epoch 12  Done   #############\n",
      "\n",
      "############# Epoch 13: Training Start   #############\n",
      "Epoch: 13, Training Loss:  0.024715563282370567\n"
     ]
    }
   ],
   "source": [
    "#For hyperparameter tuning\n",
    "for lr in learning_rate:\n",
    "    print('\\n')\n",
    "    print(f'##########################################################')\n",
    "    print(f'##########################################################')    \n",
    "    print(f'############### Training for learning rate {lr} START! ###############')\n",
    "    print(f'##########################################################')\n",
    "    print(f'##########################################################')\n",
    "    print('\\n')\n",
    "    optimizer = make_optimizer(model, lr)\n",
    "    train_model(EPOCHS,\n",
    "               train_data_loader,\n",
    "               val_data_loader,\n",
    "               model,\n",
    "               optimizer,\n",
    "               os.path.join(checkpoint_path, f\"curr_ckpt_{lr}\"),\n",
    "               best_model_path,\n",
    "               metric_path,\n",
    "               date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the training parameters.\n",
    "# num_workers = 8 # Number of CPU processes for data preprocessing\n",
    "# lr = 1e-4 # Learning rate\n",
    "# batch_size = 32\n",
    "# save_freq = 1 # Save checkpoint frequency (epochs)\n",
    "# test_freq = 200 # Test model frequency (iterations)\n",
    "# max_epoch_number = 35 # Number of epochs for training \n",
    "# # Note: on the small subset of data overfitting happens after 30-35 epochs\n",
    "\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "# # Run tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738eb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the dataloaders for training.\n",
    "# test_annotations = os.path.join(img_folder, 'small_test.json')\n",
    "# train_annotations = os.path.join(img_folder, 'small_train.json')\n",
    "\n",
    "# test_dataset = CustomDataset(img_folder, test_annotations, val_transform)\n",
    "# train_dataset = CustomDataset(img_folder, train_annotations, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here is an auxiliary function for checkpoint saving.\n",
    "# def checkpoint_save(model, save_path, epoch):\n",
    "#     f = os.path.join(save_path, 'checkpoint-{:06d}.pt'.format(epoch))\n",
    "#     if 'module' in dir(model):\n",
    "#         torch.save(model.module.state_dict(), f)\n",
    "#     else:\n",
    "#         torch.save(model.state_dict(), f)\n",
    "#     print('saved checkpoint:', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run training\n",
    "# epoch = 0\n",
    "# iteration = 0\n",
    "# while True:\n",
    "#     batch_losses = []\n",
    "#     for imgs, targets in train_dataloader:\n",
    "#         imgs, targets = imgs.to(device), targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         model_result = model(imgs)\n",
    "#         loss = criterion(model_result, targets.type(torch.float))\n",
    "\n",
    "#         batch_loss_value = loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         logger.add_scalar('train_loss', batch_loss_value, iteration)\n",
    "#         batch_losses.append(batch_loss_value)\n",
    "#         with torch.no_grad():\n",
    "#             result = calculate_metrics(model_result.cpu().numpy(), targets.cpu().numpy())\n",
    "#             for metric in result:\n",
    "#                 logger.add_scalar('train/' + metric, result[metric], iteration)\n",
    "\n",
    "#         if iteration % test_freq == 0:\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 model_result = []\n",
    "#                 targets = []\n",
    "#                 for imgs, batch_targets in test_dataloader:\n",
    "#                     imgs = imgs.to(device)\n",
    "#                     model_batch_result = model(imgs)\n",
    "#                     model_result.extend(model_batch_result.cpu().numpy())\n",
    "#                     targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "#             result = calculate_metrics(np.array(model_result), np.array(targets))\n",
    "#             for metric in result:\n",
    "#                 logger.add_scalar('test/' + metric, result[metric], iteration)\n",
    "#             print(\"epoch:{:2d} iter:{:3d} test: \"\n",
    "#                   \"micro f1: {:.3f} \"\n",
    "#                   \"macro f1: {:.3f} \"\n",
    "#                   \"samples f1: {:.3f}\".format(epoch, iteration,\n",
    "#                                               result['micro/f1'],\n",
    "#                                               result['macro/f1'],\n",
    "#                                               result['samples/f1']))\n",
    "\n",
    "#             model.train()\n",
    "#         iteration += 1\n",
    "\n",
    "#     loss_value = np.mean(batch_losses)\n",
    "#     print(\"epoch:{:2d} iter:{:3d} train: loss:{:.3f}\".format(epoch, iteration, loss_value))\n",
    "#     if epoch % save_freq == 0:\n",
    "#         checkpoint_save(model, checkpoint_path, epoch)\n",
    "#     epoch += 1\n",
    "#     if EPOCHS < epoch:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584f93a",
   "metadata": {},
   "source": [
    "### Check & Visualize validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimums of average validation loss per learning rate: {[min(x) for x in avg_val_losses_lr]}\")\n",
    "print(f\"Minimum average validation loss: {min([min(x) for x in avg_val_losses_lr])}\")\n",
    "avg_val_losses_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb57324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimums of average validation loss per learning rate: {[min(x) for x in val_losses_lr]}\")\n",
    "print(f\"Minimum of minums: {min([min(x) for x in val_losses_lr])}\")\n",
    "val_losses_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f889cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_losses_lr)):\n",
    "    plt.plot(epoch_list, val_losses_lr[i], '-o', label=learning_rate[i])\n",
    "    plt.xlabel('Epochs') #1 ~ 36\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(avg_val_losses_lr)):\n",
    "    plt.plot(epoch_list, avg_val_losses_lr[i], '-o', label=learning_rate[i])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('avg_val_loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47056b",
   "metadata": {},
   "source": [
    "__Check the losses and train again to get optimal model.__\n",
    "\n",
    "__~~Loss is too big when learning rate == 0.1. So remove and redraw graphs.~~__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738f5d9",
   "metadata": {},
   "source": [
    "__Minimum validation loss:  Learning rate = ????, epoch = ??__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe9f1b",
   "metadata": {},
   "source": [
    "## 5-5. Evaluate model\n",
    " - Test with validation dataset\n",
    " - 1st important index: Precision\n",
    " - 2nd importand index: Recall\n",
    "   - __First, get high & stable <u>Precision</u>, then improve <u>Recall</u>.__\n",
    " - And other indexes: F1 score, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad69044",
   "metadata": {},
   "source": [
    "### 5-5-1. Check loss, precision(recall) according to learning rate, batch size, optimizer (& epoch)\n",
    "- x: learning rate / y: validation loss\n",
    "- x: learning rate / y: precision or recall\n",
    "- ~~x: epoch / y: validation loss~~\n",
    "- ~~x: epoch / y: precision or recall~~\n",
    "- __epoch = 16__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc51a24",
   "metadata": {},
   "source": [
    "# 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb4b67",
   "metadata": {},
   "source": [
    "## 6-1. Define preprocessing function for input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af390d",
   "metadata": {},
   "source": [
    "## 6-2. Load saved model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "load_model = ResNeXt50Class()\n",
    "load_model = load_model.cuda() #for GPU computation\n",
    "load_model = nn.DataParallel(load_model) # Distributed\n",
    "best_model_path = os.path.join(checkpoint_path, \"20211126/curr_ckpt_1e-05_16\") #currently best model state.\n",
    "\n",
    "best_optimizer = make_optimizer(\"loaded_model or best_model_path\", \"LEARNING_RATE\") # <-- Parameter shoud be changed!\n",
    "\n",
    "predicton_model = load_ckp(best_model_path,  #Path to the saved checkpoint\n",
    "                        load_model,\n",
    "                        best_optimizer)[0] #load_ckp: [model, optimizer, checkpoint['epoch'], valid_loss_min.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654031",
   "metadata": {},
   "source": [
    "## 6-3. Define inference function\n",
    " - Return dictionaries of predicted labels: {label1: score1, label2: score2, ....}\n",
    " - __Labeles which has lower score than threshold will be ignored.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b871f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, model, device):\n",
    "    preprocessed_image = preprocessing(image)\n",
    "\n",
    "    #for gpu computation\n",
    "    if device.type == 'cuda':\n",
    "        model.cuda()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(preprocessed_image)\n",
    "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist() #1*46 list in a list\n",
    "    #     print(final_output)\n",
    "    #     print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])\n",
    "        result_pair = zip(train_df.columns[1:].to_list(), final_output[0])\n",
    "        result_dict = {}\n",
    "        for label, score in result_pair:\n",
    "            if score > 0.1: #Set prediction threshold\n",
    "                result_dict[label] = score\n",
    "    return sorted(result_dict.items(), key=(lambda x: x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b32e3",
   "metadata": {},
   "source": [
    "- Reference code for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14471026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for sample_id in [1,2,3,4,6]:\n",
    "#     test_img, test_labels = test_dataset[sample_id]\n",
    "#     test_img_path = os.path.join(img_folder, test_dataset.imgs[sample_id])\n",
    "#     with torch.no_grad():\n",
    "#         raw_pred = model(test_img.unsqueeze(0)).cpu().numpy()[0]\n",
    "#         raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
    "\n",
    "#     predicted_labels = np.array(dataset_val.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
    "#     if not len(predicted_labels):\n",
    "#         predicted_labels = ['no predictions']\n",
    "#     img_labels = np.array(dataset_val.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
    "#     plt.imshow(Image.open(test_img_path))\n",
    "#     plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a04e3",
   "metadata": {},
   "source": [
    "## 6-4. Demo test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba979626",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(test_image, #Input sentence\n",
    "         predicton_model,\n",
    "          device) #CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d960c",
   "metadata": {},
   "source": [
    "# 7. Deploying model\n",
    " - Make a pipeline:\n",
    "   - Process 1~6 to created a new model.\n",
    "   - New model should be sent to the deploying server(automatically is best, but manually is also OK).\n",
    "   - After that, deploy server should process feed text datas by the latest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test image open\n",
    "from urllib.request import urlopen\n",
    "import cv2 as cv\n",
    "#image_test = Image.open('/home/ubuntu/Desktop/Project/datasets/circlin_feeds_dataset/raw_data/raw_image/103.60.126.35/var_www_html_Image/SNS/3793/3793_1578721099.png')\n",
    "#image_test = Image.open(urlopen('http://103.60.126.35/Image/SNS/4000/4000_1583499888.png'))\n",
    "#image_test = Image.open(urlopen('https://cyld20183.speedgabia.com/Image/SNS/46272/46272_1624977386.jpg'))\n",
    "image_test = Image.open('/home/ubuntu/Desktop/Project/datasets/circlin_feeds_dataset/raw_data/raw_image/cyld20184.speedgabia.com/Image/SNS/23370/23370_1616239622_1.jpg')\n",
    "#image_test = cv.imread('/home/ubuntu/Desktop/Project/datasets/circlin_feeds_dataset/raw_data/raw_image/cyld20184.speedgabia.com/Image/SNS/23370/23370_1616239622_1.jpg', cv.IMREAD_COLOR)\n",
    "#image_test_RGB = cv.cvtColor(image_test, cv.COLOR_BGR2RGB)\n",
    "resize_test = transforms.Compose([transforms.Resize((256, 256))])\n",
    "image_resized = resize_test(image_test)\n",
    "\n",
    "plt.imshow(image_test_RGB)\n",
    "plt.title(image_test.size)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29286ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5179ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d7977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d842768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ee88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f41261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c37c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b125e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-bert-text-classification",
   "language": "python",
   "name": "tf-bert-text-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
